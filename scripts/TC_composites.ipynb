{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b2397c62-839f-45b0-88f9-b820d9d87981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import importlib\n",
    "import functools\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import cftime\n",
    "import nc_time_axis\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib, matplotlib.pyplot as plt, matplotlib.patheffects as pe\n",
    "\n",
    "import derived\n",
    "import visualization\n",
    "import utilities\n",
    "\n",
    "importlib.reload(derived);\n",
    "importlib.reload(utilities);\n",
    "importlib.reload(visualization);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7669aa-edb3-4f71-addc-c3bb4f0f6207",
   "metadata": {},
   "source": [
    "### Compositing methodology\n",
    "1. Define TC characteristics\n",
    "   - model name,\n",
    "   - experiment name, \n",
    "   - year range\n",
    "   - intensity range (optional)\n",
    "   - basin name (optional)\n",
    "2. Use TC characteristics to find corresponding saved TCs in defined directory\n",
    "3. Define reference point (`reference_timestamp`) for sampling methodology for each TC\n",
    "   - e.g., relative to LMI, relative to genesis)\n",
    "4. Define time delta from reference point (i.e., `reference_timestamp + N hours`, `N` is integer)\n",
    "5. For each TC identified:\n",
    "   - a. Lazy load TC\n",
    "   - b. Select at hour `reference_timestamp + N hours`\n",
    "   - c. Drop all nan values\n",
    "   - d. Store the selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "874a22ec-8df6-4710-bd5e-baa95bc2921c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_filename_year(filename: str) -> int:\n",
    "    delimiter_string = 'storm_ID'\n",
    "    filename_year = int(filename.split(f'{delimiter_string}-')[-1].split('-')[0])\n",
    "    return filename_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a4d9cf1-911e-4fe2-9910-f7d91b4f883b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_intensity(filename: str,\n",
    "                           delimiter_string: str) -> int:\n",
    "    filename_intensity = int(filename.split(f'{delimiter_string}-')[-1].split('.')[0])\n",
    "    return filename_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7e02790-fe36-405b-aaa2-db156f6dfed9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def storm_interpolation_grid_basis_vectors(storm: xr.Dataset,\n",
    "                                           window_size: int,\n",
    "                                           diagnostic: bool=False) -> tuple[np.array, np.array]:\n",
    "\n",
    "    ''' Method to generate uniform interpolation basis vectors for a TC-centered grid. '''\n",
    "\n",
    "    # Ensure necessary basis vector dimensions are available\n",
    "    assert ('grid_xt' in storm.dims) and ('grid_yt' in storm.dims)\n",
    "    # Get differences in grid spacing along each vector\n",
    "    d_grid_yt = storm['grid_yt'].diff(dim='grid_yt')\n",
    "    d_grid_xt = storm['grid_xt'].diff(dim='grid_xt')\n",
    "    # Ensure that the differences are equivalent for all indices to ensure equal spacing\n",
    "    grid_tolerance = 1e-6\n",
    "    assert sum(d_grid_xt.diff(dim='grid_xt') < grid_tolerance) == len(d_grid_xt) - 1, 'Grid is irregular along the `grid_xt` axis.'\n",
    "    assert sum(d_grid_yt.diff(dim='grid_yt') < grid_tolerance) == len(d_grid_yt) - 1, 'Grid is irregular along the `grid_yt` axis.'\n",
    "    # Get grid spacing along each direction\n",
    "    dx, dy = d_grid_xt.isel(grid_xt=0).item(), d_grid_yt.isel(grid_yt=0).item()\n",
    "    # Get the number of grid points in each direction for vector construction\n",
    "    number_x_grid_points = int(np.around((window_size * 2) / dx))\n",
    "    number_y_grid_points = int(np.around((window_size * 2) / dy))\n",
    "    # Construct the basis vectors for Dataset interpolation\n",
    "    arr_x_interp = np.linspace(-window_size, window_size, number_x_grid_points)\n",
    "    arr_y_interp = np.linspace(-window_size, window_size, number_y_grid_points)\n",
    "\n",
    "    if diagnostic:\n",
    "        print(f'[storm_interpolation_gridpoints]: Number of grid points: x = {number_x_grid_points}, y = {number_y_grid_points}')\n",
    "\n",
    "    return arr_x_interp, arr_y_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f0116fe9-1ded-42fd-b940-504ccbfc1660",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def storm_centered_interpolation(storm_sample: xr.Dataset,\n",
    "                                 sampling_field: str,\n",
    "                                 window_size: int=10) -> xr.DataArray:\n",
    "\n",
    "    # Generate storm-centered coordinates.\n",
    "    # This will remove dependence on global coordinates and allow for storm-centered compositing.\n",
    "    arr_x = storm_sample.grid_xt - storm_sample['center_lon']\n",
    "    arr_y = storm_sample.grid_yt - storm_sample['center_lat']\n",
    "\n",
    "    # Generate storm ID list to serve as an axis for the storm identifier that will enable xArray-based compositing\n",
    "    storm_ID = [storm_sample.attrs['storm_id']]\n",
    "    # Expand dimensions of 2D data for the storm ID axis\n",
    "    storm_sample_dataset = np.expand_dims(storm_sample[sampling_field].data, axis=0)\n",
    "    # Generate the xArray DataArray\n",
    "    sample = xr.DataArray(data=storm_sample_dataset,\n",
    "                          dims=['storm_id', 'grid_yt_TC', 'grid_xt_TC'],\n",
    "                          coords={'grid_xt_TC': (['grid_xt_TC'], arr_x.data),\n",
    "                                  'grid_yt_TC': (['grid_yt_TC'], arr_y.data),\n",
    "                                  'storm_id': (['storm_id'], storm_ID)})\n",
    "\n",
    "    # To allow for the combination of different TCs, interpolate the storm-centered coordinates to common axis values based on the window size.\n",
    "    arr_x_interp, arr_y_interp = storm_interpolation_grid_basis_vectors(storm_sample, window_size)\n",
    "\n",
    "    # Perform an interpolation from the storm-centered coordinates to the interpolated coordinates\n",
    "    interpolated_sample = sample.interp(grid_xt_TC=arr_x_interp).interp(grid_yt_TC=arr_y_interp)\n",
    "\n",
    "    return interpolated_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d43b2f62-a4af-472d-945f-120fc807bcd6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def composite_field_correction(dataset: xr.DataArray,\n",
    "                               field_name: str):\n",
    "\n",
    "    factor = 86400 if field_name in ['precip', 'evap', 'p-e'] else 1\n",
    "\n",
    "    return dataset * factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d14da87-2660-47cd-9d08-d30a7346c01f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def derived_quantities(dataset: xr.Dataset):\n",
    "\n",
    "    dataset = derived.TC_surface_wind_speed(dataset)\n",
    "    dataset = derived.TC_net_lw(dataset)\n",
    "    dataset = derived.TC_net_sw(dataset)\n",
    "    # dataset = derived.TC_temperature_disequilibrium(dataset)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f1645661-34fb-4fb0-91dd-842213759a2a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def composite_sample_statistics(pathnames: list,\n",
    "                                model_name: str,\n",
    "                                experiment_name: str,\n",
    "                                plot_histograms: bool=False):\n",
    "\n",
    "    ''' Generate statistics about the storms used for a given composite analysis. '''\n",
    "\n",
    "    assert isinstance(pathnames, list), 'Pathnames provided must be in a list object.'\n",
    "\n",
    "    intensity_parameters = ['max_wind', 'min_slp']\n",
    "    composite_sample_intensities = {}\n",
    "    \n",
    "    for pathname in pathnames:\n",
    "        storm_ID = pathname.split('storm_ID-')[1].split('.')[0]\n",
    "        composite_sample_intensities[storm_ID] = {parameter: int(pathname.split(f'{parameter}-')[1].split('.')[0]) for\n",
    "                                                  parameter in intensity_parameters}\n",
    "\n",
    "    composite_intensity_statistics = pd.DataFrame.from_dict(composite_sample_intensities, orient='index')\n",
    "    print(f'Composite sampling statistics for {model_name}-{experiment_name}: N = {len(pathnames)}; max. wind: {(composite_intensity_statistics['max_wind'].mean()):.1f} +/- {(composite_intensity_statistics['max_wind'].std()):.1f}')\n",
    "\n",
    "    ''' Intensity histograms. '''\n",
    "    if plot_histograms:\n",
    "        histogram_bins = {'max_wind': np.arange(10, 50, 2.5),\n",
    "                          'min_slp': np.arange(900, 1020, 5)}\n",
    "        print_statistical_fields = ['count', 'mean', 'std']\n",
    "        nrows = len(histogram_bins.keys())\n",
    "        fig, axes = plt.subplots(figsize=(4, 2 * nrows), nrows=nrows)\n",
    "        for index, column in enumerate(composite_intensity_statistics.columns):\n",
    "            ax = axes[index]\n",
    "            ax.hist(composite_intensity_statistics[column], bins=histogram_bins[column], alpha=0.5)\n",
    "    \n",
    "            statistics_annotation = [f'{statistical_field}: {composite_intensity_statistics[column].describe()[statistical_field]:.2f}'\n",
    "                                     for statistical_field in print_statistical_fields]\n",
    "            ax.annotate(f\"{model_name}-{experiment_name}\\n{'\\n'.join(statistics_annotation)}\", \n",
    "                        xy=(0.03, 0.96), \n",
    "                        xycoords='axes fraction', \n",
    "                        ha='left', va='top', fontsize=10)\n",
    "        fig.tight_layout()\n",
    "    \n",
    "        x_position, y_position = axes[0].get_position().bounds[0], axes[0].get_position().bounds[1]\n",
    "        fig.suptitle('Composite sample statistics', x=x_position, y=1.05, ha='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a832e521-541d-4ea2-a368-c014782904b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_composite_sample(field_name: str,\n",
    "                              sampling_reference_point: str,\n",
    "                              number_of_offset_hours: int,\n",
    "                              intensity_parameter: str,\n",
    "                              window_size: int, \n",
    "                              pathname: str) -> xr.Dataset:\n",
    "    ''' Generates a snapshot for a given TC to be used as a sample for composite analysis. Snapshot is based on provided conditions. '''\n",
    "\n",
    "    # Load the dataset\n",
    "    storm_dataset = xr.open_dataset(pathname)\n",
    "    # Get the timestamp that sampling is performed at\n",
    "    sampling_timestamp = get_sampling_timestamp(storm=storm_dataset, \n",
    "                                                reference_point=sampling_reference_point,\n",
    "                                                number_of_offset_hours=number_of_offset_hours, \n",
    "                                                intensity_parameter=intensity_parameter)\n",
    "    # Remove the empty data belonging to other timestamps\n",
    "    storm_sample = storm_dataset.sel(time=sampling_timestamp, method='nearest').dropna(dim='grid_xt', how='all').dropna(dim='grid_yt', how='all')\n",
    "    # Get derived fields\n",
    "    storm_sample_derived = derived_quantities(storm_sample)\n",
    "    # Perform grid redefinition and associated spatial interpolation\n",
    "    storm_sample_interpolated = storm_centered_interpolation(storm_sample_derived, sampling_field=field_name, window_size=window_size)\n",
    "    \n",
    "    return storm_sample_interpolated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dbfa7162-0bc3-42eb-b956-efe600a212ed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_composite(model_name: str,\n",
    "                       experiment_name: str,\n",
    "                       year_range: tuple[int, int],\n",
    "                       field_name: str,\n",
    "                       number_of_offset_hours: int=0,\n",
    "                       sampling_reference_point: str='LMI',\n",
    "                       intensity_parameter: str='min_slp',\n",
    "                       intensity_range: tuple[int|float, int|float]=(0, np.inf),\n",
    "                       window_size: int=10,\n",
    "                       troubleshooting: bool=False,\n",
    "                       parallel: bool=False,\n",
    "                       statistics: bool=True):\n",
    "\n",
    "    ''' Method to generate an xArray Dataset with individual TC snapshots at a given number of hours offset from a reference point (genesis or LMI). '''\n",
    "\n",
    "    # Use TC characteristics to find corresponding saved TCs in defined directory\n",
    "    dirname = '/tigress/GEOCLIM/gr7610/analysis/tc_storage/individual_TCs'\n",
    "    filenames = [filename for filename in os.listdir(dirname) if\n",
    "                 model_name in filename and\n",
    "                 experiment_name in filename and\n",
    "                 min(year_range) <= get_filename_year(filename) < max(year_range) and\n",
    "                 min(intensity_range) <= get_filename_intensity(filename, intensity_parameter) < max(intensity_range)]\n",
    "    # Generate list of pathnames\n",
    "    pathnames = [os.path.join(dirname, filename) for filename in filenames]\n",
    "\n",
    "    # Print output statistics for loaded TCs\n",
    "    if statistics:\n",
    "        composite_sample_statistics(pathnames, model_name, experiment_name)\n",
    "\n",
    "    # Initialize list to contain all individual TCs samples\n",
    "    storm_sample_container = []\n",
    "    # Define partial function to allow for using Pool.map since all inputs except `pathname` are the same for each storm\n",
    "    # Generate a partial function for easier parallelization\n",
    "    partial_generate_composite_sample = functools.partial(generate_composite_sample, \n",
    "                                                          field_name, \n",
    "                                                          sampling_reference_point,\n",
    "                                                          number_of_offset_hours,\n",
    "                                                          intensity_parameter,\n",
    "                                                          window_size)\n",
    "    # Gather TC samples for compositing\n",
    "    if parallel:\n",
    "        ''' Offload TC-specific data generation onto parallel processes. '''\n",
    "        # Maximum number of processors for computation\n",
    "        max_number_procs = 20\n",
    "        # Specify number of processors to use\n",
    "        number_procs = len(pathnames) if len(pathnames) < max_number_procs else max_number_procs\n",
    "        \n",
    "        with Pool(processes=number_procs) as pool:\n",
    "            storm_sample_container = pool.map(partial_generate_composite_sample, pathnames)\n",
    "            pool.close()\n",
    "    # Perform it serially\n",
    "    else:\n",
    "        for pathname in pathnames:\n",
    "            # Perform grid redefinition and associated spatial interpolation\n",
    "            storm_sample_interpolated = partial_generate_composite_sample(pathname)\n",
    "            # Append to a container list for future concatenation\n",
    "            storm_sample_container.append(storm_sample_interpolated)\n",
    "    # Concatenate all samples \n",
    "    composite_storm_samples = xr.concat(storm_sample_container, dim='storm_id')\n",
    "    # Add model and experiment designators\n",
    "    composite_storm_samples.attrs['model_name'] = model_name\n",
    "    composite_storm_samples.attrs['experiment_name'] = experiment_name\n",
    "    \n",
    "    if troubleshooting:\n",
    "        [print(f'Loading {pathname}...') for pathname in pathnames]\n",
    "        for storm_ID in composite_storm_sample['storm_id'].values:\n",
    "            fig, ax = plt.subplots(figsize=(5, 4)) \n",
    "            composite_storm_sample.sel(storm_id=storm_ID).plot(ax=ax)\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "    \n",
    "    return composite_storm_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bc5b2e2f-d5dc-4caa-a258-9d3c95af5d4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_sample_GCM_data(model_name: str,\n",
    "                        experiment_name: str,\n",
    "                        field_name: str,\n",
    "                        year_range: tuple[int, int],\n",
    "                        month: int,\n",
    "                        longitude: int|float,\n",
    "                        latitude: int|float,\n",
    "                        window_size: int,\n",
    "                        day: int|None = None):\n",
    "\n",
    "    ''' Method to pull GCM data corresponding to a given TC snapshot. '''\n",
    "\n",
    "    # Construct field dictionary for postprocessed data loading\n",
    "    # See `utilities.postprocessed_data_load` for details.\n",
    "    # Note: this currently only supports single-surface atmospheric data\n",
    "    field_dictionary = {field_name: {'domain': 'atmos', 'level': None}}\n",
    "    # Load the data\n",
    "    sample_GCM_data = utilities.postprocessed_data_load(model_name,\n",
    "                                                        experiment_name,\n",
    "                                                        field_dictionary,\n",
    "                                                        year_range,\n",
    "                                                        month_range=(month, month),\n",
    "                                                        load_full_time=True)[model_name][experiment_name]\n",
    "    # print(f'GCM data time range: {sample_GCM_data.time.min().item()} to {sample_GCM_data.time.max().item()}')\n",
    "    # Define spatial extent for sample clipping\n",
    "    grid_xt_extent = slice(longitude - window_size, longitude + window_size)\n",
    "    grid_yt_extent = slice(latitude - window_size, latitude + window_size)\n",
    "    # Trim the data spatially\n",
    "    sample_GCM_data_filtered = sample_GCM_data.sel(grid_xt=grid_xt_extent).sel(grid_yt=grid_yt_extent)\n",
    "    # Average in time\n",
    "    sample_GCM_data_filtered = sample_GCM_data_filtered.mean(dim='time')\n",
    "\n",
    "    return sample_GCM_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d0cadc46-6b4a-4ba5-b563-13970e2799d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def sample_GCM_constructor(storm_sample: xr.Dataset,\n",
    "                           GCM_sample: xr.Dataset,\n",
    "                           sampling_field: str,\n",
    "                           window_size: int=10):\n",
    "\n",
    "    ''' Modify GCM data into a TC-centered dataset. ''' \n",
    "\n",
    "    # Add center coordinates to the GCM dataset\n",
    "    GCM_sample['center_lon'] = storm_sample['center_lon']\n",
    "    GCM_sample['center_lat'] = storm_sample['center_lat']\n",
    "    GCM_sample.attrs = storm_sample.attrs\n",
    "    # Perform storm-centered interpolation for GCM data\n",
    "    interpolated_GCM_sample = storm_centered_interpolation(storm_sample=GCM_sample,\n",
    "                                                           sampling_field=sampling_field,\n",
    "                                                           window_size=window_size)    \n",
    "\n",
    "    return interpolated_GCM_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3cbba7bc-c3d8-4604-8793-da9c48796f6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_composite_climatology(model_name: str,\n",
    "                                   experiment_name: str,\n",
    "                                   year_range: tuple[int, int],\n",
    "                                   field_name: str,\n",
    "                                   storm_IDs: list[str],\n",
    "                                   number_of_offset_hours: int=0,\n",
    "                                   sampling_reference_point: str='LMI',\n",
    "                                   intensity_parameter: str='min_slp',\n",
    "                                   intensity_range: tuple[int|float, int|float]=(0, np.inf),\n",
    "                                   window_size: int=10,\n",
    "                                   troubleshooting: bool=False,\n",
    "                                   parallel: bool=False):\n",
    "\n",
    "    # Use TC characteristics to find corresponding saved TCs in defined directory\n",
    "    dirname = '/tigress/GEOCLIM/gr7610/analysis/tc_storage/individual_TCs'\n",
    "    filenames = [filename for filename in os.listdir(dirname) if\n",
    "                 model_name in filename and\n",
    "                 experiment_name in filename]\n",
    "    # Generate list of pathnames for the configuration and criteria\n",
    "    configuration_pathnames = [os.path.join(dirname, filename) for filename in filenames]\n",
    "    # Filter pathnames by storm ID\n",
    "    pathnames = [pathname for pathname in configuration_pathnames if\n",
    "                 pathname.split('storm_ID-')[1].split('.')[0] in storm_IDs]\n",
    "\n",
    "    # Initialize list to contain all individual TCs samples\n",
    "    GCM_sample_container = []\n",
    "    # Perform it serially\n",
    "    for pathname in pathnames:\n",
    "        # Load the dataset\n",
    "        storm_dataset = xr.open_dataset(pathname)\n",
    "        # Get the timestamp that sampling is performed at\n",
    "        sampling_timestamp = get_sampling_timestamp(storm=storm_dataset, \n",
    "                                                    reference_point=sampling_reference_point,\n",
    "                                                    number_of_offset_hours=number_of_offset_hours, \n",
    "                                                    intensity_parameter=intensity_parameter)\n",
    "        # Remove the empty data belonging to other timestamps\n",
    "        storm_sample = storm_dataset.sel(time=sampling_timestamp, method='nearest').dropna(dim='grid_xt', how='all').dropna(dim='grid_yt', how='all')\n",
    "            \n",
    "        # Get sample TC month and day\n",
    "        sample_month = sampling_timestamp.month\n",
    "        sample_day = sampling_timestamp.day\n",
    "        # Get sample TC center coordinates\n",
    "        sample_center_longitude = storm_sample['center_lon'].item()\n",
    "        sample_center_latitude = storm_sample['center_lat'].item()\n",
    "        # print(f'Timestamp for {storm_dataset.attrs['storm_id']}: {sampling_timestamp}, {sample_month}, {sample_day}')\n",
    "        # print(f'Coordinates at timestamp for {storm_dataset.attrs['storm_id']}: {sample_center_longitude}, {sample_center_latitude}')\n",
    "        # print('------------------------------------------------------------------------------------------')\n",
    "        # Load GCM data corresponding to the given TC sample\n",
    "        sample_GCM_data = get_sample_GCM_data(model_name, \n",
    "                                              experiment_name,\n",
    "                                              field_name,\n",
    "                                              year_range,\n",
    "                                              sample_month,\n",
    "                                              sample_center_longitude,\n",
    "                                              sample_center_latitude,\n",
    "                                              window_size)\n",
    "        # Construct TC-centered GCM xarray object\n",
    "        sample_GCM_data = sample_GCM_constructor(storm_sample, sample_GCM_data, field_name)\n",
    "        # Append to container list\n",
    "        GCM_sample_container.append(sample_GCM_data)\n",
    "        \n",
    "    # Concatenate all samples \n",
    "    composite_GCM_samples = xr.concat(GCM_sample_container, dim='storm_id')\n",
    "    \n",
    "    return composite_GCM_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e3207fa-ab6d-4451-8c29-d281ba8b8d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name: str,\n",
    "         experiment_name: str,\n",
    "         year_range: tuple[int|float, int|float],\n",
    "         field_name: str,\n",
    "         number_of_offset_hours: int=0,\n",
    "         sampling_reference_point: str='LMI',\n",
    "         intensity_parameter: str='min_slp',\n",
    "         intensity_range: tuple[int|float, int|float]=(0, np.inf),\n",
    "         parallel: bool=False):\n",
    "\n",
    "    composite_storm_samples = generate_composite(model_name, \n",
    "                                                 experiment_name, \n",
    "                                                 year_range, \n",
    "                                                 field_name, \n",
    "                                                 number_of_offset_hours,\n",
    "                                                 sampling_reference_point=sampling_reference_point, \n",
    "                                                 intensity_parameter=intensity_parameter, \n",
    "                                                 intensity_range=intensity_range,\n",
    "                                                 parallel=parallel)\n",
    "\n",
    "    composite_storm_samples = composite_field_correction(composite_storm_samples, field_name)\n",
    "\n",
    "    return composite_storm_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2b7e846e-e47a-4b8a-bec7-49ce0af843a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_timestamp(storm: xr.Dataset,\n",
    "                           reference_point: str='LMI',\n",
    "                           number_of_offset_hours: int=0,\n",
    "                           intensity_parameter: str='max_wind') -> cftime._cftime.DatetimeNoLeap:\n",
    "\n",
    "    ''' Method to obtain the timestamp to sample a TC at. '''\n",
    "\n",
    "    # Calendar types (see https://unidata.github.io/cftime/api.html#cftime.datetime for reference)\n",
    "    # Assumes calendars are either `noleap` or `julian` based on GFDL GCM output data\n",
    "    calendar_types = {\"<class 'cftime._cftime.DatetimeJulian'>\": 'julian',\n",
    "                      \"<class 'cftime._cftime.datetime'>\": 'noleap',\n",
    "                      \"<class 'cftime._cftime.DatetimeNoLeap'>\": 'noleap'}\n",
    "    # Get calendar type\n",
    "    calendar_type = storm.isel(time=0)['time'].item()\n",
    "    calendar_format = calendar_types[str(type(calendar_type))]\n",
    "    # Obtain the first timestamp for a TC (assumed to be genesis)\n",
    "    genesis_timestamp = storm.isel(time=0)['time'].item()\n",
    "    # Define the identifier used as a reference time for cftime timedelta calculation\n",
    "    reference_identifier = f'seconds since {genesis_timestamp}'\n",
    "\n",
    "    # Obtain the reference timestamp (either genesis or LMI)\n",
    "    if reference_point == 'LMI':\n",
    "        # Obtain the value of maximum intensity\n",
    "        maximum_intensity = storm[intensity_parameter].max() if intensity_parameter == 'max_wind' else storm[intensity_parameter].min()\n",
    "        # Obtain the reference timestamp based on the maximum intensity value\n",
    "        reference_timestamp = storm.where(storm[intensity_parameter] == maximum_intensity)[intensity_parameter].dropna(dim='time')['time']\n",
    "        # Ensure that there's at least one valid timestamp, and remove any duplicate values\n",
    "        assert len(reference_timestamp) > 0, f\"No reference timestamp found for storm ID {storm.attrs['storm_id']}.\"\n",
    "        reference_timestamp = reference_timestamp.item() if len(reference_timestamp) == 1 else reference_timestamp.isel(time=0).item()\n",
    "    elif reference_point == 'genesis':\n",
    "        reference_timestamp = genesis_timestamp\n",
    "        assert number_of_offset_hours >= 0, f'Number of offset hours must be at least 0 when sampling a timestamp relative to storm genesis.'\n",
    "\n",
    "    # Convert the cftime object to number of seconds since the reference time `reference_identifier`\n",
    "    reference_timestamp_seconds = cftime.date2num(reference_timestamp, reference_identifier, calendar=calendar_format)\n",
    "    # Add the number of offset hours (in units of seconds)\n",
    "    sampling_timestamp_seconds = reference_timestamp_seconds + number_of_offset_hours*3600\n",
    "    # Convert this number back to a cftime object\n",
    "    sampling_timestamp = cftime.num2date(sampling_timestamp_seconds, reference_identifier, calendar=calendar_format)\n",
    "    \n",
    "    return sampling_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a6392bd0-d119-4927-ab89-ffc85ebfcf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite sampling statistics for FLOR-CTL1990s_FA_tiger3: N = 7; max. wind: 31.1 +/- 5.9\n"
     ]
    }
   ],
   "source": [
    "model_name = 'FLOR'\n",
    "experiment_name = 'CTL1990s_FA_tiger3'\n",
    "year_range = (1901, 1905)\n",
    "intensity_parameter, intensity_range = 'min_slp', (960, 1000)\n",
    "field_name = 'wind'\n",
    "\n",
    "composite_TC_samples = main(model_name,\n",
    "                            experiment_name,\n",
    "                            year_range,\n",
    "                            field_name,\n",
    "                            parallel=False)\n",
    "composite_TC_storm_IDs = composite_TC_samples['storm_id'].values\n",
    "\n",
    "# composite_GCM_samples = generate_composite_climatology(model_name,\n",
    "#                                                        experiment_name,\n",
    "#                                                        year_range,\n",
    "#                                             field_name,\n",
    "#                                                        storm_IDs=composite_TC_storm_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "74fd1e91-2b7b-4672-950b-35a9cd4aebed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_composite' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplot_composite\u001b[49m(composite_TC_samples, field_name)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_composite' is not defined"
     ]
    }
   ],
   "source": [
    "plot_composite(composite_TC_samples, field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "62e8c470-b918-4e03-b955-f58f8b69a975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_composite(composite_samples: xr.Dataset | xr.DataArray,\n",
    "                   field_name: str,\n",
    "                   plotting_method: str='contourf',\n",
    "                   number_of_normalization_bins: int=16):\n",
    "\n",
    "    ''' Plot composite values for a given model and experiment. '''\n",
    "\n",
    "    # Get mean and standard deviation over all TC samples\n",
    "    composite_mean = composite_samples.mean(dim='storm_id')\n",
    "    composite_std = composite_samples.std(dim='storm_id')\n",
    "\n",
    "    # Get normalization and colormap for the composite plot\n",
    "    norm, cmap = visualization.norm_cmap(composite_mean, \n",
    "                                         field=field_name, \n",
    "                                         num_bounds=number_of_normalization_bins)\n",
    "\n",
    "    ''' Plotting. '''\n",
    "    # Initialize figure\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "    # Define plotting method\n",
    "    if plotting_method == 'contourf':\n",
    "        im = ax.contourf(composite_mean.grid_xt_TC, composite_mean.grid_yt_TC, composite_mean, \n",
    "                         norm=norm, cmap=cmap, levels=len(norm.boundaries))\n",
    "    else:\n",
    "        im = ax.pcolormesh(composite_mean.grid_xt_TC, composite_mean.grid_yt_TC, composite_mean, \n",
    "                           norm=norm, cmap=cmap)\n",
    "\n",
    "    # Append the colorbar\n",
    "    colorbar_axis = add_colorbar(ax, field_name, norm, cmap)\n",
    "\n",
    "def add_colorbar(ax,\n",
    "                 field_name: str,\n",
    "                 norm, \n",
    "                 cmap,\n",
    "                 orientation: str='vertical'):\n",
    "\n",
    "    # Define colorbar axis\n",
    "    colorbar_axis_thickness = 0.05 # in units of axes fraction\n",
    "    colorbar_axis_padding = 0.025 # in units of axes fraction\n",
    "    colorbar_axis_extent = [1 + colorbar_axis_padding, 0, colorbar_axis_thickness, 1] if orientation == 'vertical' else [0, 0 - colorbar_axis_padding, 1, colorbar_axis_thickness]\n",
    "    colorbar_axis = ax.inset_axes(colorbar_axis_extent)\n",
    "\n",
    "    colorbar = fig.colorbar(matplotlib.cm.ScalarMappable(norm, cmap), cax=colorbar_axis)\n",
    "\n",
    "    return colorbar_axis\n",
    "\n",
    "def plot_metadata(ax,\n",
    "                  composite_samples: xr.Dataset | xr.DataArray,\n",
    "                  field_name,\n",
    "                  ax,):\n",
    "\n",
    "    # Insert annotation for sample count\n",
    "    number_of_samples = len(composite_samples['storm_id'].values)\n",
    "    sample_number_annotation = ax.annotate(f'N = {number_of_samples}', \n",
    "                                           xy=(0.03, 0.03), \n",
    "                                           xycoords='axes fraction', \n",
    "                                           fontsize=10, \n",
    "                                           ha='left', \n",
    "                                           va='bottom')\n",
    "    sample_number_annotation.set_path_effects([pe.Stroke(linewidth=1.5, foreground='white'), pe.Normal()])\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    long_name, units = visualization.field_properties(field_name)\n",
    "    maximum_intensity = composite_mean.min() if field_name in ['slp', 'olr', 'netrad_toa'] else composite_mean.max()\n",
    "    composite_statistics_string = f'Peak change: {maximum_intensity:.2f} {units}'\n",
    "    \n",
    "    title_string = f'Composite mean for {model_name}, {experiment_name}\\n{long_name.capitalize()} [{units}]\\n{composite_statistics_string}'\n",
    "    ax.set_title(title_string, loc='left', ha='left', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a43b7-5b37-4063-a6f0-484e35a13e1e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf91f53-1189-452d-a819-c3dcb9e12ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_normalization_bins = 16\n",
    "plotting_method = 'contourf'\n",
    "field_name = 'shflx'\n",
    "\n",
    "run_CTL, run_EXP = composite_storms.keys()\n",
    "experiment_name = f'{run_CTL} - {run_EXP}'\n",
    "\n",
    "composite_CTL, composite_EXP = [composite_storms[run_CTL][field_name].mean(dim='storm_id'), \n",
    "                                composite_storms[run_EXP][field_name].mean(dim='storm_id')]\n",
    "composite_dataset = composite_CTL - composite_EXP\n",
    "\n",
    "assert plotting_method in ['contourf', 'pcolormesh'], 'Provide plotting method string that is either `contourf` or `pcolormesh`.'\n",
    "\n",
    "number_of_samples = min([len(composite_CTL), len(composite_EXP)])\n",
    "\n",
    "composite_mean = composite_dataset\n",
    "composite_std = composite_dataset.std()\n",
    "\n",
    "norm, cmap = visualization.norm_cmap(composite_mean, \n",
    "                                     field=field_name, \n",
    "                                     num_bounds=number_of_normalization_bins)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "if plotting_method == 'contourf':\n",
    "    im = ax.contourf(composite_mean.grid_xt_TC, composite_mean.grid_yt_TC, composite_mean, \n",
    "                     norm=norm, cmap=cmap, levels=len(norm.boundaries))\n",
    "else:\n",
    "    im = ax.pcolormesh(composite_mean.grid_xt_TC, composite_mean.grid_yt_TC, composite_mean, \n",
    "                       norm=norm, cmap=cmap)\n",
    "\n",
    "# Insert annotation for sample count\n",
    "sample_number_annotation = ax.annotate(f'N = {number_of_samples}', xy=(0.03, 0.03), xycoords='axes fraction', fontsize=10, ha='left', va='bottom')\n",
    "sample_number_annotation.set_path_effects([pe.Stroke(linewidth=1.5, foreground='white'), pe.Normal()])\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "cax = ax.inset_axes([1.025, 0, 0.05, 1])\n",
    "colorbar = fig.colorbar(matplotlib.cm.ScalarMappable(norm, cmap), cax=cax)\n",
    "\n",
    "long_name, units = visualization.field_properties(field_name)\n",
    "maximum_intensity = composite_mean.min() if field_name in ['slp', 'olr'] else composite_mean.max()\n",
    "composite_statistics_string = f'{maximum_intensity:.2f} +/- {composite_std.max():.2f}; min. = {composite_dataset.min():.2f}, max. = {composite_dataset.max():.2f}'\n",
    "\n",
    "title_string = f'Composite mean for {model_name}, {experiment_name}\\n{long_name.capitalize()} [{units}]\\n{composite_statistics_string}'\n",
    "ax.set_title(title_string, loc='left', ha='left', fontsize=10);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
