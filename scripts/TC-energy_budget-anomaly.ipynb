{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae42629-7ffe-4bee-bdb5-8976a41a0e47",
   "metadata": {},
   "source": [
    "### TC energy budget anomaly\n",
    "Gets the energy budget anomaly for a given TC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "334f6903-365c-42d3-9c4f-cfc206fc87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import cartopy, cartopy.crs as ccrs, matplotlib, matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import cftime\n",
    "import functools\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import utilities\n",
    "import visualization\n",
    "\n",
    "importlib.reload(utilities);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38bdaa42-a1eb-4cfd-96b6-72481fdeb1fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_filename_year(filename: str) -> int:\n",
    "    delimiter_string = 'storm_ID'\n",
    "    filename_year = int(filename.split(f'{delimiter_string}-')[-1].split('-')[0])\n",
    "    return filename_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e452632-f433-4c04-984f-6f56f1ee93c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_filename_intensity(filename: str,\n",
    "                           delimiter_string: str) -> int:\n",
    "    filename_intensity = int(filename.split(f'{delimiter_string}-')[-1].split('.')[0])\n",
    "    return filename_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0a70768-7b47-4337-b10b-f63292852852",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def field_correction(model_name: str, \n",
    "                     dataset: xr.DataArray,\n",
    "                     field_name: str):\n",
    "\n",
    "    if field_name in ['precip', 'evap', 'p-e']:\n",
    "        # Conversion of total precipitation per hour to daily precipitation rate for ERA5 data, instantaneous rate to daily for GFDL GCM data\n",
    "        factor = (1 / 3600) * 1000 * 86400 if model_name == 'ERA5' else 86400\n",
    "    else:\n",
    "        factor = 1\n",
    "    dataset[field_name] = dataset[field_name] * factor\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f4ae964-c679-456d-a2ec-4e2262e0a1e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_time_window(dataset: xr.Dataset,\n",
    "                    timestamp,\n",
    "                    window_day_size: int):\n",
    "\n",
    "    ''' Filter a dataset by a given timestamp +/- a specific number of days. '''\n",
    "\n",
    "    # Obtain day of year for the timestamp\n",
    "    timestamp_day_of_year = timestamp.dayofyr if 'cftime' in str(type(timestamp)) else timestamp.dt.dayofyear\n",
    "    # Dataset time array days of year (handled differently by time object type)\n",
    "    dataset_day_of_year = dataset.time.dt.dayofyear if 'cftime' in str(type(timestamp)) else dataset.time.dt.dayofyear\n",
    "    # Get start and end days of year\n",
    "    start_day_of_year, end_day_of_year = timestamp_day_of_year - window_day_size, timestamp_day_of_year + window_day_size\n",
    "    # Mask by window from start_day_of_year to end_day_of_year\n",
    "    window = (dataset_day_of_year >= start_day_of_year) & (dataset_day_of_year <= end_day_of_year)\n",
    "    # Mask data by the window\n",
    "    dataset_window = dataset.sel(time=window)\n",
    "\n",
    "    return dataset_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07bce2f8-406e-474a-b355-8f6ef2ba0621",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def grid_check(dataset: xr.Dataset) -> tuple[float, float]:\n",
    "\n",
    "    ''' Perform grid checks and get grid spacing for a generic xArray coordinate system. '''\n",
    "\n",
    "    # Ensure necessary basis vector dimensions are available\n",
    "    assert ('grid_xt' in dataset.dims) and ('grid_yt' in dataset.dims)\n",
    "    # Get differences in grid spacing along each vector\n",
    "    d_grid_yt = dataset['grid_yt'].diff(dim='grid_yt')\n",
    "    d_grid_xt = dataset['grid_xt'].diff(dim='grid_xt')\n",
    "    # Ensure that the differences are equivalent for all indices to ensure equal spacing\n",
    "    grid_tolerance = 1e-6\n",
    "    assert sum(d_grid_xt.diff(dim='grid_xt') < grid_tolerance) == len(d_grid_xt) - 1, 'Grid is irregular along the `grid_xt` axis.'\n",
    "    assert sum(d_grid_yt.diff(dim='grid_yt') < grid_tolerance) == len(d_grid_yt) - 1, 'Grid is irregular along the `grid_yt` axis.'\n",
    "    # Get grid spacing along each direction\n",
    "    dx, dy = d_grid_xt.isel(grid_xt=0).item(), d_grid_yt.isel(grid_yt=0).item()\n",
    "\n",
    "    return abs(dx), abs(dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d78a3a9-36fa-4d17-b615-8703b6eb4a3d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def grid_interpolation(working_grid: xr.Dataset,\n",
    "                       reference_grid: xr.DataArray,\n",
    "                       diagnostic: bool=False) -> xr.DataArray:\n",
    "\n",
    "    ''' \n",
    "    Method to generate uniform interpolation basis vectors for a TC-centered grid. \n",
    "    Boolean `equal_number_of_points` is used to ensure equal grid point numbers, and is optional.\n",
    "    '''\n",
    "\n",
    "    # Ensure necessary basis vector dimensions are available\n",
    "    assert ('grid_xt' in working_grid.dims) and ('grid_yt' in working_grid.dims)\n",
    "    # Get differences in grid spacing along each vector\n",
    "    d_grid_yt = reference_grid['grid_yt'].diff(dim='grid_yt')\n",
    "    d_grid_xt = reference_grid['grid_xt'].diff(dim='grid_xt')\n",
    "    # Ensure that the differences are equivalent for all indices to ensure equal spacing\n",
    "    grid_tolerance = 1e-6\n",
    "    assert sum(d_grid_xt.diff(dim='grid_xt') < grid_tolerance) == len(d_grid_xt) - 1, 'Grid is irregular along the `grid_xt` axis.'\n",
    "    assert sum(d_grid_yt.diff(dim='grid_yt') < grid_tolerance) == len(d_grid_yt) - 1, 'Grid is irregular along the `grid_yt` axis.'\n",
    "    # Get grid spacing along each direction\n",
    "    dx, dy = d_grid_xt.isel(grid_xt=0).item(), d_grid_yt.isel(grid_yt=0).item()\n",
    "    \n",
    "    # Padding on window search for storm coordinate grid\n",
    "    # This expands the window `padding_factor` grid cells in each direction of the window extent for a given storm timestamp\n",
    "    padding_factor = 0\n",
    "    \n",
    "    # Get extent of longitudes\n",
    "    minimum_longitude, maximum_longitude = [reference_grid['grid_xt'].min().item() - dx * padding_factor, \n",
    "                                            reference_grid['grid_xt'].max().item() + dx * padding_factor]\n",
    "    # Get extent of latitudes\n",
    "    minimum_latitude, maximum_latitude = [reference_grid['grid_yt'].min().item() - dy * padding_factor, \n",
    "                                          reference_grid['grid_yt'].max().item() + dy * padding_factor]\n",
    "\n",
    "    # Round values due to weird GFDL GCM output behavior\n",
    "    minimum_longitude, maximum_longitude = [np.round(minimum_longitude, decimals=4),\n",
    "                                            np.round(maximum_longitude, decimals=4)]\n",
    "    minimum_latitude, maximum_latitude = [np.round(minimum_latitude, decimals=4),\n",
    "                                          np.round(maximum_latitude, decimals=4)]\n",
    "    dx, dy = [np.round(dx, decimals=4),\n",
    "              np.round(dy, decimals=4)]\n",
    "\n",
    "    if diagnostic:\n",
    "        print(f'[grid_interpolation()] Window extent: longitudes = {(minimum_longitude, maximum_longitude)} and latitudes= {(minimum_latitude, maximum_latitude)}')\n",
    "        print(f'[grid_interpolation()] dx = {dx}; dy = {dy}')\n",
    "\n",
    "    # Construct interpolation arrays for interpolating the area grid onto the storm grid\n",
    "    interpolation_array_x = np.arange(minimum_longitude, maximum_longitude + dx, dx)\n",
    "    interpolation_array_y = np.arange(minimum_latitude, maximum_latitude + dy, dy)\n",
    "    \n",
    "    if diagnostic:\n",
    "        print(f'[grid_interpolation()] interpolated latitudes: {interpolation_array_y}')\n",
    "    \n",
    "    # Perform interpolation of area grid onto the storm grid\n",
    "    interpolated_working_grid = working_grid.interp(grid_xt=interpolation_array_x).interp(grid_yt=interpolation_array_y)\n",
    "\n",
    "    return interpolated_working_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2fa002c-6a29-486b-8f42-8d1f2cae7005",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_surface_area(storm_dataset_timestamp: xr.DataArray,\n",
    "                     diagnostic: bool=False) -> xr.DataArray:\n",
    "\n",
    "    # Load surface area data from GFDL GCM output\n",
    "    surface_area = xr.open_dataset('/projects/GEOCLIM/gr7610/tools/AM2.5_atmos_area.nc')['__xarray_dataarray_variable__']\n",
    "\n",
    "    # Shed null values\n",
    "    storm_dataset_timestamp = storm_dataset_timestamp.dropna('grid_xt', how='all').dropna('grid_yt', how='all')\n",
    "    # Get minimum and maximum spatial extent values\n",
    "    minimum_longitude, maximum_longitude = [storm_dataset_timestamp['grid_xt'].min().item(), \n",
    "                                            storm_dataset_timestamp['grid_xt'].max().item()]\n",
    "    minimum_latitude, maximum_latitude = [storm_dataset_timestamp['grid_yt'].min().item(), \n",
    "                                          storm_dataset_timestamp['grid_yt'].max().item()]\n",
    "    # Round values due to weird GFDL GCM output behavior\n",
    "    minimum_longitude, maximum_longitude = [np.round(minimum_longitude, decimals=4),\n",
    "                                            np.round(maximum_longitude, decimals=4)]\n",
    "    minimum_latitude, maximum_latitude = [np.round(minimum_latitude, decimals=4),\n",
    "                                          np.round(maximum_latitude, decimals=4)]\n",
    "    # Get surface area at iterand timestamp\n",
    "    surface_area_timestamp = surface_area.sel(grid_xt=slice(minimum_longitude, maximum_longitude),\n",
    "                                              grid_yt=slice(minimum_latitude, maximum_latitude))\n",
    "\n",
    "    # Interpolate area onto storm coordinates\n",
    "    interpolated_surface_area_timestamp = grid_interpolation(surface_area_timestamp, storm_dataset_timestamp)\n",
    "\n",
    "    if diagnostic:\n",
    "        print('------------------------------------')\n",
    "        print(f'Window extent: longitudes = {(minimum_longitude, maximum_longitude)} and latitudes= {(minimum_latitude, maximum_latitude)}')\n",
    "        print(f'[get_surface_area()]: Storm dataset latitudes:\\n{storm_dataset_timestamp.grid_yt.values}')\n",
    "        print(f'[get_surface_area()]: Surface area dataset latitudes:\\n{interpolated_surface_area_timestamp.grid_yt.values}')\n",
    "\n",
    "    # Make sure all longitudes and latitudes are within some tolerance of each other\n",
    "    assert np.allclose(storm_dataset_timestamp.grid_xt, interpolated_surface_area_timestamp.grid_xt), f'\\nData longitudes:\\n{storm_dataset_timestamp.grid_xt.values}; \\n Surface area longitudes:\\n{surface_area_timestamp.grid_xt.values}'\n",
    "    assert np.allclose(storm_dataset_timestamp.grid_yt, interpolated_surface_area_timestamp.grid_yt), f'\\nData latitude:\\n{storm_dataset_timestamp.grid_yt.values}; \\n Surface area latitudes:\\n{surface_area_timestamp.grid_yt.values}'\n",
    "\n",
    "    return interpolated_surface_area_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a9f0119-418d-413b-8b2f-79d272987e3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_sample_GCM_data(model_name: str,\n",
    "                        experiment_name: str,\n",
    "                        field_name: str,\n",
    "                        year_range: tuple[int, int],\n",
    "                        sampling_timestamp: pd.Timestamp,\n",
    "                        longitude: int|float,\n",
    "                        latitude: int|float,\n",
    "                        window_size: int,\n",
    "                        sampling_day_window: int=5,\n",
    "                        diagnostic: bool=False):\n",
    "\n",
    "    ''' Method to pull GCM data corresponding to a given TC snapshot. '''\n",
    "\n",
    "    # Construct field dictionary for postprocessed data loading\n",
    "    # See `utilities.postprocessed_data_load` for details.\n",
    "    # Note: this currently only supports single-surface atmospheric data\n",
    "    field_dictionary = {field_name: {'domain': 'atmos', 'level': None}}\n",
    "    # Extract month from the iterand timestamp to perform initial climatology filtering\n",
    "    sampling_year, sampling_month, sampling_day = [sampling_timestamp.year,\n",
    "                                                   sampling_timestamp.month,\n",
    "                                                   sampling_timestamp.day,]\n",
    "    # Load the data\n",
    "    sample_GCM_data = utilities.postprocessed_data_load(model_name,\n",
    "                                                        experiment_name,\n",
    "                                                        field_dictionary,\n",
    "                                                        year_range,\n",
    "                                                        data_type='mean_daily',\n",
    "                                                        month_range=(sampling_month, sampling_month),\n",
    "                                                        load_full_time=True)[model_name][experiment_name]\n",
    "    # Get GCM grid spacing\n",
    "    GCM_dx, GCM_dy = grid_check(sample_GCM_data)\n",
    "    # Define spatial extent for sample clipping\n",
    "    grid_xt_extent = slice(longitude - window_size, longitude + window_size)\n",
    "    grid_yt_extent = slice(latitude - window_size, latitude + window_size)\n",
    "    # Trim the data spatially\n",
    "    sample_GCM_data_filtered_space = sample_GCM_data.sortby('grid_yt').sel(grid_xt=grid_xt_extent).sel(grid_yt=grid_yt_extent)\n",
    "    # Subsample over the time window specified: (iterand timestamp - sampling_day_window) to (iterand_timestamp + sampling_day_window)\n",
    "    sample_GCM_data_filtered_time = get_time_window(sample_GCM_data_filtered_space, sampling_timestamp, sampling_day_window)\n",
    "    # Average in time\n",
    "    sample_GCM_data_filtered = sample_GCM_data_filtered_time.mean(dim='time')\n",
    "    \n",
    "    if diagnostic:\n",
    "        print(f'Storm timestamp center: {longitude}, {latitude}.')\n",
    "        print(f'GCM grid spacing: dx = {GCM_dx}, dy = {GCM_dy}.')\n",
    "        print(f'Storm timestamp extent = longitude: {grid_xt_extent}, latitude: {grid_yt_extent}.')\n",
    "        print(f'GCM extent = longitude: {sample_GCM_data.grid_xt.values}, latitude: {sample_GCM_data.grid_yt.values}.')\n",
    "        print(f'Filtered GCM extent = longitude: {sample_GCM_data_filtered.grid_xt.values}, latitude: {sample_GCM_data_filtered.grid_yt.values}.')\n",
    "\n",
    "    return sample_GCM_data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdc5cdf1-cbbc-443b-8dc7-f55f86f0db1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_TC_anomaly_timestamp(model_name: str,\n",
    "                             experiment_name: str,\n",
    "                             year_range: tuple[int, int],\n",
    "                             storm_reanalysis_data: xr.Dataset,\n",
    "                             field_name: str,\n",
    "                             sampling_timestamp: cftime.datetime,\n",
    "                             diagnostic: bool=False):\n",
    "    \n",
    "        # Get timestamp\n",
    "        storm_sample = storm_reanalysis_data.sel(time=sampling_timestamp)\n",
    "        sampling_timestamp = storm_sample.time.item()\n",
    "    \n",
    "        # Get sample TC month and day\n",
    "        sample_month = sampling_timestamp.month\n",
    "        sample_day = sampling_timestamp.day\n",
    "        # Get sample TC center coordinates\n",
    "        sample_center_longitude = storm_sample['center_lon'].item()\n",
    "        sample_center_latitude = storm_sample['center_lat'].item()\n",
    "    \n",
    "        # Load GCM data according to the given sample\n",
    "        sample_GCM_data = get_sample_GCM_data(model_name, \n",
    "                                              experiment_name,\n",
    "                                              field_name,\n",
    "                                              year_range,\n",
    "                                              sampling_timestamp,\n",
    "                                              sample_center_longitude,\n",
    "                                              sample_center_latitude,\n",
    "                                              window_size=10)\n",
    "        sample_GCM_data['time'] = sampling_timestamp\n",
    "\n",
    "        # Interpolate the GCM data to the storm data\n",
    "        sample_timestamp_area = get_surface_area(storm_sample)\n",
    "        interpolated_sample_timestamp_area = grid_interpolation(sample_timestamp_area, storm_sample)\n",
    "        sample_GCM_data = grid_interpolation(sample_GCM_data, storm_sample)\n",
    "    \n",
    "        # Get simple anomaly\n",
    "        TC_climatological_anomaly_timestamp = storm_sample[field_name] - sample_GCM_data[field_name]\n",
    "\n",
    "        if diagnostic:\n",
    "            print(f'Storm shape at timestamp {sampling_timestamp}: {storm_sample.grid_xt.shape}, {storm_sample.grid_yt.shape}')\n",
    "            print(f'GCM shape at timestamp {sampling_timestamp}: {sample_GCM_data.grid_xt.shape}, {sample_GCM_data.grid_yt.shape}')\n",
    "            print(f'Area shape at timestamp {sampling_timestamp}: {sample_timestamp_area.grid_xt.shape}, {sample_timestamp_area.grid_yt.shape}')\n",
    "            print(f'Anomaly shape at timestamp {sampling_timestamp}: {TC_climatological_anomaly_timestamp.grid_xt.shape}, {TC_climatological_anomaly_timestamp.grid_yt.shape}')\n",
    "    \n",
    "        # Get area-integrated anomaly\n",
    "        TC_climatological_anomaly_timestamp_integrated = (TC_climatological_anomaly_timestamp * sample_timestamp_area).sum().item()\n",
    "        \n",
    "        if diagnostic:\n",
    "            units = storm_sample[field_name].attrs['units']\n",
    "            print(f'Area-integrated TC anomaly for timestamp {sampling_timestamp}: {TC_climatological_anomaly_timestamp_integrated:.2e} {units}')\n",
    "    \n",
    "        return TC_climatological_anomaly_timestamp, TC_climatological_anomaly_timestamp_integrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5286eedb-4f13-4cbe-8e7b-e3a7dd1b2418",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_TC_anomaly(model_name: str, \n",
    "                   experiment_name: str,\n",
    "                   field_name: str,\n",
    "                   year_range: tuple,\n",
    "                   storm_reanalysis_data: xr.Dataset,\n",
    "                   parallel: bool=False):\n",
    "    \n",
    "    time_integrated_anomaly = 0\n",
    "    \n",
    "    storm_GCM_data, area_integrated_TC_anomaly = {}, {}\n",
    "\n",
    "    partial_TC_anomaly_timestamp = functools.partial(get_TC_anomaly_timestamp,\n",
    "                                                     model_name,\n",
    "                                                     experiment_name,\n",
    "                                                     year_range,\n",
    "                                                     storm_reanalysis_data,\n",
    "                                                     field_name)\n",
    "\n",
    "    sampling_timestamps = storm_reanalysis_data.time.values\n",
    "\n",
    "    # If chosen, run in parallel using the partial function\n",
    "    if parallel:\n",
    "        with Pool() as pool:\n",
    "            TC_anomaly_pool_outputs = pool.map(partial_TC_anomaly_timestamp, sampling_timestamps)\n",
    "            TC_anomaly_timestamps, TC_anomaly_timestamps_integrated = zip(*TC_anomaly_pool_outputs)\n",
    "            pool.close()\n",
    "    # Else, run serial. Serial is usually better for troubleshooting and debugging.\n",
    "    else:\n",
    "        # Initialize container lists for appending outputs for each timestamp\n",
    "        TC_anomaly_timestamps, TC_anomaly_timestamps_integrated = [], []\n",
    "        for sampling_timestamp in sampling_timestamps:\n",
    "            TC_anomaly_timestamp, TC_anomaly_timestamp_integrated = partial_TC_anomaly_timestamp(sampling_timestamp)\n",
    "            \n",
    "            TC_anomaly_timestamps.append(TC_anomaly_timestamp)\n",
    "            TC_anomaly_timestamps_integrated.append(TC_anomaly_timestamp_integrated)\n",
    "    \n",
    "    TC_anomaly_dataset = xr.concat(TC_anomaly_timestamps, dim='time').sortby('time')\n",
    "    \n",
    "    TC_anomaly_dataset['center_lon'] = storm_reanalysis_data['center_lon']\n",
    "    TC_anomaly_dataset['center_lat'] = storm_reanalysis_data['center_lat']\n",
    "    TC_anomaly_dataset['max_wind'] = storm_reanalysis_data['max_wind']\n",
    "    TC_anomaly_dataset['min_slp'] = storm_reanalysis_data['min_slp']\n",
    "    TC_anomaly_dataset.attrs['storm_id'] = storm_reanalysis_data.attrs['storm_id']\n",
    "\n",
    "    time_integrated_anomaly = np.sum(np.array(TC_anomaly_timestamps_integrated))\n",
    "    \n",
    "    print('------------------------------------------------------------------------------------')\n",
    "    units = storm_reanalysis_data[field_name].attrs['units']\n",
    "    print(f'Time-integrated TC anomaly over TC lifetime: {time_integrated_anomaly:.2e} {units}')\n",
    "    \n",
    "    return TC_anomaly_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6125b6c1-64f2-4487-8720-a12b83e8c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animator(model_name: str,\n",
    "             dataset: xr.Dataset, \n",
    "             field_name: str,\n",
    "             anomaly: bool=True,\n",
    "             extrema: tuple[int|float, int|float] | None=None):\n",
    "\n",
    "    ''' Visualization. '''\n",
    "    \n",
    "    # Perform animation-specific commands for matplotlib's backend\n",
    "    from matplotlib import animation\n",
    "    plt.rcParams['animation.embed_limit'] = 2**28\n",
    "    plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "    plt.ioff()\n",
    "    plt.cla()\n",
    "    \n",
    "    # Get colormap and normalization for the dataset entirety\n",
    "    norm, cmap = visualization.norm_cmap(dataset, field=field_name, white_adjust=anomaly, extrema=extrema)\n",
    "\n",
    "    # Initialize the figure\n",
    "    projection, reference_projection = ccrs.PlateCarree(central_longitude=180), ccrs.PlateCarree()\n",
    "    fig, ax = plt.subplots(figsize=(5, 4), subplot_kw={'projection': projection})\n",
    "    \n",
    "    def title_generator(model_name: str,\n",
    "                        storm_sample: xr.Dataset, \n",
    "                        field_name: str, \n",
    "                        sampling_timestamp: pd.Timestamp):\n",
    "        ''' Method to generate a string for titling an axis. '''\n",
    "        maximum_wind = storm_sample['max_wind']\n",
    "        minimum_pressure = storm_sample['min_slp']\n",
    "        long_name, units = visualization.field_properties(field_name)\n",
    "        ax.set_title('') # xArray bug - this blanks out the title\n",
    "        ax.set_title(f'{model_name}, storm ID: {storm_sample.attrs['storm_id']}\\nMax. wind: {maximum_wind:.2f} m/s; min. SLP: {minimum_pressure:.2f} hPa\\n{long_name.capitalize()} [{units}]', loc='left', ha='left', fontsize=10)\n",
    "    \n",
    "    def init_func():\n",
    "        ax.clear()\n",
    "        \n",
    "        gridlines = ax.gridlines(ls='--', alpha=0.5)\n",
    "        gridlines.bottom_labels = True\n",
    "        ax.coastlines()\n",
    "        \n",
    "        cax = ax.inset_axes([1.03, 0, 0.02, 1])\n",
    "        colorbar = fig.colorbar(matplotlib.cm.ScalarMappable(norm, cmap), cax=cax)\n",
    "    \n",
    "    def animate(frame):\n",
    "        ax.clear()\n",
    "    \n",
    "        ''' Begin. '''\n",
    "\n",
    "        snapshot = dataset.isel(time=frame)\n",
    "        ax.pcolormesh(snapshot.grid_xt,\n",
    "                      snapshot.grid_yt,\n",
    "                      snapshot,\n",
    "                      norm=norm,\n",
    "                      cmap=cmap,\n",
    "                      transform=ccrs.PlateCarree())\n",
    "        \n",
    "        # Plot storm center\n",
    "        ax.scatter(snapshot['center_lon'], snapshot['center_lat'], \n",
    "                   marker='x', c='k', s=50, zorder=20, transform=ccrs.PlateCarree())\n",
    "        \n",
    "        ax.set_aspect('equal')\n",
    "        ax.coastlines()\n",
    "\n",
    "        ''' End. '''\n",
    "    \n",
    "        cax = ax.inset_axes([1.03, 0, 0.02, 1])\n",
    "        cax.clear()\n",
    "        colorbar = fig.colorbar(matplotlib.cm.ScalarMappable(norm, cmap), cax=cax)\n",
    "    \n",
    "        gridlines = ax.gridlines(ls='--', alpha=0.5)\n",
    "        gridlines.bottom_labels = True\n",
    "        ax.coastlines()\n",
    "        \n",
    "        title_generator(model_name, snapshot, field_name, snapshot.time.values)\n",
    "        fig.tight_layout()\n",
    "    \n",
    "    number_of_frames = len(dataset.time.values) if len(dataset.time.values) < 100 else 12\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    anim = animation.FuncAnimation(fig, animate, frames=number_of_frames, init_func=init_func, interval=100, blit=False)\n",
    "    %matplotlib inline\n",
    "    return anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7924991d-e0a2-49da-a772-e67f793ecece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_animation(anim: matplotlib.animation.FuncAnimation,\n",
    "                   pathname: str,\n",
    "                   field_name: str,\n",
    "                   anomaly: bool=True):\n",
    "\n",
    "    ''' Method to save an animation object for a given TC showing a given field. '''\n",
    "    \n",
    "    filename = pathname.split('/')[-1]\n",
    "    model_name = filename.split('model_name-')[-1].split('.')[0]\n",
    "    experiment_name = filename.split('model_name-')[-1].split('.')[0]\n",
    "    animation_storm_ID = filename.split('storm_ID-')[-1].split('.')[0]\n",
    "    animation_max_wind = filename.split('max_wind-')[-1].split('.')[0]\n",
    "    animation_min_slp = filename.split('min_slp-')[-1].split('.')[0]\n",
    "    animation_basin = filename.split('basin-')[-1].split('.')[0]\n",
    "    anomaly_string = 'anomaly' if anomaly else 'raw'\n",
    "    animation_filename = f'animation.TC.model-{model_name}.experiment-{experiment_name}.storm_ID-{animation_storm_ID}.max_wind-{animation_max_wind}.min_slp-{animation_min_slp}.basin-{animation_basin}.field_name-{field_name}-{anomaly_string}.mp4'\n",
    "    animation_dirname = '/projects/GEOCLIM/gr7610/figs/TC-energy_budget/animations'\n",
    "\n",
    "    import matplotlib.animation as animation\n",
    "    Writer = animation.writers['ffmpeg']\n",
    "    writer = Writer(fps=12, metadata=dict(artist='Me'), bitrate=1800)\n",
    "    anim.save(os.path.join(animation_dirname, animation_filename), writer=writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d37b65-e747-4535-bf38-bfb6a5b99cad",
   "metadata": {},
   "source": [
    "#### Access data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d705beea-e73c-462e-8d71-07e527f267d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling data from filename /tigress/GEOCLIM/gr7610/analysis/tc_storage/individual_TCs/TC.model-ERA5.experiment-reanalysis.storm_ID-2004-206N20151.max_wind-44.min_slp-935.basin-WP.nc.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ERA5'\n",
    "experiment_name = 'reanalysis'\n",
    "year_range = (2001, 2005)\n",
    "\n",
    "intensity_parameter = 'min_slp'\n",
    "intensity_range = (0, 960)\n",
    "\n",
    "dirname = '/tigress/GEOCLIM/gr7610/analysis/tc_storage/individual_TCs'\n",
    "filenames = [filename for filename in os.listdir(dirname) if\n",
    "             model_name in filename and\n",
    "             experiment_name in filename and\n",
    "             min(year_range) <= get_filename_year(filename) < max(year_range) and\n",
    "             min(intensity_range) <= get_filename_intensity(filename, intensity_parameter) < max(intensity_range)]\n",
    "pathname = os.path.join(dirname, random.choice(filenames))\n",
    "\n",
    "print(f'Pulling data from filename {pathname}.')\n",
    "storm_reanalysis_data = xr.open_dataset(pathname, use_cftime=True)\n",
    "storm_reanalysis_data = utilities.field_correction(model_name, storm_reanalysis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0631ad5a-fd6d-468f-8dc4-2630e59e280f",
   "metadata": {},
   "source": [
    "#### Pull anomaly for the given TC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46da3cbb-8538-4d8a-bcad-e3d3b720b995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------\n",
      "Time-integrated TC anomaly over TC lifetime: -3.75e+14 W m$^{-2}$\n"
     ]
    }
   ],
   "source": [
    "field_name = 'olr'\n",
    "\n",
    "TC_anomaly_dataset = get_TC_anomaly(model_name, \n",
    "                                    experiment_name,\n",
    "                                    field_name,\n",
    "                                    year_range,\n",
    "                                    storm_reanalysis_data,\n",
    "                                    parallel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb34d632-fbd5-4f2d-836e-7e0a3a87883f",
   "metadata": {},
   "source": [
    "#### Show the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa3a9a5-8d54-42b5-9ecc-7348d76b8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "anim = animator(model_name,\n",
    "                TC_anomaly_dataset, \n",
    "                field_name=field_name,\n",
    "                anomaly=True)\n",
    "anim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c136b4-688d-4b82-862a-4f75dc723dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_animation(anim,\n",
    "               pathname,\n",
    "               field_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296ee872-a59f-4330-bea4-f608f8deef91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
