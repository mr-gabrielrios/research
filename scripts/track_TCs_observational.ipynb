{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7512a8be-c948-4484-a091-f746bb61f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy, numba\n",
    "# Local data storage packages\n",
    "import functools, os, pickle, collections, sys, importlib\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr, nc_time_axis\n",
    "xr.set_options(keep_attrs=True)\n",
    "# Visualization tools\n",
    "import cartopy, cartopy.crs as ccrs, matplotlib, matplotlib.pyplot as plt\n",
    "# Local imports\n",
    "import accessor, composite, composite_snapshots, derived, ibtracs, utilities, socket, visualization, tc_analysis, tc_processing, track_TCs, TC_tracker\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "importlib.reload(TC_tracker);\n",
    "importlib.reload(track_TCs);\n",
    "importlib.reload(ibtracs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c85199-725d-4aef-a40f-f2f57cc38414",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- IBTrACS data provides observational data that is more intense than ERA5 outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fb9a9d8-9cf0-4947-95f3-ea31c03bf30a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def access_IBTrACS_tracks(basin_name: str,\n",
    "                          date_range: tuple[str, str],\n",
    "                          intensity_parameter: str,\n",
    "                          intensity_range: tuple[int, int]) -> pd.DataFrame:\n",
    "\n",
    "    ''' Access IBTrACS data for a given basin, date range, and intensity bin. '''\n",
    "    \n",
    "    track_data = ibtracs.main(basin_name=basin_name,\n",
    "                              date_range=date_range,\n",
    "                              intensity_parameter=intensity_parameter,\n",
    "                              intensity_range=intensity_range)\n",
    "    \n",
    "    return track_data.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92113dee-9c99-4c08-b37f-77c2a00f57cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def access_observation_data(dataset_name: str, \n",
    "                            date_range: tuple[int, int],\n",
    "                            dirname: str|None=None) -> xr.Dataset:\n",
    "\n",
    "    ''' Obtain data for observational files within a given year range. '''\n",
    "\n",
    "    # Load a data directory containing MERRA files if one is not already given\n",
    "    if not dirname:\n",
    "        observation_dirnames = {'CERES': '/scratch/gpfs/GEOCLIM/gr7610/tiger3/reference/datasets/CERES',\n",
    "                                'MERRA': '/scratch/gpfs/GEOCLIM/gr7610/tiger3/reference/datasets/MERRA'}\n",
    "        assert dataset_name in observation_dirnames.keys(), f'Attempting to find directory for dataset {dataset_name}, but one is not available.'\n",
    "        dirname = observation_dirnames[dataset_name]\n",
    "    # Define lambda function to search for file dates\n",
    "    # Throw error if dataset not found\n",
    "    if dataset_name == 'MERRA':\n",
    "        find_date = lambda f: pd.to_datetime(f.split('.nc4')[0].split('.')[-1])\n",
    "    elif dataset_name == 'CERES':\n",
    "        find_date = lambda f: pd.to_datetime(f.split('Subset_')[-1].split('.')[0].split('-')[0])\n",
    "    else:\n",
    "        assert dataset_name in ['MERRA', 'CERES'], f'[access()] Cannot process datetimes for dataset {dataset_name}. Please define how to process this dataset or enter another dataset name.'\n",
    "    # List files in the directory that are netCDF files and are within the date range\n",
    "    filenames = [filename for filename in os.listdir(dirname) if\n",
    "                 '.nc' in filename and\n",
    "                  min(date_range) <= find_date(filename) <= max(date_range)]\n",
    "    # Generaet full pathnames\n",
    "    pathnames = [os.path.join(dirname, filename) for filename in filenames]\n",
    "    # Load the dataset\n",
    "    dataset = xr.open_mfdataset(pathnames, engine='netcdf4')\n",
    "\n",
    "    # Adjust coordinates for MERRA: map (-180, 180) to (0, 360)\n",
    "    if dataset_name == 'MERRA':\n",
    "        dataset['lon'] = dataset['lon'].where(dataset['lon'] > 0, dataset['lon'] + 360)\n",
    "        dataset = dataset.sortby('lon')\n",
    "        \n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0842d198-4969-45d2-8fb4-552ab0ec0578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_observation_fields(dataset: xr.Dataset,\n",
    "                              dataset_name: str) -> xr.Dataset:\n",
    "\n",
    "    ''' Rename fields to match GFDL GCM output data variable names. '''\n",
    "    \n",
    "    field_names = {'MERRA': {'SWTDN': 'swdn_toa',\n",
    "                             'SWTNT': 'swabs_toa',\n",
    "                             'SWGDN': 'swdn_sfc',\n",
    "                             'SWGNT': 'swnet_sfc',\n",
    "                             'LWGAB': 'lwdn_sfc',\n",
    "                             'LWGEM': 'lwup_sfc',\n",
    "                             'LWGNT': 'lwnet_sfc',\n",
    "                             'LWTUP': 'olr',\n",
    "                             'lon': 'longitude',\n",
    "                             'lat': 'latitude'},\n",
    "                   'CERES': {'toa_solar_all_1h': 'swdn_toa',\n",
    "                             'adj_atmos_sw_up_all_toa_1h': 'swup_toa',\n",
    "                             'toa_sw_all_1h': 'swabs_toa',\n",
    "                             'toa_net_all_1h': 'netrad_toa',\n",
    "                             'adj_atmos_sw_down_all_surface_1h': 'swdn_sfc',\n",
    "                             'adj_atmos_sw_up_all_surface_1h': 'swup_sfc',\n",
    "                             'adj_atmos_lw_down_all_surface_1h': 'lwdn_sfc',\n",
    "                             'adj_atmos_lw_up_all_surface_1h': 'lwup_sfc',\n",
    "                             'toa_lw_all_1h': 'olr',\n",
    "                             'lon': 'longitude',\n",
    "                             'lat': 'latitude'}}\n",
    "\n",
    "    assert dataset_name in field_names.keys(), f'Field names not found for dataset {dataset_name}.'\n",
    "\n",
    "    # Rename the data variables\n",
    "    dataset = dataset.rename(field_names[dataset_name])\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f117ab2a-0d10-4c7c-b935-bd049d7d9bb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def derive_fields(dataset: xr.Dataset,\n",
    "                  dataset_name: str) -> xr.Dataset:\n",
    "\n",
    "    ''' Derive fields from input data. '''\n",
    "\n",
    "    # Define fields to derive and the fields required to calculate them\n",
    "    derived_field_definitions = {'MERRA': {'netrad_toa': ['swabs_toa', 'olr'],\n",
    "                                           'swup_sfc': ['swabs_sfc', 'swdn_sfc'],\n",
    "                                           'swdn_sfc': ['swabs_toa', 'swdn_toa'],\n",
    "                                           'net_lw': ['lwup_sfc', 'lwdn_sfc', 'olr'],\n",
    "                                           'net_sw': ['swup_sfc', 'swdn_sfc', 'swabs_toa']},\n",
    "                                'CERES': {'net_lw': ['lwup_sfc', 'lwdn_sfc', 'olr'],\n",
    "                                          'net_sw': ['swup_sfc', 'swdn_sfc', 'swabs_toa']}}\n",
    "\n",
    "    assert dataset_name in derived_field_definitions.keys(), f'Field names not found for dataset {dataset_name}.'\n",
    "    \n",
    "    # Ensure required fields are loaded\n",
    "    for derived_field, required_fields in derived_field_definitions[dataset_name].items():\n",
    "        assert [required_field in dataset.data_vars for required_field in required_fields], f'Fields {required_fields} must be in dataset to derived {derived_field}.'\n",
    "\n",
    "    if dataset_name == 'MERRA':\n",
    "        # Derive net radiation (net downward shortwave at TOA - upwelling longwave at TOA)\n",
    "        dataset['netrad_toa'] = dataset['swabs_toa'] - dataset['olr']\n",
    "        # Derive upwards shortwave flux at surface. Flip sign such that upwards values are positive.\n",
    "        dataset['swup_sfc'] = -1 * (dataset['swnet_sfc'] - dataset['swdn_sfc'])\n",
    "        # Derive upwards shortwave flux at surface. Flip sign such that upwards values are positive.\n",
    "        dataset['swup_toa'] = -1 * (dataset['swabs_toa'] - dataset['swdn_toa'])\n",
    "        # Derive column net longwave flux, with positive equaling heating into the atmosphere.\n",
    "        dataset['net_lw'] = dataset['lwup_sfc'] - dataset['lwdn_sfc'] - dataset['olr']\n",
    "        # Derive column net shortwave flux, with positive equaling heating into the atmosphere.\n",
    "        dataset['net_sw'] = dataset['swup_sfc'] - dataset['swdn_sfc'] + dataset['swabs_toa']\n",
    "    elif dataset_name == 'CERES':\n",
    "        # Derive column net longwave flux, with positive equaling heating into the atmosphere.\n",
    "        dataset['net_lw'] = dataset['lwup_sfc'] - dataset['lwdn_sfc'] - dataset['olr']\n",
    "        # Derive column net shortwave flux, with positive equaling heating into the atmosphere.\n",
    "        dataset['net_sw'] = dataset['swup_sfc'] - dataset['swdn_sfc'] + dataset['swabs_toa']\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f383309-243b-4c3a-af08-12189d7b7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_timestamps(dataset: xr.Dataset) -> xr.Dataset:\n",
    "\n",
    "    ''' Adjust timestamps to `pandas` format. '''\n",
    "    \n",
    "    # Ensure timestamps are selected to the nearest hour\n",
    "    dataset['time'] = [datetime.datetime(year=timestamp.dt.year.item(),\n",
    "                                         month=timestamp.dt.month.item(),\n",
    "                                         day=timestamp.dt.day.item(),\n",
    "                                         hour=timestamp.dt.hour.item()) for timestamp in dataset.time]\n",
    "    # Convert to Pandas objects\n",
    "    dataset['time'] = [pd.to_datetime(timestamp) for timestamp in dataset.time.values]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42806eb2-a98a-4b5b-8c88-29d4c2e7b0bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def adjust_grid(dataset: xr.Dataset,\n",
    "                target_resolution: float|None=None) -> xr.Dataset:\n",
    "\n",
    "    ''' Make grid square if not already square by interpolation. '''\n",
    "    # Get differential in grid spacing in each direction\n",
    "    zonal_grid_spacing = np.unique(dataset['longitude'].diff('longitude').values)   \n",
    "    meridional_grid_spacing = np.unique(dataset['latitude'].diff('latitude').values)\n",
    "    # Check if differentials are equal for all cells\n",
    "    check_grid_spacing = np.all(np.isclose(zonal_grid_spacing, zonal_grid_spacing[0])) \n",
    "    check_meridional_grid_spacing = np.all(np.isclose(meridional_grid_spacing, meridional_grid_spacing[0]))\n",
    "    # Ensure grids are regularly-spaced\n",
    "    assert check_grid_spacing & check_meridional_grid_spacing, 'Grid is irregularly spaced.'\n",
    "    \n",
    "    # Get smaller of two resolutions, if target not provided\n",
    "    if not target_resolution:\n",
    "        target_resolution = min(zonal_grid_spacing[0], meridional_grid_spacing[0])\n",
    "        \n",
    "    # Define basis vectors for interpolation\n",
    "    zonal_basis_vector = np.arange(0, 360, target_resolution)\n",
    "    meridional_basis_vector = np.arange(-90, 90, target_resolution)\n",
    "\n",
    "    # Interpolate grid\n",
    "    dataset = dataset.interp(longitude=zonal_basis_vector).interp(latitude=meridional_basis_vector)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3b9ec8c-a114-44c2-b540-958d021211da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def align_timestamps(TC_track_dataset: pd.DataFrame,\n",
    "                     observation_dataset: xr.Dataset) -> list:\n",
    "    \n",
    "    # Select random timestamps from each dataset to ensure they are the same\n",
    "    # Assume all timestamps within a dataset have the same timestamp type\n",
    "    random_TC_timestamp = random.choice(TC_track_dataset.time.values)\n",
    "    random_observation_timestamp = random.choice(observation_dataset.time.values)\n",
    "\n",
    "    # Ensure timestamps are identically-typed\n",
    "    check_timestamp_formats = type(random_TC_timestamp) == type(random_observation_timestamp)\n",
    "    assert check_timestamp_formats, f'Timestamp types between IBTrACS and reanalysis require alignment. IBTrACS timestamp: {random_TC_timestamp}; observational timestamp: {random_observation_timestamp}'\n",
    "    \n",
    "    # Iterate through storm timestamps to make sure they are in the reanalysis data\n",
    "    observation_TC_timestamps = [TC_timestamp for TC_timestamp in TC_track_dataset.time.values if\n",
    "                                 TC_timestamp in observation_dataset.time.values]\n",
    "\n",
    "    return observation_TC_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bcc8c1e-304b-47ef-8bd9-d889213f13ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def observation_GFDL_compatibility_adjustments(TC_observation_dataset: xr.Dataset) -> xr.Dataset:\n",
    "\n",
    "    ''' Perform adjustments so reanalysis data can use the same conventions as GFDL output data. '''\n",
    "\n",
    "    # Rename spatial basis vector coordinate names\n",
    "    TC_observation_dataset = TC_observation_dataset.rename({'longitude': 'grid_xt', 'latitude': 'grid_yt'})\n",
    "\n",
    "    # Perform deep copy for coordinate value modification\n",
    "    TC_observation_dataset_reformatted = TC_observation_dataset.copy(deep=True)\n",
    "\n",
    "    # Adjust timestamp format to cftime on the xArray Dataset.\n",
    "    pd_timestamps = pd.to_datetime(TC_observation_dataset.time) # convert to Pandas objects for easier indexing\n",
    "    cftime_timestamps = [cftime.datetime(year=timestamp.year, \n",
    "                                         month=timestamp.month,\n",
    "                                         day=timestamp.day,\n",
    "                                         hour=timestamp.hour,\n",
    "                                         calendar='julian') for timestamp in pd_timestamps]\n",
    "    TC_observation_dataset_reformatted['time'] = cftime_timestamps\n",
    "    assert 'cftime' in str(type(TC_observation_dataset_reformatted['time'].values[0])), f'[observation_GFDL_compatibility_adjustments()] Timestamp is not a cftime object.' \n",
    "\n",
    "    return TC_observation_dataset_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f4f1a22-b45e-4225-a67b-5c881907a7be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_TC_coordinates(TC_track_dataset: pd.DataFrame,\n",
    "                       observation_dataset: xr.Dataset,\n",
    "                       observation_TC_timestamps: list,\n",
    "                       observation_resolution: float) -> dict:\n",
    "\n",
    "    interval_round = lambda x, y: y * round(x / y) # round coordinates to nearest dataset coordinates\n",
    "    \n",
    "    # Initialize dictionary for storm track coordinates\n",
    "    TC_track_coordinates = {}\n",
    "    # Construct dictionary for coordinates pertaining to each storm timestamp\n",
    "    for observation_TC_timestamp in observation_TC_timestamps:\n",
    "        # Obtain longitude and latitude for each timestamp\n",
    "        TC_track_longitude = TC_track_dataset['center_lon'].loc[TC_track_dataset['time'] == observation_TC_timestamp]\n",
    "        TC_track_latitude = TC_track_dataset['center_lat'].loc[TC_track_dataset['time'] == observation_TC_timestamp]\n",
    "        # Round coordinates to align with dataset coordinate system and resolution\n",
    "        TC_track_coordinates[observation_TC_timestamp] = {'lon': interval_round(TC_track_longitude.item(), observation_resolution),\n",
    "                                                          'lat': interval_round(TC_track_latitude.item(), observation_resolution)}\n",
    "\n",
    "    return TC_track_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fe31ca-cef6-4b44-8f1e-64d5b0b5c084",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def observation_grid_redefinition(TC_track_coordinates: dict,\n",
    "                                  observation_resolution: float,\n",
    "                                  coarsen_factor: int,\n",
    "                                  observation_TC_window_size: int|float):\n",
    "\n",
    "    ''' Generate a consistent grid for reanalysis data to allow for all timestamps to be interpolated to the same grid. '''\n",
    "\n",
    "    # Coarsening factor\n",
    "    interpolation_resolution = observation_resolution * coarsen_factor\n",
    "    \n",
    "    # Define storm spatial extents for future interpolation\n",
    "    minimum_longitude = np.min([entry['lon'] for entry in TC_track_coordinates.values()])\n",
    "    minimum_latitude = np.min([entry['lat'] for entry in TC_track_coordinates.values()])\n",
    "    maximum_longitude = np.max([entry['lon'] for entry in TC_track_coordinates.values()])\n",
    "    maximum_latitude = np.max([entry['lat'] for entry in TC_track_coordinates.values()])\n",
    "    \n",
    "    # Define basis vectors for data interpolation\n",
    "    # Subtract and add window sizes to minima and maxima, respectively, to capture full desired extent\n",
    "    zonal_basis_vector = np.arange(minimum_longitude - observation_TC_window_size, \n",
    "                                   maximum_longitude + observation_TC_window_size, interpolation_resolution)\n",
    "    meridional_basis_vector = np.arange(minimum_latitude - observation_TC_window_size, \n",
    "                                        maximum_latitude + observation_TC_window_size, interpolation_resolution)\n",
    "\n",
    "    return zonal_basis_vector, meridional_basis_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dae8965e-84a3-433e-aebe-18fca10a5297",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_observation_TC_timestamp(TC_track_coordinates: dict,\n",
    "                                  observation_TC_window_size: int | float,\n",
    "                                  observation_resolution: float,\n",
    "                                  observation_dataset: xr.Dataset,\n",
    "                                  zonal_basis_vector: np.array,\n",
    "                                  meridional_basis_vector: np.array,\n",
    "                                  TC_timestamp: cftime.datetime):\n",
    "\n",
    "    ''' \n",
    "    Method to link track data and reanalysis data for a single timestamp. \n",
    "    This is compartmentalized to allow for straightforward parallelization.\n",
    "    '''\n",
    "\n",
    "    # Define reanalysis dataset coordinate names\n",
    "    grid_xt = 'longitude'\n",
    "    grid_yt = 'latitude'\n",
    "    \n",
    "    # Initialize container dictionaries\n",
    "    observation_TC_container = {}\n",
    "    observation_TC_extent = {}\n",
    "    observation_TC_extent[TC_timestamp] = {}\n",
    "    \n",
    "    # Generate trimming window extents for each timestamp.\n",
    "    # Window extents are defined as: \n",
    "    # 'grid_xt' = (longitude - window_extent, longitude + window_extent), \n",
    "    # 'grid_yt' = (latitude - window_extent, latitude + window_extent)\n",
    "\n",
    "    # print(f\"Coordinates at {TC_timestamp}: {TC_track_coordinates[TC_timestamp]['lon']}, {TC_track_coordinates[TC_timestamp]['lat']}\")\n",
    "    \n",
    "    # Assign zonal window\n",
    "    observation_TC_extent[TC_timestamp][grid_xt] = np.arange(TC_track_coordinates[TC_timestamp]['lon'] - observation_TC_window_size,\n",
    "                                                             TC_track_coordinates[TC_timestamp]['lon'] + observation_TC_window_size + observation_resolution,\n",
    "                                                             observation_resolution)\n",
    "    # Assign meridional window\n",
    "    observation_TC_extent[TC_timestamp][grid_yt] = np.arange(TC_track_coordinates[TC_timestamp]['lat'] - observation_TC_window_size,\n",
    "                                                             TC_track_coordinates[TC_timestamp]['lat'] + observation_TC_window_size + observation_resolution,\n",
    "                                                             observation_resolution)\n",
    "    # Extract GCM data for the given timestamp and spatial extent\n",
    "    # Notice the modulo on `grid_xt` - this is used to handle Prime Meridian bugs\n",
    "    observation_TC_container[TC_timestamp] = observation_dataset.sel(time=TC_timestamp)\n",
    "    observation_TC_container[TC_timestamp] = observation_TC_container[TC_timestamp].sel({grid_xt: observation_TC_extent[TC_timestamp][grid_xt] % 360})\n",
    "    observation_TC_container[TC_timestamp] = observation_TC_container[TC_timestamp].sel({grid_yt: observation_TC_extent[TC_timestamp][grid_yt]})\n",
    "\n",
    "    # Interpolate to different resolution (shoot for 0.5 degrees)\n",
    "    observation_TC_container[TC_timestamp] = observation_TC_container[TC_timestamp].interp(longitude=zonal_basis_vector).interp(latitude=meridional_basis_vector)\n",
    "    \n",
    "    return observation_TC_container[TC_timestamp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "319f50af-12f1-4bcc-8921-927ac63a2bdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_observation_TC(TC_track_coordinates: dict,\n",
    "                        TC_timestamps: list,\n",
    "                        observation_data: xr.Dataset,\n",
    "                        observation_resolution: float,\n",
    "                        target_resolution: float,\n",
    "                        observation_TC_window_size: int|float,\n",
    "                        parallel: bool,\n",
    "                        diagnostic: bool=False):\n",
    "\n",
    "    ''' Method to load observation data for a given TC given its track coordinates, track timestamps, and reanalysis data. '''\n",
    "    \n",
    "    # Get basis vectors for reanalysis data generation\n",
    "    coarsen_factor = int(np.round(target_resolution / observation_resolution)) # factor by which observation data will be coarsened to match a target resolution\n",
    "    zonal_basis_vector, meridional_basis_vector = observation_grid_redefinition(TC_track_coordinates,\n",
    "                                                                                observation_resolution,\n",
    "                                                                                coarsen_factor,\n",
    "                                                                                observation_TC_window_size)\n",
    "\n",
    "    # Initialize a container to hold GCM output connected to each storm timestamp and the corresponding spatial extent\n",
    "    observation_TC_container = {}\n",
    "    # Define partial function to streamline function calls, since the only variable argument is `storm_timestamps`\n",
    "    partial_load_observation_TC_timestamp = functools.partial(load_observation_TC_timestamp,\n",
    "                                                                 TC_track_coordinates,\n",
    "                                                                 observation_TC_window_size,\n",
    "                                                                 observation_resolution,\n",
    "                                                                 observation_data,\n",
    "                                                                 zonal_basis_vector,\n",
    "                                                                 meridional_basis_vector)\n",
    "    # Keep time for profiling\n",
    "    start_time = time.time()\n",
    "    # Initialize container dictionary\n",
    "    observation_TC_container = {}\n",
    "    # Iterate over all timestamps to find reanalysis data for the given entry\n",
    "    for TC_timestamp in TC_timestamps:\n",
    "        observation_TC_container[TC_timestamp] = partial_load_observation_TC_timestamp(TC_timestamp)\n",
    "    # Concatenate all GCM output data corresponding to storm into a single xArray Dataset\n",
    "    observation_TC_data = xr.concat(observation_TC_container.values(), dim='time').sortby('time')\n",
    "\n",
    "    if diagnostic:\n",
    "        print(f'Elapsed time to load observation storm: {(time.time() - start_time):.2f} s.')\n",
    "        print(f'\\t per timestamp: {((time.time() - start_time) / len(TC_timestamps)):.2f} s.')\n",
    "\n",
    "    return observation_TC_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "712dad54-30a7-4b79-9e4b-1b1d994b5576",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def observation_TC_generator(dataset_name: str,\n",
    "                             observation_track_dataset: pd.DataFrame,\n",
    "                             observation_dataset: xr.Dataset,\n",
    "                             observation_resolution: float,\n",
    "                             target_resolution: float,\n",
    "                             observation_TC_window_size: int,\n",
    "                             parallel: bool,\n",
    "                             storm_ID: str|None=None):\n",
    "\n",
    "    ''' Method to perform all steps related to binding corresponding GFDL QuickTracks and GCM model output together for a given TC. '''\n",
    "\n",
    "    print(f'[storm_generator] Processing storm ID {storm_ID}...')\n",
    "\n",
    "    # 4. Find a candidate storm from the track data, ensure it is ordered by time\n",
    "    TC_track_dataset = TC_tracker.pick_storm(observation_track_dataset, \n",
    "                                             selection_method='storm_number', \n",
    "                                             storm_ID=storm_ID).sort_values('time')\n",
    "    # 5. Pull storm-specific timestamps\n",
    "    TC_track_timestamps = align_timestamps(TC_track_dataset, observation_dataset)\n",
    "    if len(TC_track_timestamps) == 0:\n",
    "        print(f\"No matching timestamps were found for storm {TC_track_dataset['storm_id'].unique().item()}. Exiting this storm.\")\n",
    "        return\n",
    "    # 6. Pull storm-specific coordinates to align with track timestamps\n",
    "    TC_track_coordinates = get_TC_coordinates(TC_track_dataset, \n",
    "                                              observation_dataset,\n",
    "                                              TC_track_timestamps, \n",
    "                                              observation_resolution)\n",
    "    # 7. Load reanalysis data for the iterand storm to align with track data\n",
    "    observation_TC_dataset = load_observation_TC(TC_track_coordinates, \n",
    "                                                 TC_track_timestamps,\n",
    "                                                 observation_dataset,\n",
    "                                                 observation_resolution,\n",
    "                                                 target_resolution,\n",
    "                                                 observation_TC_window_size,\n",
    "                                                 parallel=parallel)\n",
    "    # 8. Append information from track data to object containing reanalysis output.\n",
    "    observation_TC_dataset = TC_tracker.join_track_GCM_data(storm_track_data=TC_track_dataset,\n",
    "                                                            storm_gcm_data=observation_TC_dataset,\n",
    "                                                            storm_time_variable='time')\n",
    "    # 9. Derive fields present in GCM output data but not directly provided in ERA5 data\n",
    "    observation_TC_dataset = derive_fields(observation_TC_dataset, dataset_name)\n",
    "    # 10. Perform adjustments for compatibility with GFDL GCM outputs\n",
    "    observation_TC_dataset = observation_GFDL_compatibility_adjustments(observation_TC_dataset)\n",
    "    # 11. Save xArray Dataset to netCDF file\n",
    "    TC_tracker.save_storm_netcdf(observation_TC_dataset, model_name=dataset_name, experiment_name='OBS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "85aa8cef-3d2c-415b-98b6-6240ddca3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset_name: str,\n",
    "         date_range: tuple[str, str],\n",
    "         basin_name: str='global',\n",
    "         intensity_parameter: str='min_slp',\n",
    "         intensity_range: tuple[int | float, int | float]=(980, 1000),\n",
    "         number_of_storms: int=1,\n",
    "         observation_resolution: float=0.5,\n",
    "         target_resolution: float=0.5,\n",
    "         observation_TC_window_size: int|float=12,\n",
    "         parallel: bool=True):\n",
    "\n",
    "    # 1. Pull track data for a given date range, basin, and intensity range\n",
    "    print('Loading TC tracks from IBTrACS.')\n",
    "    TC_track_dataset = access_IBTrACS_tracks(date_range=date_range, \n",
    "                                             basin_name=basin_name, \n",
    "                                             intensity_parameter=intensity_parameter, \n",
    "                                             intensity_range=intensity_range)\n",
    "    # 2. Access reanalysis dataset. Ensure this is done lazily to avoid excessive memory usage.\n",
    "    print(f'Loading observational dataset for {dataset_name}.')\n",
    "    date_range = [pd.to_datetime(date) for date in date_range]\n",
    "    observation_dataset = access_observation_data(dataset_name,\n",
    "                                                  date_range)\n",
    "    # 2a. Perform observation dataset renaming field correction\n",
    "    print('Correcting field names.')\n",
    "    observation_dataset = rename_observation_fields(observation_dataset,\n",
    "                                                    dataset_name=dataset_name)\n",
    "    # 2b. Perform observation dataset timestamp adjustments\n",
    "    print('Adjusting timestamp conventions.')\n",
    "    observation_dataset = adjust_timestamps(observation_dataset)\n",
    "    # 2c. Adjust grid to ensure the resolution is equal in both axes\n",
    "    print('Interpolating grid.')\n",
    "    observation_dataset = adjust_grid(observation_dataset, observation_resolution)\n",
    "    # 3. Obtain N randomized storm IDs from the filtered track data, where 'N' is `number_of_storms`\n",
    "    storm_IDs = TC_tracker.pick_storm_IDs(TC_track_dataset, number_of_storms)\n",
    "    \n",
    "    # Define partial function to allow for using Pool.map since `track_data` is equivalent for all subprocesses\n",
    "    partial_observation_TC_generator = functools.partial(observation_TC_generator, \n",
    "                                                         dataset_name,\n",
    "                                                         TC_track_dataset, \n",
    "                                                         observation_dataset,\n",
    "                                                         observation_resolution,\n",
    "                                                         target_resolution,\n",
    "                                                         observation_TC_window_size,\n",
    "                                                         parallel)\n",
    "    # Load storms in parallel    \n",
    "    if parallel:\n",
    "        with Pool() as pool:\n",
    "            pool.map(partial_observation_TC_generator, storm_IDs)\n",
    "            pool.close()\n",
    "    else:\n",
    "        for storm_ID in storm_IDs:\n",
    "            partial_observation_TC_generator(storm_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24e39dfa-f5c5-46ba-9213-5905daf3a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TC tracks from IBTrACS.\n",
      "Loading observational dataset for CERES.\n",
      "Correcting field names.\n",
      "Adjusting timestamp conventions.\n",
      "Interpolating grid.\n",
      "[storm_generator] Processing storm ID 2019-218N21088...\n",
      "[storm_generator] Processing storm ID 2019-242N18126...[storm_generator] Processing storm ID 2019-176N18128...\n",
      "\n",
      "[storm_generator] Processing storm ID 2019-272N14261...\n",
      "[storm_generator] Processing storm ID 2019-260N10253...\n",
      "[storm_generator] Processing storm ID 2019-291N22264...\n",
      "No matching timestamps were found for storm 2019-291N22264. Exiting this storm.\n",
      "[storm_generator] Processing storm ID 2019-236N08143...\n",
      "[storm_generator] Processing storm ID 2019-126S05129...\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-176N18128.max_wind-21.min_slp-994.basin-WP.nc\n",
      "[storm_generator] Processing storm ID 2019-001S10162...\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-242N18126.max_wind-18.min_slp-996.basin-WP.nc\n",
      "[storm_generator] Processing storm ID 2019-129S10160...\n",
      "[storm_generator] Processing storm ID 2019-177N15256...\n",
      "[storm_generator] Processing storm ID 2019-260N12262...\n",
      "[storm_generator] Processing storm ID 2019-093S07133...\n",
      "[storm_generator] Processing storm ID 2019-182N18114...\n",
      "[storm_generator] Processing storm ID 2018-362S13147...\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-236N08143.max_wind-21.min_slp-992.basin-WP.nc\n",
      "[storm_generator] Processing storm ID 2019-335N05058...\n",
      "[storm_generator] Processing storm ID 2019-299N25265...\n",
      "No matching timestamps were found for storm 2019-299N25265. Exiting this storm.\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-218N21088.max_wind-15.min_slp-988.basin-NI.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-272N14261.max_wind-23.min_slp-997.basin-EP.nc\n",
      "[storm_generator] Processing storm ID 2019-018S24033...\n",
      "[storm_generator] Processing storm ID 2019-195N13136...\n",
      "[storm_generator] Processing storm ID 2019-322N11144...\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-182N18114.max_wind-18.min_slp-992.basin-WP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-322N11144.max_wind-28.min_slp-990.basin-WP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-001S10162.max_wind-26.min_slp-985.basin-SP.nc[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-177N15256.max_wind-33.min_slp-992.basin-EP.nc\n",
      "\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-126S05129.max_wind-21.min_slp-996.basin-SP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-260N12262.max_wind-39.min_slp-985.basin-EP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-018S24033.max_wind-18.min_slp-995.basin-ET.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-260N10253.max_wind-31.min_slp-991.basin-EP.nc[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-195N13136.max_wind-23.min_slp-985.basin-WP.nc\n",
      "\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-335N05058.max_wind-21.min_slp-998.basin-NI.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-129S10160.max_wind-28.min_slp-990.basin-SP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2019-093S07133.max_wind-28.min_slp-986.basin-SP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-CERES.experiment-OBS.storm_ID-2018-362S13147.max_wind-26.min_slp-987.basin-SP.nc\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-176N18128.max_wind-21.min_slp-994.basin-WP.nc: 9.09 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-176N18128.max_wind-21.min_slp-994.basin-WP.nc: 1.07 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-242N18126.max_wind-18.min_slp-996.basin-WP.nc: 9.26 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-242N18126.max_wind-18.min_slp-996.basin-WP.nc: 1.17 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-236N08143.max_wind-21.min_slp-992.basin-WP.nc: 12.14 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-236N08143.max_wind-21.min_slp-992.basin-WP.nc: 3.18 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-182N18114.max_wind-18.min_slp-992.basin-WP.nc: 11.79 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-182N18114.max_wind-18.min_slp-992.basin-WP.nc: 2.84 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-272N14261.max_wind-23.min_slp-997.basin-EP.nc: 13.53 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-272N14261.max_wind-23.min_slp-997.basin-EP.nc: 7.23 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-177N15256.max_wind-33.min_slp-992.basin-EP.nc: 14.60 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-177N15256.max_wind-33.min_slp-992.basin-EP.nc: 10.65 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-218N21088.max_wind-15.min_slp-988.basin-NI.nc: 21.34 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-218N21088.max_wind-15.min_slp-988.basin-NI.nc: 7.54 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-322N11144.max_wind-28.min_slp-990.basin-WP.nc: 22.67 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-322N11144.max_wind-28.min_slp-990.basin-WP.nc: 4.02 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-001S10162.max_wind-26.min_slp-985.basin-SP.nc: 24.50 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-001S10162.max_wind-26.min_slp-985.basin-SP.nc: 9.92 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-260N12262.max_wind-39.min_slp-985.basin-EP.nc: 24.34 sElapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-126S05129.max_wind-21.min_slp-996.basin-SP.nc: 24.61 s\n",
      "\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-126S05129.max_wind-21.min_slp-996.basin-SP.nc: 8.12 MB\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-260N12262.max_wind-39.min_slp-985.basin-EP.nc: 15.25 MB\n",
      "\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-195N13136.max_wind-23.min_slp-985.basin-WP.nc: 24.22 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-195N13136.max_wind-23.min_slp-985.basin-WP.nc: 9.25 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-260N10253.max_wind-31.min_slp-991.basin-EP.nc: 24.40 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-260N10253.max_wind-31.min_slp-991.basin-EP.nc: 19.19 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-018S24033.max_wind-18.min_slp-995.basin-ET.nc: 24.63 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-018S24033.max_wind-18.min_slp-995.basin-ET.nc: 7.41 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-335N05058.max_wind-21.min_slp-998.basin-NI.nc: 24.34 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-335N05058.max_wind-21.min_slp-998.basin-NI.nc: 12.81 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2018-362S13147.max_wind-26.min_slp-987.basin-SP.nc: 24.32 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2018-362S13147.max_wind-26.min_slp-987.basin-SP.nc: 26.42 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-129S10160.max_wind-28.min_slp-990.basin-SP.nc: 27.20 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-129S10160.max_wind-28.min_slp-990.basin-SP.nc: 32.78 MB\n",
      "\n",
      "Elapsed loading time for TC.model-CERES.experiment-OBS.storm_ID-2019-093S07133.max_wind-28.min_slp-986.basin-SP.nc: 27.44 s\n",
      "File size for TC.model-CERES.experiment-OBS.storm_ID-2019-093S07133.max_wind-28.min_slp-986.basin-SP.nc: 31.72 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_range = ('2019-01-01', '2020-01-01')\n",
    "basin_name = 'global'\n",
    "intensity_parameter = 'min_slp'\n",
    "intensity_range = (980, 1000)\n",
    "observation_dataset_name = 'CERES'\n",
    "\n",
    "number_of_storms = 20\n",
    "\n",
    "main(dataset_name=observation_dataset_name, \n",
    "     basin_name=basin_name, \n",
    "     date_range=date_range, \n",
    "     intensity_parameter=intensity_parameter,\n",
    "     intensity_range=intensity_range, \n",
    "     number_of_storms=number_of_storms,\n",
    "     parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfefb07-600b-470e-a685-5c0b988e2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = '/projects/GEOCLIM/gr7610/analysis/tc_storage/individual_TCs'\n",
    "filename = 'TC.model-CERES.experiment-OBS.storm_ID-2019-255N15251.max_wind-59.min_slp-950.basin-EP.nc'\n",
    "pathname = os.path.join(dirname, filename)\n",
    "dataset = xr.open_dataset(pathname)\n",
    "fig, ax = plt.subplots(figsize=(8, 3.5))\n",
    "dataset['olr'].isel(time=13).plot(ax=ax)\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0d890-8aec-492d-b26f-aa88e6853cb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
