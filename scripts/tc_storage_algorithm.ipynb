{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c07d45bf-dfcf-416b-8fa3-32ece4e664be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Import packages. '''\n",
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy\n",
    "# Local data storage packages\n",
    "import dill, os, pickle\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7bfbc3dc-217d-4eea-a6c6-2683781723cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tc_storage(model, experiment, year_range, storm_type, benchmarking=True):\n",
    "    \n",
    "    ''' \n",
    "    Function to build unified xArray Dataset to hold model and track data for tracked TCs in a given model and experiment.\n",
    "    \n",
    "    Input(s):\n",
    "    - model (str):                name of model to access (typically \"AM2.5\" or \"HIRAM\")\n",
    "    - experiment (str):           name of experiment to access (typically \"control\", \"ktc\", \"plus2K\", etc.)\n",
    "    - year_range (tuple of ints): 2-element tuple with a start and end year\n",
    "    - storm_type (str):           type of storm to evaluate from TC tracks data (\"TS\" for all storms or \"C15w\" for hurricanes)\n",
    "    - benchmarking (bool):        boolean to enable time benchmarking\n",
    "    Output(s):\n",
    "    - dirs (tuple):               2-element tuple with pathnames for the model- and experiment-specific GCM and track directories.\n",
    "    '''\n",
    "    \n",
    "    # Define paths to model- and experiment-specific data\n",
    "    model_dir, track_dir = dirnames(model, experiment)\n",
    "    # Define date range\n",
    "    date_range = [cftime.DatetimeNoLeap(year=year, month=1, day=1, hour=0) for year in range(min(year_range), max(year_range))]\n",
    "    # Iterate over years. This is performed because data is saved into year-specific files.\n",
    "    # for year in date_range:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "969fb8ac-4279-481e-bceb-0f0065cdaf44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dirnames(model, experiment):\n",
    "\n",
    "    ''' \n",
    "    Function to store pathnames for selected models and experiments. \n",
    "    \n",
    "    Input(s):\n",
    "    - model (str):  name of model to access (typically \"AM2.5\" or \"HIRAM\").\n",
    "    Output(s):\n",
    "    - dirs (tuple): 2-element tuple with pathnames for the model- and experiment-specific GCM and track directories.\n",
    "    '''\n",
    "    \n",
    "    # Define experiment-specific pathnames\n",
    "    experiments = {'control': {'AM2.5': 'CTL1990s_tigercpu_intelmpi_18_540PE',\n",
    "                               'HIRAM': 'CTL1990s_v201910_tigercpu_intelmpi_18_540PE'},\n",
    "                   'ktc': {'AM2.5': 'CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE',\n",
    "                           'HIRAM': 'CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE'}}\n",
    "    # Model directories\n",
    "    model_dirs = {'AM2.5': {'control': '/tigress/wenchang/analysis/TC/AM2.5/CTL1990s_tigercpu_intelmpi_18_540PE/model_out/POSTP',\n",
    "                            'ktc': '/tigress/wenchang/analysis/TC/AM2.5ktc2/CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE/modelout/POSTP'},\n",
    "                  'HIRAM': {'control': '/tigress/wenchang/analysis/TC/HIRAM/CTL1990s_v201910_tigercpu_intelmpi_18_540PE/model_out/POSTP',\n",
    "                            'ktc': '/tigress/wenchang/analysis/TC/HIRAMktc2/CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE/modelout/POSTP'}}\n",
    "    # Track directories\n",
    "    # Potential issue: ask Wenchang about using track .nc files in the same parent directory tree as the model data\n",
    "    track_dirs = {'AM2.5': {'control': '/tigress/wenchang/MODEL_OUT/AM2.5/CTL1990s_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k',\n",
    "                            'ktc': '/tigress/wenchang/MODEL_OUT/AM2.5ktc2/CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k'},\n",
    "                  'HIRAM': {'control': '/tigress/wenchang/MODEL_OUT/HIRAM/CTL1990s_v201910_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_2p5C_330k',\n",
    "                            'ktc': '/tigress/wenchang/MODEL_OUT/HIRAMktc2/CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k'}}\n",
    "\n",
    "    dirs = (model_dirs[model][experiment], track_dirs[model][experiment])\n",
    "    \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "0d4303b6-d456-46b3-b84b-45090ebe2bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lmh_parser(path):\n",
    "    \n",
    "    ''' \n",
    "    This method parses through text files from Lucas Harris' run outputs (held in directories titled 'analysis_lmh') \n",
    "    and produces an output DataFrame. \n",
    "    \n",
    "    Input(s):\n",
    "    - path (str):            path containing raw tracker data from Lucas Harris' runs.\n",
    "    Output(s):\n",
    "    - df (Pandas DataFrame): Pandas DataFrame containing tracked TC data\n",
    "    '''\n",
    "    \n",
    "    # Create file object instance\n",
    "    fobj = open(path, 'r').readlines()\n",
    "    # Initialize dictionary to hold data\n",
    "    data = {'storm_num': {}}\n",
    "    # Initialize storm counter\n",
    "    count = 1\n",
    "    # Iterate through text file\n",
    "    for line in fobj:\n",
    "        # Extract information from the line\n",
    "        content = line.strip()\n",
    "        # Creates new storm-specific dict in the parent dict. The '+++' demarcates a new storm.\n",
    "        if '+++' in line:\n",
    "            storm_num = '{0:04d}'.format(count)\n",
    "            data['storm_num'][storm_num] = {'storm_id': [], 'time': [], 'lon': [], 'lat': [], 'slp': [], 'max_wnd': [], 'flag': []}\n",
    "            count += 1\n",
    "        # Populates the storm-specific dict\n",
    "        else:\n",
    "            storm_num = '{0:04d}'.format(count-1) \n",
    "            tc_info = [x for x in content.split(' ') if x]\n",
    "            year = tc_info[0][0:4] # get 4-digit year\n",
    "            data['storm_num'][storm_num]['storm_id'].append('{0}-{1:04d}'.format(year, count-1))\n",
    "            data['storm_num'][storm_num]['time'].append(tc_info[0])\n",
    "            data['storm_num'][storm_num]['lon'].append(tc_info[1])\n",
    "            data['storm_num'][storm_num]['lat'].append(tc_info[2])\n",
    "            data['storm_num'][storm_num]['slp'].append(tc_info[3])\n",
    "            data['storm_num'][storm_num]['max_wnd'].append(tc_info[4])\n",
    "            data['storm_num'][storm_num]['flag'].append(tc_info[5])\n",
    "    \n",
    "    try:\n",
    "        # Converts the dictionary into a DataFrame\n",
    "        df = pd.concat({k: pd.DataFrame(v).T for k, v in data.items()}, axis=1)['storm_num']\n",
    "        df = df.explode(df.columns.to_list()).reset_index().rename(columns={'index': 'storm_num'})\n",
    "        # Re-cast column data types\n",
    "        df = df.astype({'lon': 'float', 'lat': 'float', 'slp': 'float', 'max_wnd': 'float', 'flag': 'float'})\n",
    "    except:\n",
    "        df = pd.DataFrame(columns=['storm_id', 'time', 'lon', 'lat', 'slp', 'max_wnd', 'flag'])\n",
    "    \n",
    "    ''' DataFrame refinement. '''\n",
    "    # Remove cold-core data points (flag == -1)\n",
    "    df = df.loc[df['flag'] != -1].reset_index(drop=True)\n",
    "    # Convert timestamps to datetime objects\n",
    "    df['time'] = pd.to_datetime(df['time'], format='%Y%m%d%H')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "76e43711-3711-4da6-a6c8-274a2c301b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coords_to_dist(a, b):\n",
    "    ''' Convert coordinates to distance in meters. '''\n",
    "    \n",
    "    R = 6371e3\n",
    "    \n",
    "    lon_a, lat_a = np.array(a)*np.pi/180\n",
    "    lon_b, lat_b = np.array(b)*np.pi/180\n",
    "    \n",
    "    dlon, dlat = lon_b - lon_a, lat_b - lat_a\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat_a)*np.cos(lat_b)*np.sin(dlon/2)**2    \n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    distance = R*c\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1bacb5f3-1e9a-45be-9b9b-77d094415a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_tracked_TCs(dirname, storm_type, year_range):\n",
    "\n",
    "    '''\n",
    "    Function to collect tracked TC data and add derived data, such as duration and storm speed.\n",
    "    \n",
    "    Input(s):\n",
    "    - dirname (str):              name of directory containing files of interest\n",
    "    - storm_type (str):           type of storm to evaluate from TC tracks data (\"TS\" for all storms or \"C15w\" for hurricanes)\n",
    "    - year_range (tuple of ints): 2-element tuple with a start and end year\n",
    "    Output(s):\n",
    "    - data (Pandas DataFrame):    Pandas DataFrame with tracked TC data\n",
    "    '''\n",
    "    \n",
    "    ''' File collection. '''\n",
    "    # Get filenames for all files within the specified directory \n",
    "    # Filenames will correspond to the determined storm type\n",
    "    fnames = [[os.path.join(dirname, file, 'Harris.TC', f) for f in os.listdir(os.path.join(dirname, file, 'Harris.TC')) \n",
    "               if '{0}.world'.format(storm_type) in f]\n",
    "               for file in sorted(os.listdir(dirname))]\n",
    "    # Compress 2D list to 1D list\n",
    "    fnames = [item for sublist in fnames for item in sublist]\n",
    "\n",
    "    # Select files with dates within 'year_range'\n",
    "    # Note: the '+ 1900' is added because tracked TCs are on the 2000 year range, whereas model output is on the 100 year range\n",
    "    fnames = [f for f in fnames \n",
    "              if min(year_range) + 1900 <= pd.to_datetime(f.split('.')[-2].split('-')[0]).year < max(year_range) + 1900]\n",
    "    \n",
    "    # Concatenate all tracked TC data from the filename list\n",
    "    data = pd.concat([lmh_parser(os.path.join(dirname, fname)) for fname in fnames])\n",
    "    \n",
    "    ''' Derived data algorithm. Storm-specific derived properties will be generated in here. '''\n",
    "    \n",
    "    # Initialize empty duration column to populate iteratively\n",
    "    data[['duration', 'speed', 'direction']] = np.nan\n",
    "    # Initialize list to populate iteratively for each storm, then concatenate\n",
    "    storms = []\n",
    "    # Iterate through each unique storm (identify by 'storm_id') and get duration\n",
    "    for storm_id in data['storm_id'].unique():\n",
    "        # Define iterand storm\n",
    "        storm = data.loc[data['storm_id'] == storm_id].copy().reset_index(drop=True)\n",
    "        \n",
    "        ''' Duration derivation. '''\n",
    "        # Get difference between minimum and maximum timestamps\n",
    "        dt = (storm['time'].max() - storm['time'].min())\n",
    "        # Convert difference timedelta into hours\n",
    "        dt = dt.days + dt.seconds/86400\n",
    "        # Add duration to the outer DataFrame for the corresponding storm\n",
    "        data.loc[data['storm_id'] == storm_id, 'duration'] = dt\n",
    "        # Re-define iterand storm to incorporate duration\n",
    "        storm = data.loc[data['storm_id'] == storm_id].copy().reset_index(drop=True)\n",
    "        \n",
    "        ''' Velocity (speed, direction) derivation. '''\n",
    "        # Initialize dictionary for preliminary storage. Will be reassigned into the DataFrame by the join() method using time as the matching criterion.\n",
    "        velocity = {'time': [storm.iloc[0]['time']], 'speed': [np.nan], 'direction': [np.nan]}\n",
    "        # Iterate over all of the iterand storm timestamps\n",
    "        for i in range(1, len(storm)):\n",
    "            # Define coordinates for two points considered (i, i-1)\n",
    "            lon_a, lat_a = [storm.iloc[i-1]['lon'], storm.iloc[i-1]['lat']]\n",
    "            lon_b, lat_b = [storm.iloc[i]['lon'], storm.iloc[i]['lat']]\n",
    "            # Determine timedelta between points (i, i-1)\n",
    "            dt = storm.iloc[i]['time'] - storm.iloc[i-1]['time']\n",
    "            # Derive speed (distance / time in m s^-1)\n",
    "            speed = coords_to_dist((lon_b, lat_b), (lon_a, lat_a))/dt.seconds\n",
    "            # Get changes in longtiude and latitude\n",
    "            dlon, dlat = lon_b - lon_a, lat_b - lat_a\n",
    "            # Derive direction relative to north (range of 0 to 360)\n",
    "            direction = 180*np.arctan(dlon/dlat)/np.pi % 360\n",
    "            # Append quantities to the 'velocity' dictionary\n",
    "            velocity['time'].append(storm.iloc[i]['time'])    \n",
    "            velocity['speed'].append(speed)    \n",
    "            velocity['direction'].append(direction)\n",
    "        # Build DataFrame\n",
    "        velocity = pd.DataFrame(velocity)\n",
    "        # Re-cast time column as a datetime object\n",
    "        velocity['time'] = pd.to_datetime(velocity['time'])\n",
    "        # Merge the storm and velocity DataFrames\n",
    "        storm = storm.merge(velocity, how='left', on='time', suffixes=['_x', None]).drop(columns={'speed_x', 'direction_x'}).reset_index(drop=True)\n",
    "        # Append to the list for future concatenation\n",
    "        storms.append(storm)\n",
    "        \n",
    "    # Concatenate DataFrames\n",
    "    data = pd.concat(storms)   \n",
    "    # Rename columns for future addition into xArray Dataset, and reset index\n",
    "    data = data.rename(columns={'lon': 'center_lon', 'lat': 'center_lat', 'flag': 'core_temp', 'slp': 'min_slp'}).reset_index(drop=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "94aa2604-9a4d-4597-8181-ea8ca9805f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dirname = '/tigress/wenchang/MODEL_OUT/HIRAMktc2/CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k'\n",
    "df = retrieve_tracked_TCs(dirname, 'C15w', year_range=(101, 110))\n",
    "tests = []\n",
    "for storm in df['storm_id'].unique():\n",
    "    test = df.loc[df['storm_id'] == storm]\n",
    "    tests.append(test)\n",
    "tests = pd.concat(tests)\n",
    "# del dirname, df, storm, test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
