{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07d45bf-dfcf-416b-8fa3-32ece4e664be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Import packages. '''\n",
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy\n",
    "# Local data storage packages\n",
    "import dill, os, pickle\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr\n",
    "# Visualization tools\n",
    "import cartopy.crs as ccrs, matplotlib, matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfbc3dc-217d-4eea-a6c6-2683781723cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tc_storage(model, experiment, year_range, storm_type, random_num=None, benchmarking=True):\n",
    "    \n",
    "    ''' \n",
    "    Function to build unified xArray Dataset to hold model and track data for tracked TCs in a given model and experiment.\n",
    "    \n",
    "    Input(s):\n",
    "    - model (str):                name of model to access (typically \"AM2.5\" or \"HIRAM\")\n",
    "    - experiment (str):           name of experiment to access (typically \"control\", \"ktc\", \"plus2K\", etc.)\n",
    "    - year_range (tuple of ints): 2-element tuple with a start and end year\n",
    "    - storm_type (str):           type of storm to evaluate from TC tracks data (\"TS\" for all storms or \"C15w\" for hurricanes)\n",
    "    - benchmarking (bool):        boolean to enable time benchmarking\n",
    "    Output(s):\n",
    "    - dirs (tuple):               2-element tuple with pathnames for the model- and experiment-specific GCM and track directories.\n",
    "    '''\n",
    "    \n",
    "    if benchmarking:\n",
    "        start = time.time()\n",
    "    \n",
    "    # Define paths to model- and experiment-specific data.\n",
    "    model_dir, track_dir = dirnames(model, experiment)\n",
    "    \n",
    "    if benchmarking:\n",
    "        print('Directory access time: {0:.3f} s'.format(time.time() - start))\n",
    "        lap = time.time()\n",
    "    \n",
    "    # Retrieve tracked TCs over the specified year range. Note: year_range re-specified to ensure increasing order in tuple.\n",
    "    storm_track_output = retrieve_tracked_TCs(track_dir, storm_type, year_range=(min(year_range), max(year_range)))\n",
    "    \n",
    "    if benchmarking:\n",
    "        print('Track access time: {0:.3f} s'.format(time.time() - lap))\n",
    "        lap = time.time()\n",
    "    \n",
    "    # Retrieve model data over specified year range. \n",
    "    # This is here so that both TC-specific and associated global model data can be analyzed together.\n",
    "    model_output = retrieve_model_data(model_dir, year_range=(min(year_range), max(year_range)), output_type='atmos_daily')\n",
    "    \n",
    "    if benchmarking:\n",
    "        print('Model output access time: {0:.3f} s'.format(time.time() - lap))\n",
    "        lap = time.time()\n",
    "    \n",
    "    # Retrieve model data specified to tracked TCs.\n",
    "    # Note: default model output type is 'atmos_4xdaily'. If 'output_type' matches 'model_output', then access pre-loaded data from 'model_output'.\n",
    "    storm_model_output = retrieve_model_TCs(model_dir, (min(year_range), max(year_range)), storm_track_output, \n",
    "                                            model_output, output_type='atmos_4xdaily', random_num=random_num)\n",
    "    \n",
    "    if benchmarking:\n",
    "        print('TC accessing output access time: {0:.3f} s'.format(time.time() - lap))\n",
    "        lap = time.time()\n",
    "    \n",
    "    # Get radius for each storm to use for future normalization\n",
    "    storm_model_output = add_radius(storm_model_output)\n",
    "    \n",
    "    if benchmarking:\n",
    "        print('Radius derivation computation time: {0:.3f} s'.format(time.time() - lap))\n",
    "        lap = time.time()\n",
    "    \n",
    "    if benchmarking:\n",
    "        print('Total runtime: {0:.3f} s'.format(time.time() - start))\n",
    "    \n",
    "    return storm_track_output, model_output, storm_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "969fb8ac-4279-481e-bceb-0f0065cdaf44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dirnames(model, experiment):\n",
    "\n",
    "    ''' \n",
    "    Function to store pathnames for selected models and experiments. \n",
    "    \n",
    "    Input(s):\n",
    "    - model (str):  name of model to access (typically \"AM2.5\" or \"HIRAM\").\n",
    "    Output(s):\n",
    "    - dirs (tuple): 2-element tuple with pathnames for the model- and experiment-specific GCM and track directories.\n",
    "    '''\n",
    "    \n",
    "    # Define experiment-specific pathnames\n",
    "    experiments = {'control': {'AM2.5': 'CTL1990s_tigercpu_intelmpi_18_540PE',\n",
    "                               'HIRAM': 'CTL1990s_v201910_tigercpu_intelmpi_18_540PE'},\n",
    "                   'ktc': {'AM2.5': 'CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE',\n",
    "                           'HIRAM': 'CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE'}}\n",
    "    # Model directories\n",
    "    model_dirs = {'AM2.5': {'control': '/tigress/wenchang/analysis/TC/AM2.5/CTL1990s_tigercpu_intelmpi_18_540PE/model_out/POSTP',\n",
    "                            'ktc': '/tigress/wenchang/analysis/TC/AM2.5ktc2/CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE/modelout/POSTP'},\n",
    "                  'HIRAM': {'control': '/tigress/wenchang/analysis/TC/HIRAM/CTL1990s_v201910_tigercpu_intelmpi_18_540PE/model_out/POSTP',\n",
    "                            'ktc': '/tigress/wenchang/analysis/TC/HIRAMktc2/CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE/modelout/POSTP'}}\n",
    "    # Track directories\n",
    "    # Potential issue: ask Wenchang about using track .nc files in the same parent directory tree as the model data\n",
    "    track_dirs = {'AM2.5': {'control': '/tigress/wenchang/MODEL_OUT/AM2.5/CTL1990s_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k',\n",
    "                            'ktc': '/tigress/wenchang/MODEL_OUT/AM2.5ktc2/CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k'},\n",
    "                  'HIRAM': {'control': '/tigress/wenchang/MODEL_OUT/HIRAM/CTL1990s_v201910_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_2p5C_330k',\n",
    "                            'ktc': '/tigress/wenchang/MODEL_OUT/HIRAMktc2/CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k'}}\n",
    "\n",
    "    dirs = (model_dirs[model][experiment], track_dirs[model][experiment])\n",
    "    \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d4303b6-d456-46b3-b84b-45090ebe2bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lmh_parser(path):\n",
    "    \n",
    "    ''' \n",
    "    This method parses through text files from Lucas Harris' run outputs (held in directories titled 'analysis_lmh') \n",
    "    and produces an output DataFrame. \n",
    "    \n",
    "    Input(s):\n",
    "    - path (str):            path containing raw tracker data from Lucas Harris' runs.\n",
    "    Output(s):\n",
    "    - df (Pandas DataFrame): Pandas DataFrame containing tracked TC data\n",
    "    '''\n",
    "    \n",
    "    # Create file object instance\n",
    "    fobj = open(path, 'r').readlines()\n",
    "    # Initialize dictionary to hold data\n",
    "    data = {'storm_num': {}}\n",
    "    # Initialize storm counter\n",
    "    count = 1\n",
    "    # Iterate through text file\n",
    "    for line in fobj:\n",
    "        # Extract information from the line\n",
    "        content = line.strip()\n",
    "        # Creates new storm-specific dict in the parent dict. The '+++' demarcates a new storm.\n",
    "        if '+++' in line:\n",
    "            storm_num = '{0:04d}'.format(count)\n",
    "            data['storm_num'][storm_num] = {'storm_id': [], 'time': [], 'lon': [], 'lat': [], 'slp': [], 'max_wnd': [], 'flag': []}\n",
    "            count += 1\n",
    "        # Populates the storm-specific dict\n",
    "        else:\n",
    "            storm_num = '{0:04d}'.format(count-1) \n",
    "            tc_info = [x for x in content.split(' ') if x]\n",
    "            year = tc_info[0][0:4] # get 4-digit year\n",
    "            data['storm_num'][storm_num]['storm_id'].append('{0}-{1:04d}'.format(year, count-1))\n",
    "            data['storm_num'][storm_num]['time'].append(tc_info[0])\n",
    "            data['storm_num'][storm_num]['lon'].append(tc_info[1])\n",
    "            data['storm_num'][storm_num]['lat'].append(tc_info[2])\n",
    "            data['storm_num'][storm_num]['slp'].append(tc_info[3])\n",
    "            data['storm_num'][storm_num]['max_wnd'].append(tc_info[4])\n",
    "            data['storm_num'][storm_num]['flag'].append(tc_info[5])\n",
    "    \n",
    "    try:\n",
    "        # Converts the dictionary into a DataFrame\n",
    "        df = pd.concat({k: pd.DataFrame(v).T for k, v in data.items()}, axis=1)['storm_num']\n",
    "        df = df.explode(df.columns.to_list()).reset_index().rename(columns={'index': 'storm_num'})\n",
    "        # Re-cast column data types\n",
    "        df = df.astype({'lon': 'float', 'lat': 'float', 'slp': 'float', 'max_wnd': 'float', 'flag': 'float'})\n",
    "    except:\n",
    "        df = pd.DataFrame(columns=['storm_id', 'time', 'lon', 'lat', 'slp', 'max_wnd', 'flag'])\n",
    "    \n",
    "    ''' DataFrame refinement. '''\n",
    "    # Remove cold-core data points (flag == -1)\n",
    "    df = df.loc[df['flag'] != -1].reset_index(drop=True)\n",
    "    # Convert timestamps to datetime objects\n",
    "    df['time'] = pd.to_datetime(df['time'], format='%Y%m%d%H')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e43711-3711-4da6-a6c8-274a2c301b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coords_to_dist(a, b):\n",
    "    ''' Convert coordinates to distance in meters. '''\n",
    "    \n",
    "    R = 6371e3\n",
    "    \n",
    "    lon_a, lat_a = np.array(a)*np.pi/180\n",
    "    lon_b, lat_b = np.array(b)*np.pi/180\n",
    "    \n",
    "    dlon, dlat = lon_b - lon_a, lat_b - lat_a\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat_a)*np.cos(lat_b)*np.sin(dlon/2)**2    \n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    distance = R*c\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bacb5f3-1e9a-45be-9b9b-77d094415a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_tracked_TCs(dirname, storm_type, year_range):\n",
    "\n",
    "    '''\n",
    "    Function to collect tracked TC data and add derived data, such as duration and storm speed.\n",
    "    \n",
    "    Input(s):\n",
    "    - dirname (str):              name of directory containing files of interest\n",
    "    - storm_type (str):           type of storm to evaluate from TC tracks data (\"TS\" for all storms or \"C15w\" for hurricanes)\n",
    "    - year_range (tuple of ints): 2-element tuple with a start and end year\n",
    "    Output(s):\n",
    "    - data (Pandas DataFrame):    Pandas DataFrame with tracked TC data\n",
    "    '''\n",
    "    \n",
    "    ''' File collection. '''\n",
    "    # Get filenames for all files within the specified directory \n",
    "    # Filenames will correspond to the determined storm type\n",
    "    fnames = [[os.path.join(dirname, file, 'Harris.TC', f) for f in os.listdir(os.path.join(dirname, file, 'Harris.TC')) \n",
    "               if '{0}.world'.format(storm_type) in f]\n",
    "               for file in sorted(os.listdir(dirname))]\n",
    "    # Compress 2D list to 1D list\n",
    "    fnames = [item for sublist in fnames for item in sublist]\n",
    "\n",
    "    # Select files with dates within 'year_range'\n",
    "    # Note: the '+ 1900' is added because tracked TCs are on the 2000 year range, whereas model output is on the 100 year range\n",
    "    fnames = [f for f in fnames \n",
    "              if min(year_range) + 1900 <= pd.to_datetime(f.split('.')[-2].split('-')[0]).year < max(year_range) + 1900]\n",
    "    \n",
    "    # Concatenate all tracked TC data from the filename list\n",
    "    data = pd.concat([lmh_parser(os.path.join(dirname, fname)) for fname in fnames])\n",
    "    \n",
    "    ''' Derived track-based data algorithm. Storm-specific derived properties will be generated in here. '''\n",
    "    \n",
    "    # Initialize empty duration column to populate iteratively\n",
    "    data[['duration', 'speed', 'direction']] = np.nan\n",
    "    # Initialize list to populate iteratively for each storm, then concatenate\n",
    "    storms = []\n",
    "    # Iterate through each unique storm (identify by 'storm_id') and get duration\n",
    "    for storm_id in data['storm_id'].unique():\n",
    "        # Define iterand storm\n",
    "        storm = data.loc[data['storm_id'] == storm_id].copy().reset_index(drop=True)\n",
    "        \n",
    "        ''' Duration derivation. '''\n",
    "        # Get difference between minimum and maximum timestamps\n",
    "        dt = (storm['time'].max() - storm['time'].min())\n",
    "        # Convert difference timedelta into hours\n",
    "        dt = dt.days + dt.seconds/86400\n",
    "        # Add duration to the outer DataFrame for the corresponding storm\n",
    "        data.loc[data['storm_id'] == storm_id, 'duration'] = dt\n",
    "        # Re-define iterand storm to incorporate duration\n",
    "        storm = data.loc[data['storm_id'] == storm_id].copy().reset_index(drop=True)\n",
    "        \n",
    "        ''' Velocity (speed, direction) derivation. '''\n",
    "        # Initialize dictionary for preliminary storage. Will be reassigned into the DataFrame by the join() method using time as the matching criterion.\n",
    "        velocity = {'time': [storm.iloc[0]['time']], 'speed': [np.nan], 'direction': [np.nan]}\n",
    "        # Iterate over all of the iterand storm timestamps\n",
    "        for i in range(1, len(storm)):\n",
    "            # Define coordinates for two points considered (i, i-1)\n",
    "            lon_a, lat_a = [storm.iloc[i-1]['lon'], storm.iloc[i-1]['lat']]\n",
    "            lon_b, lat_b = [storm.iloc[i]['lon'], storm.iloc[i]['lat']]\n",
    "            # Determine timedelta between points (i, i-1)\n",
    "            dt = storm.iloc[i]['time'] - storm.iloc[i-1]['time']\n",
    "            # Derive speed (distance / time in m s^-1)\n",
    "            speed = coords_to_dist((lon_b, lat_b), (lon_a, lat_a))/dt.seconds\n",
    "            # Get changes in longtiude and latitude\n",
    "            dlon, dlat = lon_b - lon_a, lat_b - lat_a\n",
    "            # Derive direction relative to north (range of 0 to 360)\n",
    "            direction = 180*np.arctan(dlon/dlat)/np.pi % 360\n",
    "            # Append quantities to the 'velocity' dictionary\n",
    "            velocity['time'].append(storm.iloc[i]['time'])    \n",
    "            velocity['speed'].append(speed)    \n",
    "            velocity['direction'].append(direction)\n",
    "        # Build DataFrame\n",
    "        velocity = pd.DataFrame(velocity)\n",
    "        # Re-cast time column as a datetime object\n",
    "        velocity['time'] = pd.to_datetime(velocity['time'])\n",
    "        # Merge the storm and velocity DataFrames\n",
    "        storm = storm.merge(velocity, how='left', on='time', suffixes=['_x', None]).drop(columns={'speed_x', 'direction_x'}).reset_index(drop=True)\n",
    "        # Append to the list for future concatenation\n",
    "        storms.append(storm)\n",
    "        \n",
    "    # Concatenate DataFrames\n",
    "    data = pd.concat(storms)   \n",
    "    # Rename columns for future addition into xArray Dataset, and reset index\n",
    "    data = data.rename(columns={'lon': 'center_lon', 'lat': 'center_lat', 'flag': 'core_temp', 'slp': 'min_slp'}).reset_index(drop=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7fda4cc-e3a9-447c-8c59-7d9521f49f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pull_gcm_data(dirname, year, output_type):\n",
    "    \n",
    "    ''' Method to read data for given parameters. '''\n",
    "    \n",
    "    # Get filenames for corresponding files\n",
    "    filename = '{0:04d}0101.{1}.nc'.format(year, output_type)\n",
    "    try:\n",
    "        fname = os.path.join(dirname, filename)\n",
    "        # Retrieve data\n",
    "        data = xr.open_dataset(fname)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18cff38e-7fd0-4c50-bc59-209984d5ef97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_model_data(dirname, year_range, output_type='atmos_daily'):\n",
    "    \n",
    "    '''\n",
    "    Method to open experiment-specific GCM output data for a specified year range and model output type.\n",
    "    \n",
    "    Input(s):\n",
    "    - dirname (str):            directory name for the given model and experiment type.\n",
    "    - year_range (tuple, list): tuple or list (minimum of 2 items) containing year range for desired data.\n",
    "    - output_type (str):        string denoting GCM output desired.\n",
    "    Output(s):\n",
    "    - data (xArray Dataset):    xArray Dataset containing concatenated data for the year range selected.\n",
    "    '''\n",
    "    \n",
    "    # Identifier substring definition. This will be used for splitting the filename for identification purposes.\n",
    "    substring = '0101.'\n",
    "    # Access parent directory with experiment-specific model data and list directories corresponding to year range\n",
    "    files = [os.path.join(dirname, file) for file in os.listdir(dirname) for year in range(min(year_range), max(year_range)) \n",
    "             if str(year) in file.split(substring)[0] and output_type in file]\n",
    "    # Store file data into an xArray Dataset.\n",
    "    # Note: benchmarking showed ~1.5 s for opening 4 files using 'open_mfdataset', and at least 10x longer using 'open_dataset' + 'xr.concat'\n",
    "    data = xr.open_mfdataset(files)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34600793-0d64-4796-a27e-0bbebff07eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_model_TCs(dirname, year_range, storms, model_output=None, output_type='atmos_4xdaily', extent=10, random_num=None):\n",
    "    \n",
    "    # Check to see if model_output (previously-access model data) is the same output type as desired for TCs. If so, pull from that Dataset.\n",
    "    if model_output and output_type == model_output.attrs['filename'].split('.')[0]:\n",
    "        data = model_output\n",
    "    else:\n",
    "        # Identifier substring definition. This will be used for splitting the filename for identification purposes.\n",
    "        substring = '0101.'\n",
    "        # Access parent directory with experiment-specific model data and list directories corresponding to year range\n",
    "        files = [os.path.join(dirname, file) for file in os.listdir(dirname) for year in range(min(year_range), max(year_range)) \n",
    "                 if str(year) in file.split(substring)[0] and output_type in file]\n",
    "        # Store file data into an xArray Dataset.\n",
    "        data = xr.open_mfdataset(files)\n",
    "    \n",
    "    # Initialize list to hold each storm Dataset for future concatenation\n",
    "    storms_xr = []\n",
    "    # Define range of storm IDs to iterate over. \n",
    "    # If 'random_num' is defined, get that many randomized storms. Otherwise, get all.\n",
    "    storm_ids = random.sample(list(storms['storm_id'].unique()), random_num) if random_num else storms['storm_id'].unique()\n",
    "    # Access model data that are specific to each tracked TC.\n",
    "    for storm_id in storm_ids:\n",
    "        # Pull tracked TC data relative to storm\n",
    "        storm = storms.loc[storms['storm_id'] == storm_id]\n",
    "        # Initialize list to hold Dataset entries for each storm timestamp\n",
    "        storm_xr = []\n",
    "        # Iterate over each storm Series\n",
    "        for i in range(0, len(storm)):                   \n",
    "            # Convert from tracked TC timestamp convention (datetime) to model timestamp convention (cftime DatetimeNoLeap)\n",
    "            cf_timestamp = cftime.DatetimeNoLeap(year=storm.iloc[i]['time'].year-1900, month=storm.iloc[i]['time'].month, \n",
    "                                                 day=storm.iloc[i]['time'].day, hour=storm.iloc[i]['time'].hour)\n",
    "            # Take snapshot of data at this timestamp\n",
    "            # Note 1: this is where the connection between track and model output data happens\n",
    "            # Note 2: any operations beyond selection result in active computation, which disrupts the 'lazy' approach\n",
    "            snapshot = data.sel(time=cf_timestamp)\n",
    "            # Get iterand information to append to Dataset for the given timestamp\n",
    "            snapshot[['center_lon', 'center_lat', 'min_slp', \n",
    "                      'max_wnd', 'core_temp', 'speed', 'heading']] = [storm.iloc[i]['center_lon'], storm.iloc[i]['center_lat'], \n",
    "                                                                      storm.iloc[i]['min_slp'], storm.iloc[i]['max_wnd'], \n",
    "                                                                      storm.iloc[i]['core_temp'], storm.iloc[i]['speed'], storm.iloc[i]['direction']]  \n",
    "            # Append to list\n",
    "            storm_xr.append(snapshot)\n",
    "        # Concatenate into unified Dataset for a storm\n",
    "        storm_xr = xr.concat(storm_xr, dim='time')\n",
    "        # Assign storm identifier (storm_id)\n",
    "        # Note: to select specific storm from concatenated Dataset, use command as follows:\n",
    "        #       storms.where(storms['storm_id'] == <storm_id>, drop=True)\n",
    "        storm_xr['storm_id'] = storm_id\n",
    "    \n",
    "        ''' Spatial clipping - only at maximum intensity to save computation time. '''\n",
    "        # Grab storm at timestamp with maximum wind speed\n",
    "        # Calling this 'lifetime maximum intensity', or 'LMI', to use terminology from Wing et al, 2016 (10.1175/JCLI-D-18-0599.1)\n",
    "        lmi = storm_xr.where(storm_xr['max_wnd'] == storm_xr['max_wnd'].max(), drop=True)\n",
    "        # If multiple wind maxima are detected, get the storm with lower pressure\n",
    "        lmi = lmi.where(lmi['min_slp'] == lmi['min_slp'].min(), drop=True) if len(lmi.time.values) > 1 else lmi\n",
    "        # Clip storm        \n",
    "        lmi = lmi.sel(grid_xt=np.arange(lmi['center_lon']-extent, lmi['center_lon']+extent), \n",
    "                      grid_yt=np.arange(lmi['center_lat']-extent, lmi['center_lat']+extent), method='nearest')\n",
    "        \n",
    "        # Append to list for future concatenation\n",
    "        storms_xr.append(lmi)\n",
    "        \n",
    "    # Concatenate into single xArray Dataset\n",
    "    storms_xr = xr.concat(storms_xr, dim='time')\n",
    "    \n",
    "    return storms_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6a6ece-d7b1-4e08-8e39-e2cecc18ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radius_estimate(storm, box, overlay_check=True):\n",
    "    \n",
    "    '''\n",
    "    Algorithm to estimate the radius of a TC based on filters set below.\n",
    "    Returns a radius in meters.\n",
    "    \n",
    "    Note 1: the idea here is: \n",
    "            (1) regrid data to higher resolution by interpolation, \n",
    "            (2) smooth data and use gradient-based filtering to prevent dual-vortex identification,\n",
    "            (3) identify thresholds for value-based filtering,\n",
    "            (4) use gradient- and value-based filtering to identify data points that match criteria,\n",
    "            (5) clip data and estimate radius from filtered data\n",
    "    '''\n",
    "    \n",
    "    # Ensure that only one timestamp is in the Dataset\n",
    "    try:\n",
    "        storm = storm.isel(time=0)\n",
    "    except:\n",
    "        storm = storm\n",
    "    # Ensure that Dataset is from the 'atmos_4xdaily' output types\n",
    "    if storm.attrs['filename'].split('.')[0] != 'atmos_4xdaily':\n",
    "        return np.nan\n",
    "    # Derives horizontal wind speed (proxy for azimuthal wind)\n",
    "    if 'U' not in storm.data_vars.keys():\n",
    "        storm['U'] = np.sqrt(storm['u_ref']**2 + storm['v_ref']**2)\n",
    "        \n",
    "    ''' Perform linear interpolation to allow for better gradient estimation for future filtering. '''\n",
    "    # Pull numerical data from parameters relevant to radius estimation\n",
    "    params = ['grid_xt', 'grid_yt', 'vort850', 'tm', 'slp', 'U']\n",
    "    # Define the interpolation resolution (in degrees) and spatial extent of clipping\n",
    "    resolution, extent = 0.5, 15\n",
    "    # Define dictionary to store data in\n",
    "    params = {param: [] for param in params}\n",
    "    # Iterate through parameters and interpolate\n",
    "    for param in params.keys():\n",
    "        if param == 'grid_xt':\n",
    "            params[param] =  np.arange(storm['center_lon'] - extent, storm['center_lon'] + extent, resolution)\n",
    "        elif param == 'grid_yt':\n",
    "            params[param] =  np.arange(storm['center_lat'] - extent, storm['center_lat'] + extent, resolution)\n",
    "        else:\n",
    "            params[param] =  storm[param].interp(grid_xt=np.arange(storm['center_lon'] - extent, storm['center_lon'] + extent, resolution), \n",
    "                                                 grid_yt=np.arange(storm['center_lat'] - extent, storm['center_lat'] + extent, resolution)).values\n",
    "\n",
    "        \n",
    "    ''' \n",
    "    Data smoothing and gradient filtering algorithm. \n",
    "    The idea here is to use gradients for a chosen field to isolate TC extent and prevent dual-vortex pickup for a given storm, which distorts radius calculation. \n",
    "    '''\n",
    "    # 1 hPa/deg pressure gradient (attempt at similarity to Harris)\n",
    "    diff_var, diff_val = 'slp', 1*resolution\n",
    "    # Use a 1-sigma Gaussian smoothing filter\n",
    "    smoothed = scipy.ndimage.gaussian_filter(np.abs(np.diff(np.diff(params[diff_var], axis=0), axis=1)), sigma=1)\n",
    "    # Apply the filter and resize such that filter boolean array shape matches the data array shape\n",
    "    diff_filter = smoothed > diff_val\n",
    "    diff_filter = np.hstack((diff_filter, np.full((diff_filter.shape[0], 1), False)))\n",
    "    diff_filter = np.vstack((diff_filter, np.full((1, diff_filter.shape[1]), False)))\n",
    "    \n",
    "    ''' Apply thresholds to absolute values of identified parameters. '''\n",
    "    # Define number of standard deviations to analyze\n",
    "    sigma = 1\n",
    "    # Exact magnitude thresholds\n",
    "    filters = {'vort850': np.abs(params['vort850']) > 1.5e-4,\n",
    "               'tm': params['tm'] > (np.nanmean(params['tm']) + sigma*np.nanstd(params['tm'].std())),\n",
    "               'slp': params['slp'] < 1000,\n",
    "               'U': params['U'] > 15}\n",
    "    \n",
    "    ''' Perform the filtering and associated array clipping. '''\n",
    "    # Define the conditional based on the threshold and gradient filters\n",
    "    conditional = (filters['vort850'] & filters['slp'] & filters['U'] & diff_filter)\n",
    "    # Define variable for filtering on\n",
    "    filter_var = 'slp'\n",
    "    # Perform filtering based on chosen variable\n",
    "    filtered = np.where(conditional, params[filter_var], np.nan)\n",
    "    # Crop all-nan rows in the zonal and meridional (x- and y-) array axes\n",
    "    crop_x, crop_x_idx = filtered[~np.all(np.isnan(filtered), axis=1), :], ~np.all(np.isnan(filtered), axis=1)\n",
    "    crop_y, crop_y_idx = crop_x[:, ~np.all(np.isnan(crop_x), axis=0)], ~np.all(np.isnan(crop_x), axis=0)\n",
    "    # Output masked array for visualization of algorithm output\n",
    "    arr = np.ma.masked_values(filtered, np.nan)\n",
    "    \n",
    "    ''' Derive radius, if the filtered array is not empty. '''\n",
    "    # If filtering results in populated output array, get a radius\n",
    "    if crop_y.shape != (0, 0):\n",
    "        # If there's a mismatch in grid sizes, crop the larger one\n",
    "        if params['grid_xt'].shape != params['grid_yt'].shape:\n",
    "            if params['grid_xt'].shape[0] > params['grid_yt'].shape[0]:\n",
    "                cut = params['grid_yt'].shape[0]\n",
    "                # Perform the slicing\n",
    "                params['grid_xt'] = params['grid_xt'][:cut]\n",
    "                crop_y_idx = crop_y_idx[:cut]\n",
    "            else:\n",
    "                cut = params['grid_xt'].shape[0]\n",
    "                # Perform the slicing\n",
    "                params['grid_yt'] = params['grid_yt'][:cut]\n",
    "                crop_x_idx = crop_x_idx[:cut]\n",
    "                \n",
    "        # Get the longitude and latitude extrema corresponding to the filtered array\n",
    "        lons = np.min(params['grid_xt'][crop_x_idx]), np.max(params['grid_xt'][crop_x_idx])\n",
    "        lats = np.min(params['grid_yt'][crop_y_idx]), np.max(params['grid_yt'][crop_y_idx])\n",
    "        # Get the coordinate extrema for radius derivation\n",
    "        coords = [lons[0], lats[0]], [lons[1], lats[1]]\n",
    "        # Derive radius from coordinate pairs (divide by 2 and divide by 1000 to go from diameter to radius and m to km)\n",
    "        radius = coords_to_dist(coords[0], coords[1])/2000\n",
    "        # Add radius to the storm Dataset\n",
    "        storm['radius'] = radius\n",
    "        \n",
    "        # Overlay the storm size algorithm output on maps of the storms \n",
    "        if overlay_check:\n",
    "            # Define the plot basics\n",
    "            fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "            ax.coastlines()\n",
    "            ax.set_extent([storm['center_lon']-extent, storm['center_lon']+extent, storm['center_lat']-extent, storm['center_lat']+extent])\n",
    "            # Plot data\n",
    "            im = ax.contourf(params['grid_xt'][:-1], params['grid_yt'][:-1], smoothed, levels=16)\n",
    "            ax.pcolormesh(params['grid_xt'], params['grid_yt'], arr[:-1, :-1], \n",
    "                          zorder=9, cmap='Reds', transform=ccrs.PlateCarree())\n",
    "            # Plot metadata\n",
    "            ax.set_title('radius: {0:.2f} km'.format(radius))\n",
    "            fig.colorbar(im)\n",
    "            \n",
    "        return storm\n",
    "    # Else, return nan\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d231d19-97e5-4990-bf16-6e3fc95766d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_radius(storms, test_num=None):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    samples = random.sample(list(set(storms['storm_id'].values)), test_num) if test_num else list(set(storms['storm_id'].values))\n",
    "\n",
    "    container = []\n",
    "    for storm_id in samples:\n",
    "        storm = storms.where(storms['storm_id'] == storm_id, drop=True).load()\n",
    "        storm = storm.dropna(dim='grid_xt', how='all').dropna(dim='grid_yt', how='all')\n",
    "        storm = radius_estimate(storm, box=10, overlay_check=False)\n",
    "        if 'xarray' in str(type(storm)):\n",
    "            container.append(storm)\n",
    "    storms = xr.concat(container, dim='time')\n",
    "    \n",
    "    print('Radius estimation runtime per storm: {0:.3f} s'.format((time.time() - start)/len(samples)))\n",
    "    \n",
    "    return storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de217d54-e152-496b-98b4-89f8bab0a6a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tc_normalization(storms, var='U', test_num=None):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    samples = random.sample(list(set(storms['storm_id'].values)), test_num) if test_num else list(set(storms['storm_id'].values))\n",
    "    num_pts = 40 # chosen because it's twice the resolution of incoming plots\n",
    "    \n",
    "    container = []\n",
    "    for storm_id in samples:\n",
    "        # Pull storm by its ID - figure out if this can be sped up\n",
    "        storm = storms.where(storms['storm_id'] == storm_id, drop=True).load()\n",
    "        storm = storm.dropna(dim='grid_xt', how='all').dropna(dim='grid_yt', how='all')\n",
    "        # Ensure that only one timestamp is in the Dataset\n",
    "        try:\n",
    "            storm = storm.isel(time=0)\n",
    "        except:\n",
    "            storm = storm\n",
    "        # Get grid data and field of interest\n",
    "        x, y, v = storm.grid_xt.values, storm.grid_yt.values, storm[var]\n",
    "        # Normalize grid values to 1\n",
    "        x = (x - x.min())*storm['radius'].values\n",
    "        x = x/x.max()\n",
    "        y = (y - y.min())*storm['radius'].values\n",
    "        y = y/y.max()\n",
    "        \n",
    "        # If there's a mismatch in grid sizes, crop the larger one\n",
    "        if len(x) != len(y):\n",
    "            if len(x) > len(y):\n",
    "                cut = len(y)\n",
    "                # Perform the slicing\n",
    "                x = x[:cut]\n",
    "                v = v[:cut, :]\n",
    "            else:\n",
    "                cut = len(x)\n",
    "                # Perform the slicing\n",
    "                y = y[:cut]\n",
    "                v = v[:, :cut]\n",
    "                \n",
    "        if v.shape[0] != len(y):\n",
    "            v = v[:len(y), :]\n",
    "        \n",
    "        if v.shape[1] != len(x):\n",
    "            v = v[:, :len(x)]\n",
    "        \n",
    "        # Create nominal Dataset\n",
    "        ds = xr.DataArray(data=v, dims=['x', 'y'], coords={'x': x, 'y': y})\n",
    "        try:\n",
    "            # Regrid\n",
    "            ds = ds.interp(x=np.linspace(0, 1, num_pts), y=np.linspace(0, 1, num_pts))\n",
    "            # Add storm ID\n",
    "            ds['storm_id'] = storm['storm_id']\n",
    "            # Append to container\n",
    "            container.append(ds)\n",
    "        except:\n",
    "            print('Failed: ', storm_id)\n",
    "            continue\n",
    "    var_norm = xr.concat(container, dim='time')\n",
    "    \n",
    "    return var_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0ebf7d-d37d-484e-b6e5-8c956bbac537",
   "metadata": {},
   "source": [
    "Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94aa2604-9a4d-4597-8181-ea8ca9805f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIRAM\n",
      "\t control\n",
      "Directory access time: 0.000 s\n",
      "Track access time: 70.812 s\n",
      "Model output access time: 136.008 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experiment \u001b[38;5;129;01min\u001b[39;00m experiments:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(experiment))\n\u001b[0;32m---> 14\u001b[0m     track_output, global_model_output, tc_model_output \u001b[38;5;241m=\u001b[39m \u001b[43mtc_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmarking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     data[model][experiment][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrack_output\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m track_output\n\u001b[1;32m     16\u001b[0m     data[model][experiment][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_model_output\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m global_model_output\n",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m, in \u001b[0;36mtc_storage\u001b[0;34m(model, experiment, year_range, storm_type, random_num, benchmarking)\u001b[0m\n\u001b[1;32m     39\u001b[0m     lap \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Retrieve model data specified to tracked TCs.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Note: default model output type is 'atmos_4xdaily'. If 'output_type' matches 'model_output', then access pre-loaded data from 'model_output'.\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m storm_model_output \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_model_TCs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myear_range\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myear_range\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorm_track_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43matmos_4xdaily\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_num\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m benchmarking:\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTC accessing output access time: \u001b[39m\u001b[38;5;132;01m{0:.3f}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m lap))\n",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m, in \u001b[0;36mretrieve_model_TCs\u001b[0;34m(dirname, year_range, storms, model_output, output_type, extent, random_num)\u001b[0m\n\u001b[1;32m     10\u001b[0m     files \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dirname, file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(dirname) \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mmin\u001b[39m(year_range), \u001b[38;5;28mmax\u001b[39m(year_range)) \n\u001b[1;32m     11\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(year) \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39msplit(substring)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m output_type \u001b[38;5;129;01min\u001b[39;00m file]\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Store file data into an xArray Dataset.\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_mfdataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Initialize list to hold each storm Dataset for future concatenation\u001b[39;00m\n\u001b[1;32m     16\u001b[0m storms_xr \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/backends/api.py:983\u001b[0m, in \u001b[0;36mopen_mfdataset\u001b[0;34m(paths, chunks, concat_dim, compat, preprocess, engine, data_vars, coords, combine, parallel, join, attrs_file, combine_attrs, **kwargs)\u001b[0m\n\u001b[1;32m    980\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m    981\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m--> 983\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    984\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/backends/api.py:983\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    980\u001b[0m     open_ \u001b[38;5;241m=\u001b[39m open_dataset\n\u001b[1;32m    981\u001b[0m     getattr_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m\n\u001b[0;32m--> 983\u001b[0m datasets \u001b[38;5;241m=\u001b[39m [\u001b[43mopen_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopen_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths]\n\u001b[1;32m    984\u001b[0m closers \u001b[38;5;241m=\u001b[39m [getattr_(ds, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_close\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m datasets]\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/backends/api.py:526\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    515\u001b[0m     decode_cf,\n\u001b[1;32m    516\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    522\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    523\u001b[0m )\n\u001b[1;32m    525\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 526\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdecoders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    532\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[1;32m    533\u001b[0m     backend_ds,\n\u001b[1;32m    534\u001b[0m     filename_or_obj,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    543\u001b[0m )\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:591\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[1;32m    589\u001b[0m store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n\u001b[0;32m--> 591\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mstore_entrypoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_and_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_and_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_times\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconcat_characters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcat_characters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cftime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cftime\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_timedelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_timedelta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/backends/store.py:47\u001b[0m, in \u001b[0;36mStoreBackendEntrypoint.open_dataset\u001b[0;34m(self, store, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta)\u001b[0m\n\u001b[1;32m     33\u001b[0m encoding \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget_encoding()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mvars\u001b[39m, attrs, coord_names \u001b[38;5;241m=\u001b[39m conventions\u001b[38;5;241m.\u001b[39mdecode_cf_variables(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mvars\u001b[39m,\n\u001b[1;32m     37\u001b[0m     attrs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m     decode_timedelta\u001b[38;5;241m=\u001b[39mdecode_timedelta,\n\u001b[1;32m     45\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mvars\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mset_coords(coord_names\u001b[38;5;241m.\u001b[39mintersection(\u001b[38;5;28mvars\u001b[39m))\n\u001b[1;32m     49\u001b[0m ds\u001b[38;5;241m.\u001b[39mset_close(store\u001b[38;5;241m.\u001b[39mclose)\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/dataset.py:612\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, data_vars, coords, attrs)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(coords, Dataset):\n\u001b[1;32m    610\u001b[0m     coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mvariables\n\u001b[0;32m--> 612\u001b[0m variables, coord_names, dims, indexes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmerge_data_and_coords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbroadcast_equals\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    614\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(attrs) \u001b[38;5;28;01mif\u001b[39;00m attrs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/merge.py:564\u001b[0m, in \u001b[0;36mmerge_data_and_coords\u001b[0;34m(data_vars, coords, compat, join)\u001b[0m\n\u001b[1;32m    562\u001b[0m objects \u001b[38;5;241m=\u001b[39m [data_vars, coords]\n\u001b[1;32m    563\u001b[0m explicit_coords \u001b[38;5;241m=\u001b[39m coords\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m--> 564\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge_core\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplicit_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplicit_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIndexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/merge.py:744\u001b[0m, in \u001b[0;36mmerge_core\u001b[0;34m(objects, compat, join, combine_attrs, priority_arg, explicit_coords, indexes, fill_value)\u001b[0m\n\u001b[1;32m    740\u001b[0m coerced \u001b[38;5;241m=\u001b[39m coerce_pandas_values(objects)\n\u001b[1;32m    741\u001b[0m aligned \u001b[38;5;241m=\u001b[39m deep_align(\n\u001b[1;32m    742\u001b[0m     coerced, join\u001b[38;5;241m=\u001b[39mjoin, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, indexes\u001b[38;5;241m=\u001b[39mindexes, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[1;32m    743\u001b[0m )\n\u001b[0;32m--> 744\u001b[0m collected \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_variables_and_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43maligned\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m prioritized \u001b[38;5;241m=\u001b[39m _get_priority_vars_and_indexes(aligned, priority_arg, compat\u001b[38;5;241m=\u001b[39mcompat)\n\u001b[1;32m    746\u001b[0m variables, out_indexes \u001b[38;5;241m=\u001b[39m merge_collected(\n\u001b[1;32m    747\u001b[0m     collected, prioritized, compat\u001b[38;5;241m=\u001b[39mcompat, combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs\n\u001b[1;32m    748\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/merge.py:354\u001b[0m, in \u001b[0;36mcollect_variables_and_indexes\u001b[0;34m(list_of_mappings, indexes)\u001b[0m\n\u001b[1;32m    351\u001b[0m     indexes_\u001b[38;5;241m.\u001b[39mpop(name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    352\u001b[0m     append_all(coords_, indexes_)\n\u001b[0;32m--> 354\u001b[0m variable \u001b[38;5;241m=\u001b[39m \u001b[43mas_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m indexes:\n\u001b[1;32m    356\u001b[0m     append(name, variable, indexes[name])\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/variable.py:158\u001b[0m, in \u001b[0;36mas_variable\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MissingDimensionsError(\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has more than 1-dimension and the same name as one of its \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimensions \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m. xarray disallows such variables because they \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconflict with the coordinates used to label dimensions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         )\n\u001b[0;32m--> 158\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_index_variable\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/variable.py:614\u001b[0m, in \u001b[0;36mVariable.to_index_variable\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_index_variable\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IndexVariable:\n\u001b[1;32m    613\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return this variable as an xarray.IndexVariable\"\"\"\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIndexVariable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/variable.py:2843\u001b[0m, in \u001b[0;36mIndexVariable.__init__\u001b[0;34m(self, dims, data, attrs, encoding, fastpath)\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[38;5;66;03m# Unlike in Variable, always eagerly load values into memory\u001b[39;00m\n\u001b[1;32m   2842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data, PandasIndexingAdapter):\n\u001b[0;32m-> 2843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[43mPandasIndexingAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/indexing.py:1407\u001b[0m, in \u001b[0;36mPandasIndexingAdapter.__init__\u001b[0;34m(self, array, dtype)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, array: pd\u001b[38;5;241m.\u001b[39mIndex, dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1405\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindexes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m safe_cast_to_index\n\u001b[0;32m-> 1407\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m \u001b[43msafe_cast_to_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1410\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dtype \u001b[38;5;241m=\u001b[39m get_valid_numpy_dtype(array)\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/indexes.py:172\u001b[0m, in \u001b[0;36msafe_cast_to_index\u001b[0;34m(array)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m--> 172\u001b[0m     index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mIndex(np\u001b[38;5;241m.\u001b[39masarray(array), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_cast_to_cftimeindex(index)\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/indexing.py:524\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    523\u001b[0m     array \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray)\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(array[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/coding/variables.py:73\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/coding/times.py:279\u001b[0m, in \u001b[0;36mdecode_cf_datetime\u001b[0;34m(num_dates, units, calendar, use_cftime)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_cf_datetime\u001b[39m(\n\u001b[1;32m    263\u001b[0m     num_dates, units: \u001b[38;5;28mstr\u001b[39m, calendar: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, use_cftime: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    264\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    265\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Given an array of numeric dates in netCDF format, convert it into a\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    numpy array of date time objects.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;03m    cftime.num2date\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     num_dates \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(num_dates)\n\u001b[1;32m    280\u001b[0m     flat_num_dates \u001b[38;5;241m=\u001b[39m num_dates\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m calendar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/indexing.py:524\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    523\u001b[0m     array \u001b[38;5;241m=\u001b[39m as_indexable(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray)\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:91\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m---> 91\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplicit_indexing_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexing\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexingSupport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOUTER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/core/indexing.py:815\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[0;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m \n\u001b[1;32m    795\u001b[0m \u001b[38;5;124;03mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[38;5;124;03mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    814\u001b[0m raw_key, numpy_indices \u001b[38;5;241m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[0;32m--> 815\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mraw_indexing_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_key\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtuple\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m numpy_indices\u001b[38;5;241m.\u001b[39mtuple:\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;66;03m# index the loaded np.ndarray\u001b[39;00m\n\u001b[1;32m    818\u001b[0m     result \u001b[38;5;241m=\u001b[39m NumpyIndexingAdapter(np\u001b[38;5;241m.\u001b[39masarray(result))[numpy_indices]\n",
      "File \u001b[0;32m~/mambaforge/envs/dev/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:104\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper._getitem\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatastore\u001b[38;5;241m.\u001b[39mlock:\n\u001b[1;32m    103\u001b[0m         original_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_array(needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 104\u001b[0m         array \u001b[38;5;241m=\u001b[39m getitem(original_array, key)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe indexing operation you are attempting to perform \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data into memory first by calling .load().\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = ['HIRAM']\n",
    "experiments = ['control', 'ktc']\n",
    "storm_type = 'C15w'\n",
    "year_range = (101, 120)\n",
    "random_num = 10\n",
    "\n",
    "data = {model: {} for model in models}\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "    data[model] = {experiment: {} for experiment in experiments}\n",
    "    for experiment in experiments:\n",
    "        print('\\t {0}'.format(experiment))\n",
    "        track_output, global_model_output, tc_model_output = tc_storage(model, experiment, year_range, storm_type, random_num=random_num, benchmarking=True)\n",
    "        data[model][experiment]['track_output'] = track_output\n",
    "        data[model][experiment]['global_model_output'] = global_model_output\n",
    "        data[model][experiment]['tc_model_output'] = tc_model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b27c2c-4cb6-40e4-83fe-bdb9cd74ed51",
   "metadata": {},
   "source": [
    "Normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716466f-8ce9-4611-9ae1-3db239cbf402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var = 'U'\n",
    "data['AM2.5']['control']['tc_normalized_{0}'.format(var)] = tc_normalization(data['AM2.5']['control']['tc_model_output'], var=var)\n",
    "data['AM2.5']['ktc']['tc_normalized_{0}'.format(var)] = tc_normalization(data['AM2.5']['ktc']['tc_model_output'], var=var)\n",
    "(data['AM2.5']['ktc']['tc_normalized_{0}'.format(var)].mean('time') - data['AM2.5']['control']['tc_normalized_{0}'.format(var)].mean('time')).plot.contourf(levels=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbc635-44ea-4d7d-afa8-b985d75a9294",
   "metadata": {},
   "source": [
    "#### Storage and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c9078b3-ad6d-4134-b480-53d64eb3b445",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "dill.dump_session('../backup/202304061347.db')\n",
    "# dill.load_session('../backup/202303282020.db')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50418324-35a1-4cea-acdb-2caf7ec3a0f5",
   "metadata": {},
   "source": [
    "Radius estimation \"test unit\". Iterates through storm IDs and gets radii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc47f4-1dbc-4103-b050-f09a5271979d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = 'AM2.5'\n",
    "# experiment = 'control'\n",
    "# storm_type = 'C15w'\n",
    "# year_range = (101, 103)\n",
    "# storm_track_output, model_output, storm_model_output = tc_storage(model, experiment, year_range, storm_type, random_num=10, benchmarking=True)\n",
    "\n",
    "storms = storm_model_output.copy()\n",
    "\n",
    "start = time.time()\n",
    "N = 3\n",
    "\n",
    "for storm_id in list(set(storms['storm_id'].values))[:N]:\n",
    "    test_storm = storms.where(storms['storm_id'] == storm_id, drop=True).dropna(dim='grid_xt', how='all').dropna(dim='grid_yt', how='all')\n",
    "    print(test_storm['storm_id'].values)\n",
    "    print(radius_estimate(test_storm, box=15, overlay_check=False))\n",
    "    print('--------------------')\n",
    "    \n",
    "print('Radius estimation runtime per storm: {0:.3f} s'.format((time.time() - start)/N))\n",
    "\n",
    "# del model, experiment, storm_type, year_range, storm_track_output, model_output, storm_model_output, storm_id, test_storm, N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
