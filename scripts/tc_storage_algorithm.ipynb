{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c07d45bf-dfcf-416b-8fa3-32ece4e664be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "''' Import packages. '''\n",
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy\n",
    "# Local data storage packages\n",
    "import dill, os, pickle\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr\n",
    "# Visualization tools\n",
    "import cartopy.crs as ccrs, matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "7bfbc3dc-217d-4eea-a6c6-2683781723cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tc_storage(model, experiment, year_range, storm_type, random_num=None, benchmarking=True):\n",
    "    \n",
    "    ''' \n",
    "    Function to build unified xArray Dataset to hold model and track data for tracked TCs in a given model and experiment.\n",
    "    \n",
    "    Input(s):\n",
    "    - model (str):                name of model to access (typically \"AM2.5\" or \"HIRAM\")\n",
    "    - experiment (str):           name of experiment to access (typically \"control\", \"ktc\", \"plus2K\", etc.)\n",
    "    - year_range (tuple of ints): 2-element tuple with a start and end year\n",
    "    - storm_type (str):           type of storm to evaluate from TC tracks data (\"TS\" for all storms or \"C15w\" for hurricanes)\n",
    "    - benchmarking (bool):        boolean to enable time benchmarking\n",
    "    Output(s):\n",
    "    - dirs (tuple):               2-element tuple with pathnames for the model- and experiment-specific GCM and track directories.\n",
    "    '''\n",
    "    \n",
    "    if benchmarking:\n",
    "        start = time.time()\n",
    "    \n",
    "    # Define paths to model- and experiment-specific data.\n",
    "    model_dir, track_dir = dirnames(model, experiment)\n",
    "    # Retrieve tracked TCs over the specified year range. Note: year_range re-specified to ensure increasing order in tuple.\n",
    "    storm_track_output = retrieve_tracked_TCs(track_dir, storm_type, year_range=(min(year_range), max(year_range)))\n",
    "    # Retrieve model data over specified year range. \n",
    "    # This is here so that both TC-specific and associated global model data can be analyzed together.\n",
    "    model_output = retrieve_model_data(model_dir, year_range=(min(year_range), max(year_range)), output_type='atmos_daily')\n",
    "    # Retrieve model data specified to tracked TCs.\n",
    "    # Note: default model output type is 'atmos_4xdaily'. If 'output_type' matches 'model_output', then access pre-loaded data from 'model_output'.\n",
    "    storm_model_output = retrieve_model_TCs(model_dir, (min(year_range), max(year_range)), storm_track_output, \n",
    "                                            model_output, output_type='atmos_4xdaily', random_num=random_num)\n",
    "    \n",
    "    if benchmarking:\n",
    "        print('Runtime: {0:.3f} s'.format(time.time() - start))\n",
    "    \n",
    "    return storm_track_output, model_output, storm_model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "969fb8ac-4279-481e-bceb-0f0065cdaf44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dirnames(model, experiment):\n",
    "\n",
    "    ''' \n",
    "    Function to store pathnames for selected models and experiments. \n",
    "    \n",
    "    Input(s):\n",
    "    - model (str):  name of model to access (typically \"AM2.5\" or \"HIRAM\").\n",
    "    Output(s):\n",
    "    - dirs (tuple): 2-element tuple with pathnames for the model- and experiment-specific GCM and track directories.\n",
    "    '''\n",
    "    \n",
    "    # Define experiment-specific pathnames\n",
    "    experiments = {'control': {'AM2.5': 'CTL1990s_tigercpu_intelmpi_18_540PE',\n",
    "                               'HIRAM': 'CTL1990s_v201910_tigercpu_intelmpi_18_540PE'},\n",
    "                   'ktc': {'AM2.5': 'CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE',\n",
    "                           'HIRAM': 'CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE'}}\n",
    "    # Model directories\n",
    "    model_dirs = {'AM2.5': {'control': '/tigress/wenchang/analysis/TC/AM2.5/CTL1990s_tigercpu_intelmpi_18_540PE/model_out/POSTP',\n",
    "                            'ktc': '/tigress/wenchang/analysis/TC/AM2.5ktc2/CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE/modelout/POSTP'},\n",
    "                  'HIRAM': {'control': '/tigress/wenchang/analysis/TC/HIRAM/CTL1990s_v201910_tigercpu_intelmpi_18_540PE/model_out/POSTP',\n",
    "                            'ktc': '/tigress/wenchang/analysis/TC/HIRAMktc2/CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE/modelout/POSTP'}}\n",
    "    # Track directories\n",
    "    # Potential issue: ask Wenchang about using track .nc files in the same parent directory tree as the model data\n",
    "    track_dirs = {'AM2.5': {'control': '/tigress/wenchang/MODEL_OUT/AM2.5/CTL1990s_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k',\n",
    "                            'ktc': '/tigress/wenchang/MODEL_OUT/AM2.5ktc2/CTL1990s_killtc13-13-15_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k'},\n",
    "                  'HIRAM': {'control': '/tigress/wenchang/MODEL_OUT/HIRAM/CTL1990s_v201910_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_2p5C_330k',\n",
    "                            'ktc': '/tigress/wenchang/MODEL_OUT/HIRAMktc2/CTL1990s_v201910_killtc13-13-15_tigercpu_intelmpi_18_540PE/analysis_lmh/cyclones_gav_ro110_1C_330k'}}\n",
    "\n",
    "    dirs = (model_dirs[model][experiment], track_dirs[model][experiment])\n",
    "    \n",
    "    return dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "0d4303b6-d456-46b3-b84b-45090ebe2bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lmh_parser(path):\n",
    "    \n",
    "    ''' \n",
    "    This method parses through text files from Lucas Harris' run outputs (held in directories titled 'analysis_lmh') \n",
    "    and produces an output DataFrame. \n",
    "    \n",
    "    Input(s):\n",
    "    - path (str):            path containing raw tracker data from Lucas Harris' runs.\n",
    "    Output(s):\n",
    "    - df (Pandas DataFrame): Pandas DataFrame containing tracked TC data\n",
    "    '''\n",
    "    \n",
    "    # Create file object instance\n",
    "    fobj = open(path, 'r').readlines()\n",
    "    # Initialize dictionary to hold data\n",
    "    data = {'storm_num': {}}\n",
    "    # Initialize storm counter\n",
    "    count = 1\n",
    "    # Iterate through text file\n",
    "    for line in fobj:\n",
    "        # Extract information from the line\n",
    "        content = line.strip()\n",
    "        # Creates new storm-specific dict in the parent dict. The '+++' demarcates a new storm.\n",
    "        if '+++' in line:\n",
    "            storm_num = '{0:04d}'.format(count)\n",
    "            data['storm_num'][storm_num] = {'storm_id': [], 'time': [], 'lon': [], 'lat': [], 'slp': [], 'max_wnd': [], 'flag': []}\n",
    "            count += 1\n",
    "        # Populates the storm-specific dict\n",
    "        else:\n",
    "            storm_num = '{0:04d}'.format(count-1) \n",
    "            tc_info = [x for x in content.split(' ') if x]\n",
    "            year = tc_info[0][0:4] # get 4-digit year\n",
    "            data['storm_num'][storm_num]['storm_id'].append('{0}-{1:04d}'.format(year, count-1))\n",
    "            data['storm_num'][storm_num]['time'].append(tc_info[0])\n",
    "            data['storm_num'][storm_num]['lon'].append(tc_info[1])\n",
    "            data['storm_num'][storm_num]['lat'].append(tc_info[2])\n",
    "            data['storm_num'][storm_num]['slp'].append(tc_info[3])\n",
    "            data['storm_num'][storm_num]['max_wnd'].append(tc_info[4])\n",
    "            data['storm_num'][storm_num]['flag'].append(tc_info[5])\n",
    "    \n",
    "    try:\n",
    "        # Converts the dictionary into a DataFrame\n",
    "        df = pd.concat({k: pd.DataFrame(v).T for k, v in data.items()}, axis=1)['storm_num']\n",
    "        df = df.explode(df.columns.to_list()).reset_index().rename(columns={'index': 'storm_num'})\n",
    "        # Re-cast column data types\n",
    "        df = df.astype({'lon': 'float', 'lat': 'float', 'slp': 'float', 'max_wnd': 'float', 'flag': 'float'})\n",
    "    except:\n",
    "        df = pd.DataFrame(columns=['storm_id', 'time', 'lon', 'lat', 'slp', 'max_wnd', 'flag'])\n",
    "    \n",
    "    ''' DataFrame refinement. '''\n",
    "    # Remove cold-core data points (flag == -1)\n",
    "    df = df.loc[df['flag'] != -1].reset_index(drop=True)\n",
    "    # Convert timestamps to datetime objects\n",
    "    df['time'] = pd.to_datetime(df['time'], format='%Y%m%d%H')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "76e43711-3711-4da6-a6c8-274a2c301b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def coords_to_dist(a, b):\n",
    "    ''' Convert coordinates to distance in meters. '''\n",
    "    \n",
    "    R = 6371e3\n",
    "    \n",
    "    lon_a, lat_a = np.array(a)*np.pi/180\n",
    "    lon_b, lat_b = np.array(b)*np.pi/180\n",
    "    \n",
    "    dlon, dlat = lon_b - lon_a, lat_b - lat_a\n",
    "    \n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat_a)*np.cos(lat_b)*np.sin(dlon/2)**2    \n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    \n",
    "    distance = R*c\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "1bacb5f3-1e9a-45be-9b9b-77d094415a53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_tracked_TCs(dirname, storm_type, year_range):\n",
    "\n",
    "    '''\n",
    "    Function to collect tracked TC data and add derived data, such as duration and storm speed.\n",
    "    \n",
    "    Input(s):\n",
    "    - dirname (str):              name of directory containing files of interest\n",
    "    - storm_type (str):           type of storm to evaluate from TC tracks data (\"TS\" for all storms or \"C15w\" for hurricanes)\n",
    "    - year_range (tuple of ints): 2-element tuple with a start and end year\n",
    "    Output(s):\n",
    "    - data (Pandas DataFrame):    Pandas DataFrame with tracked TC data\n",
    "    '''\n",
    "    \n",
    "    ''' File collection. '''\n",
    "    # Get filenames for all files within the specified directory \n",
    "    # Filenames will correspond to the determined storm type\n",
    "    fnames = [[os.path.join(dirname, file, 'Harris.TC', f) for f in os.listdir(os.path.join(dirname, file, 'Harris.TC')) \n",
    "               if '{0}.world'.format(storm_type) in f]\n",
    "               for file in sorted(os.listdir(dirname))]\n",
    "    # Compress 2D list to 1D list\n",
    "    fnames = [item for sublist in fnames for item in sublist]\n",
    "\n",
    "    # Select files with dates within 'year_range'\n",
    "    # Note: the '+ 1900' is added because tracked TCs are on the 2000 year range, whereas model output is on the 100 year range\n",
    "    fnames = [f for f in fnames \n",
    "              if min(year_range) + 1900 <= pd.to_datetime(f.split('.')[-2].split('-')[0]).year < max(year_range) + 1900]\n",
    "    \n",
    "    # Concatenate all tracked TC data from the filename list\n",
    "    data = pd.concat([lmh_parser(os.path.join(dirname, fname)) for fname in fnames])\n",
    "    \n",
    "    ''' Derived track-based data algorithm. Storm-specific derived properties will be generated in here. '''\n",
    "    \n",
    "    # Initialize empty duration column to populate iteratively\n",
    "    data[['duration', 'speed', 'direction']] = np.nan\n",
    "    # Initialize list to populate iteratively for each storm, then concatenate\n",
    "    storms = []\n",
    "    # Iterate through each unique storm (identify by 'storm_id') and get duration\n",
    "    for storm_id in data['storm_id'].unique():\n",
    "        # Define iterand storm\n",
    "        storm = data.loc[data['storm_id'] == storm_id].copy().reset_index(drop=True)\n",
    "        \n",
    "        ''' Duration derivation. '''\n",
    "        # Get difference between minimum and maximum timestamps\n",
    "        dt = (storm['time'].max() - storm['time'].min())\n",
    "        # Convert difference timedelta into hours\n",
    "        dt = dt.days + dt.seconds/86400\n",
    "        # Add duration to the outer DataFrame for the corresponding storm\n",
    "        data.loc[data['storm_id'] == storm_id, 'duration'] = dt\n",
    "        # Re-define iterand storm to incorporate duration\n",
    "        storm = data.loc[data['storm_id'] == storm_id].copy().reset_index(drop=True)\n",
    "        \n",
    "        ''' Velocity (speed, direction) derivation. '''\n",
    "        # Initialize dictionary for preliminary storage. Will be reassigned into the DataFrame by the join() method using time as the matching criterion.\n",
    "        velocity = {'time': [storm.iloc[0]['time']], 'speed': [np.nan], 'direction': [np.nan]}\n",
    "        # Iterate over all of the iterand storm timestamps\n",
    "        for i in range(1, len(storm)):\n",
    "            # Define coordinates for two points considered (i, i-1)\n",
    "            lon_a, lat_a = [storm.iloc[i-1]['lon'], storm.iloc[i-1]['lat']]\n",
    "            lon_b, lat_b = [storm.iloc[i]['lon'], storm.iloc[i]['lat']]\n",
    "            # Determine timedelta between points (i, i-1)\n",
    "            dt = storm.iloc[i]['time'] - storm.iloc[i-1]['time']\n",
    "            # Derive speed (distance / time in m s^-1)\n",
    "            speed = coords_to_dist((lon_b, lat_b), (lon_a, lat_a))/dt.seconds\n",
    "            # Get changes in longtiude and latitude\n",
    "            dlon, dlat = lon_b - lon_a, lat_b - lat_a\n",
    "            # Derive direction relative to north (range of 0 to 360)\n",
    "            direction = 180*np.arctan(dlon/dlat)/np.pi % 360\n",
    "            # Append quantities to the 'velocity' dictionary\n",
    "            velocity['time'].append(storm.iloc[i]['time'])    \n",
    "            velocity['speed'].append(speed)    \n",
    "            velocity['direction'].append(direction)\n",
    "        # Build DataFrame\n",
    "        velocity = pd.DataFrame(velocity)\n",
    "        # Re-cast time column as a datetime object\n",
    "        velocity['time'] = pd.to_datetime(velocity['time'])\n",
    "        # Merge the storm and velocity DataFrames\n",
    "        storm = storm.merge(velocity, how='left', on='time', suffixes=['_x', None]).drop(columns={'speed_x', 'direction_x'}).reset_index(drop=True)\n",
    "        # Append to the list for future concatenation\n",
    "        storms.append(storm)\n",
    "        \n",
    "    # Concatenate DataFrames\n",
    "    data = pd.concat(storms)   \n",
    "    # Rename columns for future addition into xArray Dataset, and reset index\n",
    "    data = data.rename(columns={'lon': 'center_lon', 'lat': 'center_lat', 'flag': 'core_temp', 'slp': 'min_slp'}).reset_index(drop=True)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d7fda4cc-e3a9-447c-8c59-7d9521f49f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pull_gcm_data(dirname, year, output_type):\n",
    "    \n",
    "    ''' Method to read data for given parameters. '''\n",
    "    \n",
    "    # Get filenames for corresponding files\n",
    "    filename = '{0:04d}0101.{1}.nc'.format(year, output_type)\n",
    "    try:\n",
    "        fname = os.path.join(dirname, filename)\n",
    "        # Retrieve data\n",
    "        data = xr.open_dataset(fname)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "18cff38e-7fd0-4c50-bc59-209984d5ef97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_model_data(dirname, year_range, output_type='atmos_daily'):\n",
    "    \n",
    "    '''\n",
    "    Method to open experiment-specific GCM output data for a specified year range and model output type.\n",
    "    \n",
    "    Input(s):\n",
    "    - dirname (str):            directory name for the given model and experiment type.\n",
    "    - year_range (tuple, list): tuple or list (minimum of 2 items) containing year range for desired data.\n",
    "    - output_type (str):        string denoting GCM output desired.\n",
    "    Output(s):\n",
    "    - data (xArray Dataset):    xArray Dataset containing concatenated data for the year range selected.\n",
    "    '''\n",
    "    \n",
    "    # Identifier substring definition. This will be used for splitting the filename for identification purposes.\n",
    "    substring = '0101.'\n",
    "    # Access parent directory with experiment-specific model data and list directories corresponding to year range\n",
    "    files = [os.path.join(dirname, file) for file in os.listdir(dirname) for year in range(min(year_range), max(year_range)) \n",
    "             if str(year) in file.split(substring)[0] and output_type in file]\n",
    "    # Store file data into an xArray Dataset.\n",
    "    # Note: benchmarking showed ~1.5 s for opening 4 files using 'open_mfdataset', and at least 10x longer using 'open_dataset' + 'xr.concat'\n",
    "    data = xr.open_mfdataset(files)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "34600793-0d64-4796-a27e-0bbebff07eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def retrieve_model_TCs(dirname, year_range, storms, model_output=None, output_type='atmos_4xdaily', extent=10, random_num=None):\n",
    "    \n",
    "    # Check to see if model_output (previously-access model data) is the same output type as desired for TCs. If so, pull from that Dataset.\n",
    "    if model_output and output_type == model_output.attrs['filename'].split('.')[0]:\n",
    "        data = model_output\n",
    "    else:\n",
    "        # Identifier substring definition. This will be used for splitting the filename for identification purposes.\n",
    "        substring = '0101.'\n",
    "        # Access parent directory with experiment-specific model data and list directories corresponding to year range\n",
    "        files = [os.path.join(dirname, file) for file in os.listdir(dirname) for year in range(min(year_range), max(year_range)) \n",
    "                 if str(year) in file.split(substring)[0] and output_type in file]\n",
    "        # Store file data into an xArray Dataset.\n",
    "        data = xr.open_mfdataset(files)\n",
    "    \n",
    "    # Initialize list to hold each storm Dataset for future concatenation\n",
    "    storms_xr = []\n",
    "    # Define range of storm IDs to iterate over. \n",
    "    # If 'random_num' is defined, get that many randomized storms. Otherwise, get all.\n",
    "    storm_ids = random.sample(list(storms['storm_id'].unique()), random_num) if random_num else storms['storm_id'].unique()\n",
    "    # Access model data that are specific to each tracked TC.\n",
    "    for storm_id in storm_ids:\n",
    "        # Pull tracked TC data relative to storm\n",
    "        storm = storms.loc[storms['storm_id'] == storm_id]\n",
    "        # Initialize list to hold Dataset entries for each storm timestamp\n",
    "        storm_xr = []\n",
    "        # Iterate over each storm Series\n",
    "        for i in range(0, len(storm)):                   \n",
    "            # Convert from tracked TC timestamp convention (datetime) to model timestamp convention (cftime DatetimeNoLeap)\n",
    "            cf_timestamp = cftime.DatetimeNoLeap(year=storm.iloc[i]['time'].year-1900, month=storm.iloc[i]['time'].month, \n",
    "                                                 day=storm.iloc[i]['time'].day, hour=storm.iloc[i]['time'].hour)\n",
    "            # Take snapshot of data at this timestamp\n",
    "            # Note 1: this is where the connection between track and model output data happens\n",
    "            # Note 2: any operations beyond selection result in active computation, which disrupts the 'lazy' approach\n",
    "            snapshot = data.sel(time=cf_timestamp)\n",
    "            # Get iterand information to append to Dataset for the given timestamp\n",
    "            snapshot[['center_lon', 'center_lat', 'min_slp', 'max_wnd', 'core_temp', 'speed']] = [storm.iloc[i]['center_lon'], storm.iloc[i]['center_lat'], storm.iloc[i]['min_slp'],\n",
    "                                                                                                  storm.iloc[i]['max_wnd'], storm.iloc[i]['core_temp'],\n",
    "                                                                                                  storm.iloc[i]['speed']]  \n",
    "            # Append to list\n",
    "            storm_xr.append(snapshot)\n",
    "        # Concatenate into unified Dataset for a storm\n",
    "        storm_xr = xr.concat(storm_xr, dim='time')\n",
    "        # Assign storm identifier (storm_id)\n",
    "        # Note: to select specific storm from concatenated Dataset, use command as follows:\n",
    "        #       storms.where(storms['storm_id'] == <storm_id>, drop=True)\n",
    "        storm_xr['storm_id'] = storm_id\n",
    "    \n",
    "        ''' Spatial clipping - only at maximum intensity to save computation time. '''\n",
    "        # Grab storm at timestamp with maximum wind speed\n",
    "        # Calling this 'lifetime maximum intensity', or 'LMI', to use terminology from Wing et al, 2016 (10.1175/JCLI-D-18-0599.1)\n",
    "        lmi = storm_xr.where(storm_xr['max_wnd'] == storm_xr['max_wnd'].max(), drop=True)\n",
    "        # If multiple wind maxima are detected, get the storm with lower pressure\n",
    "        lmi = lmi.where(lmi['min_slp'] == lmi['min_slp'].min(), drop=True) if len(lmi.time.values) > 1 else lmi\n",
    "        # Clip storm        \n",
    "        lmi = lmi.sel(grid_xt=np.arange(lmi['center_lon']-extent, lmi['center_lon']+extent), \n",
    "                      grid_yt=np.arange(lmi['center_lat']-extent, lmi['center_lat']+extent), method='nearest')\n",
    "        \n",
    "        # Append to list for future concatenation\n",
    "        storms_xr.append(lmi)\n",
    "        \n",
    "    # Concatenate into single xArray Dataset\n",
    "    storms_xr = xr.concat(storms_xr, dim='time')\n",
    "    \n",
    "    return storms_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "be6a6ece-d7b1-4e08-8e39-e2cecc18ce89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def radius_estimate(storm, box, overlay_check=True):\n",
    "    \n",
    "    '''\n",
    "    Algorithm to estimate the radius of a TC based on filters set below.\n",
    "    Returns a radius in meters.\n",
    "    \n",
    "    Note 1: the idea here is: \n",
    "            (1) regrid data to higher resolution by interpolation, \n",
    "            (2) smooth data and use gradient-based filtering to prevent dual-vortex identification,\n",
    "            (3) identify thresholds for value-based filtering,\n",
    "            (4) use gradient- and value-based filtering to identify data points that match criteria,\n",
    "            (5) clip data and estimate radius from filtered data\n",
    "    '''\n",
    "    \n",
    "    # Ensure that only one timestamp is in the Dataset\n",
    "    try:\n",
    "        storm = storm.isel(time=0)\n",
    "    except:\n",
    "        storm = storm\n",
    "    # Ensure that Dataset is from the 'atmos_4xdaily' output types\n",
    "    if storm.attrs['filename'].split('.')[0] != 'atmos_4xdaily':\n",
    "        return np.nan\n",
    "    # Derives horizontal wind speed (proxy for azimuthal wind)\n",
    "    if 'U' not in storm.data_vars.keys():\n",
    "        storm['U'] = np.sqrt(storm['u_ref']**2 + storm['v_ref']**2)\n",
    "    \n",
    "    ''' Perform linear interpolation to allow for better gradient estimation for future filtering. '''\n",
    "    # Pull numerical data from parameters relevant to radius estimation\n",
    "    params = ['grid_xt', 'grid_yt', 'vort850', 'tm', 'slp', 'U']\n",
    "    # Define the interpolation resolution (in degrees) and spatial extent of clipping\n",
    "    resolution, extent = 0.5, 15\n",
    "    # Define dictionary to store data in\n",
    "    params = {param: [] for param in params}\n",
    "    # Iterate through parameters and interpolate\n",
    "    for param in params.keys():\n",
    "        if param == 'grid_xt':\n",
    "            params[param] =  np.arange(storm['center_lon'] - extent, storm['center_lon'] + extent, resolution)\n",
    "        elif param == 'grid_yt':\n",
    "            params[param] =  np.arange(storm['center_lat'] - extent, storm['center_lat'] + extent, resolution)\n",
    "        else:\n",
    "            params[param] =  storm[param].interp(grid_xt=np.arange(storm['center_lon'] - extent, storm['center_lon'] + extent, resolution), \n",
    "                                                 grid_yt=np.arange(storm['center_lat'] - extent, storm['center_lat'] + extent, resolution)).values\n",
    "\n",
    "        \n",
    "    ''' \n",
    "    Data smoothing and gradient filtering algorithm. \n",
    "    The idea here is to use gradients for a chosen field to isolate TC extent and prevent dual-vortex pickup for a given storm, which distorts radius calculation. \n",
    "    '''\n",
    "    # 1 hPa/deg pressure gradient (attempt at similarity to Harris)\n",
    "    diff_var, diff_val = 'slp', 1*resolution\n",
    "    # Use a 1-sigma Gaussian smoothing filter\n",
    "    smoothed = scipy.ndimage.gaussian_filter(np.abs(np.diff(np.diff(params[diff_var], axis=0), axis=1)), sigma=1)\n",
    "    # Apply the filter and resize such that filter boolean array shape matches the data array shape\n",
    "    diff_filter = smoothed > diff_val\n",
    "    diff_filter = np.hstack((diff_filter, np.full((diff_filter.shape[0], 1), False)))\n",
    "    diff_filter = np.vstack((diff_filter, np.full((1, diff_filter.shape[1]), False)))\n",
    "    \n",
    "    ''' Apply thresholds to absolute values of identified parameters. '''\n",
    "    # Define number of standard deviations to analyze\n",
    "    sigma = 1\n",
    "    # Exact magnitude thresholds\n",
    "    filters = {'vort850': np.abs(params['vort850']) > 1.5e-4,\n",
    "               'tm': params['tm'] > (np.nanmean(params['tm']) + sigma*np.nanstd(params['tm'].std())),\n",
    "               'slp': params['slp'] < 1000,\n",
    "               'U': params['U'] > 15}\n",
    "    \n",
    "    ''' Perform the filtering and associated array clipping. '''\n",
    "    # Define the conditional based on the threshold and gradient filters\n",
    "    conditional = (filters['vort850'] & filters['slp'] & filters['U'] & diff_filter)\n",
    "    # Define variable for filtering on\n",
    "    filter_var = 'slp'\n",
    "    # Perform filtering based on chosen variable\n",
    "    filtered = np.where(conditional, params[filter_var], np.nan)\n",
    "    # Crop all-nan rows in the zonal and meridional (x- and y-) array axes\n",
    "    crop_x, crop_x_idx = filtered[~np.all(np.isnan(filtered), axis=1), :], ~np.all(np.isnan(filtered), axis=1)\n",
    "    crop_y, crop_y_idx = crop_x[:, ~np.all(np.isnan(crop_x), axis=0)], ~np.all(np.isnan(crop_x), axis=0)\n",
    "    # Output masked array for visualization of algorithm output\n",
    "    arr = np.ma.masked_values(filtered, np.nan)\n",
    "    \n",
    "    ''' Derive radius, if the filtered array is not empty. '''\n",
    "    # If filtering results in populated output array, get a radius\n",
    "    if crop_y.shape != (0, 0):\n",
    "        # If there's a mismatch in grid sizes, crop the larger one\n",
    "        if params['grid_xt'].shape != params['grid_yt'].shape:\n",
    "            if params['grid_xt'].shape[0] > params['grid_yt'].shape[0]:\n",
    "                dshape = (0, params['grid_xt'].shape[0] - params['grid_yt'].shape[0]) \n",
    "                cut = params['grid_yt'].shape[0]\n",
    "                # Perform the slicing\n",
    "                params['grid_xt'] = params['grid_xt'][:cut]\n",
    "                crop_y_idx = crop_y_idx[:cut]\n",
    "            else:\n",
    "                dshape = (params['grid_yt'].shape[0] - params['grid_xt'].shape[0], 0) \n",
    "                cut = params['grid_xt'].shape[0]\n",
    "                # Perform the slicing\n",
    "                params['grid_yt'] = params['grid_yt'][:cut]\n",
    "                crop_x_idx = crop_x_idx[:cut]\n",
    "                \n",
    "        # Get the longitude and latitude extrema corresponding to the filtered array\n",
    "        lons = np.min(params['grid_xt'][crop_x_idx]), np.max(params['grid_xt'][crop_x_idx])\n",
    "        lats = np.min(params['grid_yt'][crop_y_idx]), np.max(params['grid_yt'][crop_y_idx])\n",
    "        # Get the coordinate extrema for radius derivation\n",
    "        coords = [lons[0], lats[0]], [lons[1], lats[1]]\n",
    "        # Derive radius from coordinate pairs (divide by 2 and divide by 1000 to go from diameter to radius and m to km)\n",
    "        radius = coords_to_dist(coords[0], coords[1])/2000\n",
    "        \n",
    "        # Overlay the storm size algorithm output on maps of the storms \n",
    "        if overlay_check:\n",
    "            # Define the plot basics\n",
    "            fig, ax = plt.subplots(subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "            ax.coastlines()\n",
    "            ax.set_extent([storm['center_lon']-extent, storm['center_lon']+extent, storm['center_lat']-extent, storm['center_lat']+extent])\n",
    "            # Plot data\n",
    "            im = ax.contourf(params['grid_xt'], params['grid_yt'][:-1], smoothed, levels=16)\n",
    "            ax.pcolormesh(params['grid_xt'], params['grid_yt'], arr[:-1, :-1], \n",
    "                          zorder=9, cmap='Reds', transform=ccrs.PlateCarree())\n",
    "            # Plot metadata\n",
    "            ax.set_title('radius: {0:.2f} km'.format(radius))\n",
    "            fig.colorbar(im)\n",
    "            \n",
    "        return radius\n",
    "    # Else, return nan\n",
    "    else: \n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "94aa2604-9a4d-4597-8181-ea8ca9805f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 6.660 s\n"
     ]
    }
   ],
   "source": [
    "model = 'AM2.5'\n",
    "experiment = 'control'\n",
    "storm_type = 'C15w'\n",
    "year_range = (101, 103)\n",
    "storm_track_output, model_output, storm_model_output = tc_storage(model, experiment, year_range, storm_type, random_num=10, benchmarking=True)\n",
    "\n",
    "del model, experiment, storm_type, year_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "dfbc47f4-1dbc-4103-b050-f09a5271979d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2001-0017']\n",
      "170.38940114639078\n",
      "--------------------\n",
      "['2001-0003']\n",
      "160.02041974889528\n",
      "--------------------\n",
      "['2002-0030']\n",
      "155.21294906240502\n",
      "--------------------\n",
      "['2002-0024']\n",
      "191.06290467726961\n",
      "--------------------\n",
      "['2002-0027']\n",
      "174.21778266216225\n",
      "--------------------\n",
      "['2001-0002']\n",
      "117.6641614647327\n",
      "--------------------\n",
      "['2002-0018']\n",
      "186.4414358222715\n",
      "--------------------\n",
      "['2001-0026']\n",
      "156.0040320401423\n",
      "--------------------\n",
      "['2001-0022']\n",
      "173.3825344452795\n",
      "--------------------\n",
      "['2002-0015']\n",
      "205.75047858672755\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "storms = storm_model_output.copy()\n",
    "storm_ids = list(set(storms['storm_id'].values))\n",
    "# test_storm = storms.where(storms['storm_id'] == random.sample(storm_ids, 1), drop=True).dropna(dim='grid_xt', how='all').dropna(dim='grid_yt', how='all')\n",
    "\n",
    "for storm_id in list(set(storms['storm_id'].values)):\n",
    "    test_storm = storms.where(storms['storm_id'] == storm_id, drop=True).dropna(dim='grid_xt', how='all').dropna(dim='grid_yt', how='all')\n",
    "    print(test_storm['storm_id'].values)\n",
    "    print(radius_estimate(test_storm, box=15, overlay_check=False))\n",
    "    print('--------------------')\n",
    "    # del test_storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da72183c-f954-4789-9a0c-ba03d79afd61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
