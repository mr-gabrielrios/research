{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7512a8be-c948-4484-a091-f746bb61f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy, numba\n",
    "# Local data storage packages\n",
    "import functools, os, pickle, collections, sys, importlib\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr, nc_time_axis\n",
    "xr.set_options(keep_attrs=True)\n",
    "# Visualization tools\n",
    "import cartopy, cartopy.crs as ccrs, matplotlib, matplotlib.pyplot as plt\n",
    "# Local imports\n",
    "import accessor, composite, composite_snapshots, derived, ibtracs, utilities, socket, visualization, tc_analysis, tc_processing, track_TCs, TC_tracker\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "importlib.reload(TC_tracker);\n",
    "importlib.reload(track_TCs);\n",
    "importlib.reload(ibtracs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c85199-725d-4aef-a40f-f2f57cc38414",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "- IBTrACS data provides observational data that is more intense than ERA5 outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49f95d-0468-4ecf-9590-88d0c88a6176",
   "metadata": {},
   "source": [
    "#### Load and format reanalysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb9a9d8-9cf0-4947-95f3-ea31c03bf30a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def access_IBTrACS_tracks(basin_name: str,\n",
    "                          date_range: tuple[str, str],\n",
    "                          intensity_parameter: str,\n",
    "                          intensity_range: tuple[int, int]) -> pd.DataFrame:\n",
    "    \n",
    "    track_data = ibtracs.main(basin_name=basin_name,\n",
    "                              intensity_parameter=intensity_parameter,\n",
    "                              intensity_range=intensity_range)\n",
    "    \n",
    "    return track_data.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92113dee-9c99-4c08-b37f-77c2a00f57cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_reanalysis_data():\n",
    "\n",
    "    # Load reanalysis data\n",
    "    # Assumes that all files in `reanalysis_dirname` have congruent coordinates, like with ERA5\n",
    "    reanalysis_dirname = '/scratch/gpfs/GEOCLIM/gr7610/tiger3/reference/datasets/ERA5'\n",
    "    reanalysis_filenames = [filename for filename in os.listdir(reanalysis_dirname) if\n",
    "                            filename.endswith('nc')]\n",
    "    reanalysis_pathnames = [os.path.join(reanalysis_dirname, reanalysis_filename) for reanalysis_filename in reanalysis_filenames]\n",
    "    reanalysis_data = xr.open_mfdataset(reanalysis_pathnames).drop_vars('sp')\n",
    "    \n",
    "    # Rename coordinates and data variables to adjust to GFDL QuickTracks outputs\n",
    "    # This naming convention follows ERA5 outputs\n",
    "    reanalysis_data = reanalysis_data.rename({'valid_time': 'time',\n",
    "                                              'u10': 'u_ref',\n",
    "                                              'v10': 'v_ref',\n",
    "                                              't2m': 't_ref',\n",
    "                                              'sst': 't_surf',\n",
    "                                              'msl': 'slp',\n",
    "                                              'tp': 'precip',\n",
    "                                              'tcwv': 'WVP'})\n",
    "    \n",
    "\n",
    "    return reanalysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a49d90c-327d-4f93-a409-1ab6bcc33296",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def field_correction(reanalysis_dataset: xr.Dataset) -> xr.Dataset:\n",
    "\n",
    "    ''' \n",
    "    Modify data to adjust for fields that do not readily provide desired units. \n",
    "    This is primarily performed to accommodate fields from 'accumulated' datasets in ERA5.\n",
    "    '''\n",
    "\n",
    "    # Define fields termed 'accumulated' from ERA5 - this represents parameters integrated hourly\n",
    "    accumulated_fields = {'slhf': 'lhflx',\n",
    "                          'sshf': 'shflx',\n",
    "                          'ssr': 'swnet_sfc',\n",
    "                          'ssrd': 'swdn_sfc',\n",
    "                          'str': 'lwnet_sfc',\n",
    "                          'strd': 'lwdn_sfc',\n",
    "                          'tisr': 'swdn_toa',\n",
    "                          'tsr': 'swnet_toa',\n",
    "                          'ttr': 'olr'}\n",
    "    accumulated_factor = 1 / 3600 # converts from J m^-2 to W m^-2\n",
    "\n",
    "    # Iterate through all fields and perform correction\n",
    "    for accumulated_field_name, accumulated_field_rename in accumulated_fields.items():\n",
    "        reanalysis_dataset[accumulated_field_name] = reanalysis_dataset[accumulated_field_name] * accumulated_factor\n",
    "        reanalysis_dataset = reanalysis_dataset.rename({accumulated_field_name: accumulated_field_rename})\n",
    "\n",
    "    return reanalysis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f117ab2a-0d10-4c7c-b935-bd049d7d9bb9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def derived_fields(reanalysis_dataset: xr.Dataset) -> xr.Dataset:\n",
    "\n",
    "    ''' Derive certain fields for reanalysis data to match GCM output conventions. '''\n",
    "\n",
    "    # Correct pressure units from Pa to hPa\n",
    "    reanalysis_dataset['slp'] = reanalysis_dataset['slp'] / 100\n",
    "    \n",
    "    # Correct sign conventions\n",
    "    sign_correction_field_names = ['olr', 'lhflx', 'shflx']\n",
    "    for sign_correction_field_name in sign_correction_field_names:\n",
    "        reanalysis_dataset[sign_correction_field_name] = reanalysis_dataset[sign_correction_field_name] * -1\n",
    "\n",
    "    # Upwards longwave radiative flux at surface\n",
    "    assert 'lwnet_sfc' in reanalysis_dataset.data_vars and 'lwdn_sfc' in reanalysis_dataset.data_vars, f'Fields lwnet_sfc and lwdn_sfc must be in dataset to compute lwup_sfc.'\n",
    "    reanalysis_dataset['lwup_sfc'] = reanalysis_dataset['lwnet_sfc'] - reanalysis_dataset['lwdn_sfc']\n",
    "\n",
    "    # Upwards shortwave radiative flux at surface\n",
    "    assert 'swnet_sfc' in reanalysis_dataset.data_vars and 'swdn_sfc' in reanalysis_dataset.data_vars, f'Fields swnet_sfc and swdn_sfc must be in dataset to compute swup_sfc.'\n",
    "    reanalysis_dataset['swup_sfc'] = reanalysis_dataset['swnet_sfc'] - reanalysis_dataset['swdn_sfc']\n",
    "\n",
    "    # Upwards shortwave radiative flux at TOA\n",
    "    assert 'swnet_toa' in reanalysis_dataset.data_vars and 'swdn_sfc' in reanalysis_dataset.data_vars, f'Fields swnet_toa and swdn_toa must be in dataset to compute swup_toa.'\n",
    "    reanalysis_dataset['swup_toa'] = reanalysis_dataset['swnet_toa'] - reanalysis_dataset['swdn_sfc']\n",
    "\n",
    "    # Net radiation at TOA\n",
    "    assert 'swnet_toa' in reanalysis_dataset.data_vars and 'olr' in reanalysis_dataset.data_vars, f'Fields swnet_toa and olr must be in dataset to compute netrad_toa.'\n",
    "    reanalysis_dataset['netrad_toa'] = reanalysis_dataset['swnet_toa'] - reanalysis_dataset['olr']\n",
    "\n",
    "    return reanalysis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b9ec8c-a114-44c2-b540-958d021211da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_reanalysis_timestamps(TC_track_dataset: pd.DataFrame,\n",
    "                              reanalysis_data: xr.Dataset) -> list:\n",
    "    \n",
    "    # Select random timestamps from each dataset to ensure they are the same\n",
    "    # Assume all timestamps within a dataset have the same timestamp type\n",
    "    random_storm_timestamp = random.choice(TC_track_dataset.time.values)\n",
    "    random_reanalysis_timestamp = random.choice(reanalysis_data.time.values)\n",
    "\n",
    "    # Ensure timestamps are identically-typed\n",
    "    check_timestamp_formats = type(random_storm_timestamp) == type(random_reanalysis_timestamp)\n",
    "    assert check_timestamp_formats, 'Timestamp types between IBTrACS and reanalysis require alignment.'\n",
    "    \n",
    "    # Iterate through storm timestamps to make sure they are in the reanalysis data\n",
    "    reanalysis_storm_timestamps = [storm_timestamp for storm_timestamp in TC_track_dataset.time.values if \n",
    "                                   storm_timestamp in reanalysis_data.time.values]\n",
    "\n",
    "    return reanalysis_storm_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f4f1a22-b45e-4225-a67b-5c881907a7be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_coordinates(TC_track_dataset: pd.DataFrame,\n",
    "                          reanalysis_data: xr.Dataset,\n",
    "                          reanalysis_storm_timestamps: list,\n",
    "                          reanalysis_resolution: float) -> dict:\n",
    "\n",
    "    interval_round = lambda x, y: y * round(x / y) # round coordinates to nearest dataset coordinates\n",
    "    \n",
    "    # Initialize dictionary for storm track coordinates\n",
    "    storm_track_coordinates = {}\n",
    "    # Construct dictionary for coordinates pertaining to each storm timestamp\n",
    "    for reanalysis_storm_timestamp in reanalysis_storm_timestamps:\n",
    "        # Obtain longitude and latitude for each timestamp\n",
    "        storm_track_longitude = TC_track_dataset['center_lon'].loc[TC_track_dataset['time'] == reanalysis_storm_timestamp]\n",
    "        storm_track_latitude = TC_track_dataset['center_lat'].loc[TC_track_dataset['time'] == reanalysis_storm_timestamp]\n",
    "        # Round coordinates to align with dataset coordinate system and resolution\n",
    "        storm_track_coordinates[reanalysis_storm_timestamp] = {'lon': interval_round(storm_track_longitude.item(), reanalysis_resolution),\n",
    "                                                               'lat': interval_round(storm_track_latitude.item(), reanalysis_resolution)}\n",
    "\n",
    "    return storm_track_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17fe31ca-cef6-4b44-8f1e-64d5b0b5c084",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reanalysis_grid_redefinition(storm_track_coordinates: dict,\n",
    "                                 reanalysis_resolution: float,\n",
    "                                 coarsen_factor: int,\n",
    "                                 storm_reanalysis_window_size: int|float):\n",
    "\n",
    "    ''' Generate a consistent grid for reanalysis data to allow for all timestamps to be interpolated to the same grid. '''\n",
    "\n",
    "    # Coarsening factor\n",
    "    interpolation_resolution = reanalysis_resolution * coarsen_factor\n",
    "    \n",
    "    # Define storm spatial extents for future interpolation\n",
    "    minimum_longitude = np.min([entry['lon'] for entry in storm_track_coordinates.values()])\n",
    "    minimum_latitude = np.min([entry['lat'] for entry in storm_track_coordinates.values()])\n",
    "    maximum_longitude = np.max([entry['lon'] for entry in storm_track_coordinates.values()])\n",
    "    maximum_latitude = np.max([entry['lat'] for entry in storm_track_coordinates.values()])\n",
    "    \n",
    "    # Define basis vectors for data interpolation\n",
    "    # Subtract and add window sizes to minima and maxima, respectively, to capture full desired extent\n",
    "    zonal_basis_vector = np.arange(minimum_longitude - storm_reanalysis_window_size, \n",
    "                                   maximum_longitude + storm_reanalysis_window_size, interpolation_resolution)\n",
    "    meridional_basis_vector = np.arange(minimum_latitude - storm_reanalysis_window_size, \n",
    "                                        maximum_latitude + storm_reanalysis_window_size, interpolation_resolution)\n",
    "\n",
    "    return zonal_basis_vector, meridional_basis_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae8965e-84a3-433e-aebe-18fca10a5297",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_reanalysis_storm_timestamp(storm_track_coordinates: dict,\n",
    "                                    storm_reanalysis_window_size: int | float,\n",
    "                                    reanalysis_resolution: float,\n",
    "                                    reanalysis_data: xr.Dataset,\n",
    "                                    zonal_basis_vector: np.array,\n",
    "                                    meridional_basis_vector: np.array,\n",
    "                                    storm_timestamp):\n",
    "\n",
    "    ''' \n",
    "    Method to link track data and reanalysis data for a single timestamp. \n",
    "    This is compartmentalized to allow for straightforward parallelization.\n",
    "    '''\n",
    "\n",
    "    # Define reanalysis dataset coordinate names\n",
    "    grid_xt = 'longitude'\n",
    "    grid_yt = 'latitude'\n",
    "    \n",
    "    # Initialize container dictionaries\n",
    "    storm_reanalysis_container = {}\n",
    "    storm_reanalysis_window_extent = {}\n",
    "    storm_reanalysis_window_extent[storm_timestamp] = {}\n",
    "    \n",
    "    # Generate trimming window extents for each timestamp.\n",
    "    # Window extents are defined as: \n",
    "    # 'grid_xt' = (longitude - window_extent, longitude + window_extent), \n",
    "    # 'grid_yt' = (latitude - window_extent, latitude + window_extent)\n",
    "    \n",
    "    # Assign zonal window\n",
    "    storm_reanalysis_window_extent[storm_timestamp][grid_xt] = np.arange(storm_track_coordinates[storm_timestamp]['lon'] - storm_reanalysis_window_size,\n",
    "                                                                         storm_track_coordinates[storm_timestamp]['lon'] + storm_reanalysis_window_size,\n",
    "                                                                         reanalysis_resolution)\n",
    "    # Assign meridional window\n",
    "    storm_reanalysis_window_extent[storm_timestamp][grid_yt] = np.arange(storm_track_coordinates[storm_timestamp]['lat'] - storm_reanalysis_window_size,\n",
    "                                                                         storm_track_coordinates[storm_timestamp]['lat'] + storm_reanalysis_window_size,\n",
    "                                                                         reanalysis_resolution)\n",
    "    # Extract GCM data for the given timestamp and spatial extent\n",
    "    storm_reanalysis_container[storm_timestamp] = reanalysis_data.sel(time=storm_timestamp)\n",
    "    storm_reanalysis_container[storm_timestamp] = storm_reanalysis_container[storm_timestamp].sel({grid_xt: storm_reanalysis_window_extent[storm_timestamp][grid_xt]})\n",
    "    storm_reanalysis_container[storm_timestamp] = storm_reanalysis_container[storm_timestamp].sel({grid_yt: storm_reanalysis_window_extent[storm_timestamp][grid_yt]})\n",
    "\n",
    "    # Interpolate to different resolution (shoot for 0.5 degrees)\n",
    "    storm_reanalysis_container[storm_timestamp] = storm_reanalysis_container[storm_timestamp].interp(longitude=zonal_basis_vector).interp(latitude=meridional_basis_vector)\n",
    "    \n",
    "    return storm_reanalysis_container[storm_timestamp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "319f50af-12f1-4bcc-8921-927ac63a2bdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_reanalysis_storm(storm_track_coordinates: dict,\n",
    "                          storm_timestamps: list,\n",
    "                          reanalysis_data: xr.Dataset,\n",
    "                          reanalysis_resolution: float,\n",
    "                          target_resolution: float,\n",
    "                          storm_reanalysis_window_size: int|float,\n",
    "                          parallel: bool,\n",
    "                          diagnostic: bool=False):\n",
    "\n",
    "    ''' Method to load reanalysis data for a given storm given its track coordinates, track timestamps, and reanalysis data. '''\n",
    "    \n",
    "    # Get basis vectors for reanalysis data generation\n",
    "    coarsen_factor = int(np.round(target_resolution / reanalysis_resolution)) # factor by which reanalysis data will be coarsened to match a target resolution\n",
    "    zonal_basis_vector, meridional_basis_vector = reanalysis_grid_redefinition(storm_track_coordinates,\n",
    "                                                                               reanalysis_resolution,\n",
    "                                                                               coarsen_factor,\n",
    "                                                                               storm_reanalysis_window_size)\n",
    "\n",
    "    # Initialize a container to hold GCM output connected to each storm timestamp and the corresponding spatial extent\n",
    "    storm_reanalysis_container = {}\n",
    "    # Define partial function to streamline function calls, since the only variable argument is `storm_timestamps`\n",
    "    partial_load_timestamp_reanalysis_entry = functools.partial(load_reanalysis_storm_timestamp,\n",
    "                                                                 storm_track_coordinates,\n",
    "                                                                 storm_reanalysis_window_size,\n",
    "                                                                 reanalysis_resolution,\n",
    "                                                                 reanalysis_data,\n",
    "                                                                 zonal_basis_vector,\n",
    "                                                                 meridional_basis_vector)\n",
    "    # Keep time for profiling\n",
    "    start_time = time.time()\n",
    "    # Parallel implementation\n",
    "    if parallel:\n",
    "        # Distribute data loading in parallel over each timestamp\n",
    "        with Pool() as pool:\n",
    "            storm_reanalysis_timestamp_entry = pool.map(partial_load_timestamp_reanalysis_entry, storm_timestamps)\n",
    "            storm_reanalysis_data = xr.concat(storm_reanalysis_timestamp_entry, dim='time').sortby('time')\n",
    "            pool.close()\n",
    "    # Serial implementation\n",
    "    else:\n",
    "        # Initialize container dictionary\n",
    "        storm_reanalysis_container = {}\n",
    "        # Iterate over all timestamps to find reanalysis data for the given entry\n",
    "        for storm_timestamp in storm_timestamps:\n",
    "            storm_reanalysis_container[storm_timestamp] = partial_load_timestamp_reanalysis_entry(storm_timestamp)\n",
    "        # Concatenate all GCM output data corresponding to storm into a single xArray Dataset\n",
    "        storm_reanalysis_data = xr.concat(storm_reanalysis_container.values(), dim='time').sortby('time')\n",
    "\n",
    "    if diagnostic:\n",
    "        print(f'Elapsed time to load reanalysis storm: {(time.time() - start_time):.2f} s.')\n",
    "        print(f'\\t per timestamp: {((time.time() - start_time) / len(storm_timestamps)):.2f} s.')\n",
    "\n",
    "    return storm_reanalysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bcc8c1e-304b-47ef-8bd9-d889213f13ef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reanalysis_GFDL_compatibility_adjustments(storm_reanalysis_dataset: xr.Dataset) -> xr.Dataset:\n",
    "\n",
    "    ''' Perform adjustments so reanalysis data can use the same conventions as GFDL output data. '''\n",
    "\n",
    "    # Rename spatial basis vector coordinate names\n",
    "    storm_reanalysis_dataset = storm_reanalysis_dataset.rename({'longitude': 'grid_xt', 'latitude': 'grid_yt'})\n",
    "\n",
    "    # Perform deep copy for coordinate value modification\n",
    "    storm_reanalysis_dataset_reformatted = storm_reanalysis_dataset.copy(deep=True)\n",
    "\n",
    "    # Adjust timestamp format to cftime on the xArray Dataset.\n",
    "    pd_timestamps = pd.to_datetime(storm_reanalysis_dataset.time) # convert to Pandas objects for easier indexing\n",
    "    cftime_timestamps = [cftime.datetime(year=timestamp.year, \n",
    "                                         month=timestamp.month,\n",
    "                                         day=timestamp.day,\n",
    "                                         hour=timestamp.hour,\n",
    "                                         calendar='julian') for timestamp in pd_timestamps]\n",
    "    storm_reanalysis_dataset_reformatted['time'] = cftime_timestamps\n",
    "    assert 'cftime' in str(type(storm_reanalysis_dataset_reformatted['time'].values[0])), f'[reanalysis_GFDL_compatibility_adjustments()] Timestamp is not a cftime object.' \n",
    "\n",
    "    return storm_reanalysis_dataset_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "712dad54-30a7-4b79-9e4b-1b1d994b5576",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reanalysis_storm_generator(reanalysis_track_dataset: pd.DataFrame,\n",
    "                               reanalysis_dataset: xr.Dataset,\n",
    "                               reanalysis_resolution: float,\n",
    "                               target_resolution: float,\n",
    "                               storm_reanalysis_window_size: int,\n",
    "                               parallel: bool=True,\n",
    "                               storm_ID: str|None=None):\n",
    "\n",
    "    ''' Method to perform all steps related to binding corresponding GFDL QuickTracks and GCM model output together for a given TC. '''\n",
    "\n",
    "    print(f'[storm_generator] Processing storm ID {storm_ID}...')\n",
    "\n",
    "    # 4. Find a candidate storm from the track data, ensure it is ordered by time\n",
    "    storm_track_dataset = TC_tracker.pick_storm(reanalysis_track_dataset, selection_method='storm_number', storm_ID=storm_ID).sort_values('time')\n",
    "    # 5. Pull storm-specific timestamps\n",
    "    storm_track_timestamps = get_reanalysis_timestamps(storm_track_dataset, reanalysis_dataset)\n",
    "    # 6. Pull storm-specific coordinates to align with track timestamps\n",
    "    storm_track_coordinates = get_storm_coordinates(storm_track_dataset, reanalysis_dataset, storm_track_timestamps, reanalysis_resolution)\n",
    "    # 7. Load reanalysis data for the iterand storm to align with track data\n",
    "    storm_reanalysis_dataset = load_reanalysis_storm(storm_track_coordinates, \n",
    "                                                     storm_track_timestamps,\n",
    "                                                     reanalysis_dataset,\n",
    "                                                     reanalysis_resolution,\n",
    "                                                     target_resolution,\n",
    "                                                     storm_reanalysis_window_size,\n",
    "                                                     parallel=parallel)\n",
    "    # 8. Append information from track data to object containing reanalysis output.\n",
    "    storm_reanalysis_dataset = TC_tracker.join_track_GCM_data(storm_track_data=storm_track_dataset,\n",
    "                                                              storm_gcm_data=storm_reanalysis_dataset,\n",
    "                                                              storm_time_variable='time')\n",
    "    # 9. Derive fields present in GCM output data but not directly provided in ERA5 data\n",
    "    storm_reanalysis_dataset = derived_fields(storm_reanalysis_dataset)\n",
    "    # 10. Perform adjustments for compatibility with GFDL GCM outputs\n",
    "    storm_reanalysis_dataset = reanalysis_GFDL_compatibility_adjustments(storm_reanalysis_dataset)\n",
    "    # 11. Save xArray Dataset to netCDF file\n",
    "    TC_tracker.save_storm_netcdf(storm_reanalysis_dataset, model_name='ERA5', experiment_name='reanalysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85aa8cef-3d2c-415b-98b6-6240ddca3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(date_range: tuple[str, str],\n",
    "         basin_name: str='global',\n",
    "         intensity_parameter: str='min_slp',\n",
    "         intensity_range: tuple[int | float, int | float]=(980, 1000),\n",
    "         number_of_storms: int=1,\n",
    "         reanalysis_resolution: float=0.25,\n",
    "         target_resolution: float=0.5,\n",
    "         storm_reanalysis_window_size: int|float=12,\n",
    "         parallel: bool=True):\n",
    "\n",
    "    # 1. Pull track data for a given date range, basin, and intensity range\n",
    "    reanalysis_track_dataset = access_IBTrACS_tracks(date_range, basin_name, intensity_parameter, intensity_range)\n",
    "    # 2. Access reanalysis dataset. Ensure this is done lazily to avoid excessive memory usage.\n",
    "    reanalysis_dataset = load_reanalysis_data()\n",
    "    # 2a. Perform dataset field correction\n",
    "    reanalysis_dataset = field_correction(reanalysis_dataset)\n",
    "    # 3. Obtain N randomized storm IDs from the filtered track data, where 'N' is `number_of_storms`\n",
    "    storm_IDs = TC_tracker.pick_storm_IDs(reanalysis_track_dataset, number_of_storms)\n",
    "    \n",
    "    # Define partial function to allow for using Pool.map since `track_data` is equivalent for all subprocesses\n",
    "    preloaded_reanalysis_storm_generator = functools.partial(reanalysis_storm_generator, \n",
    "                                                             reanalysis_track_dataset, \n",
    "                                                             reanalysis_dataset,\n",
    "                                                             reanalysis_resolution,\n",
    "                                                             target_resolution,\n",
    "                                                             storm_reanalysis_window_size,\n",
    "                                                             parallel)\n",
    "    \n",
    "    for storm_ID in storm_IDs:\n",
    "        preloaded_reanalysis_storm_generator(storm_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e39dfa-f5c5-46ba-9213-5905daf3a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[storm_generator] Processing storm ID 2010-040S11185...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010-040S11185.max_wind-44.min_slp-945.basin-SP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010-040S11185.max_wind-44.min_slp-945.basin-SP.nc: 17.96 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010-040S11185.max_wind-44.min_slp-945.basin-SP.nc: 49.33 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010-240N15142...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010-240N15142.max_wind-41.min_slp-960.basin-WP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010-240N15142.max_wind-41.min_slp-960.basin-WP.nc: 9.95 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010-240N15142.max_wind-41.min_slp-960.basin-WP.nc: 13.98 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010-151N14065...\n"
     ]
    }
   ],
   "source": [
    "date_range = ('2010-01-01', '2011-01-01')\n",
    "basin_name = 'global'\n",
    "intensity_parameter = 'min_slp'\n",
    "intensity_range = (0, 970)\n",
    "\n",
    "number_of_storms = -1\n",
    "\n",
    "main(basin_name, date_range, intensity_parameter, intensity_range, number_of_storms=number_of_storms)\n",
    "\n",
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ce5275-f7fc-4d8c-936b-ef54237c59d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
