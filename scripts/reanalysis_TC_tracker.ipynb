{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7512a8be-c948-4484-a091-f746bb61f2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy, numba\n",
    "# Local data storage packages\n",
    "import functools, os, pickle, collections, sys, importlib\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr, nc_time_axis\n",
    "xr.set_options(keep_attrs=True)\n",
    "# Visualization tools\n",
    "import cartopy, cartopy.crs as ccrs, matplotlib, matplotlib.pyplot as plt\n",
    "# Local imports\n",
    "import accessor, composite, composite_snapshots, derived, ibtracs, utilities, socket, visualization, tc_analysis, tc_processing, track_TCs, TC_tracker\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "importlib.reload(TC_tracker);\n",
    "importlib.reload(track_TCs);\n",
    "importlib.reload(ibtracs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49f95d-0468-4ecf-9590-88d0c88a6176",
   "metadata": {},
   "source": [
    "#### Load and format reanalysis data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fb9a9d8-9cf0-4947-95f3-ea31c03bf30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_IBTrACS_tracks(basin_name: str,\n",
    "                          date_range: tuple[str, str],\n",
    "                          intensity_parameter: str,\n",
    "                          intensity_range: tuple[int, int]) -> pd.DataFrame:\n",
    "    \n",
    "    track_data = ibtracs.main(basin_name=basin_name,\n",
    "                              intensity_parameter=intensity_parameter,\n",
    "                              intensity_range=intensity_range)\n",
    "    \n",
    "    return track_data.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92113dee-9c99-4c08-b37f-77c2a00f57cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reanalysis_data():\n",
    "\n",
    "    # Load reanalysis data\n",
    "    # Assumes that all files in `reanalysis_dirname` have congruent coordinates, like with ERA5\n",
    "    reanalysis_dirname = '/scratch/gpfs/GEOCLIM/gr7610/tiger3/reference/datasets/ERA5'\n",
    "    reanalysis_filenames = [filename for filename in os.listdir(reanalysis_dirname) if\n",
    "                            filename.endswith('nc')]\n",
    "    reanalysis_pathnames = [os.path.join(reanalysis_dirname, reanalysis_filename) for reanalysis_filename in reanalysis_filenames]\n",
    "    reanalysis_data = xr.open_mfdataset(reanalysis_pathnames)\n",
    "    \n",
    "    # Rename coordinates and data variables to adjust to GFDL QuickTracks outputs\n",
    "    # This naming convention follows ERA5 outputs\n",
    "    reanalysis_data = reanalysis_data.rename({'valid_time': 'time',\n",
    "                                          'u10': 'u_ref',\n",
    "                                          'v10': 'v_ref',\n",
    "                                          't2m': 't_ref',\n",
    "                                          'sst': 't_surf',\n",
    "                                          'sp': 'slp',\n",
    "                                          'tp': 'precip',\n",
    "                                          'tcwv': 'WVP'})\n",
    "\n",
    "    return reanalysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a49d90c-327d-4f93-a409-1ab6bcc33296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def field_correction(reanalysis_dataset: xr.Dataset) -> xr.Dataset:\n",
    "\n",
    "    ''' \n",
    "    Modify data to adjust for fields that do not readily provide desired units. \n",
    "    This is primarily performed to \n",
    "    '''\n",
    "\n",
    "    # Define fields termed 'accumulated' from ERA5 - this represents parameters integrated hourly\n",
    "    accumulated_fields = {'slhf': 'lhflx',\n",
    "                          'sshf': 'shflx',\n",
    "                          'ssr': 'swnet_sfc',\n",
    "                          'ssrd': 'swdn_sfc',\n",
    "                          'str': 'lwnet_sfc',\n",
    "                          'strd': 'lwdn_sfc',\n",
    "                          'tisr': 'swdn_toa',\n",
    "                          'tsr': 'swnet_toa',\n",
    "                          'ttr': 'olr'}\n",
    "    accumulated_factor = 1 / 3600 # converts from J m^-2 to W m^-2\n",
    "\n",
    "    # Iterate through all fields and perform correction\n",
    "    for accumulated_field_name, accumulated_field_rename in accumulated_fields.items():\n",
    "        reanalysis_dataset[accumulated_field_name] = reanalysis_dataset[accumulated_field_name] * accumulated_factor\n",
    "        reanalysis_dataset = reanalysis_dataset.rename({accumulated_field_name: accumulated_field_rename})\n",
    "\n",
    "    return reanalysis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3b9ec8c-a114-44c2-b540-958d021211da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_reanalysis_timestamps(TC_track_dataset: pd.DataFrame,\n",
    "                              reanalysis_data: xr.Dataset) -> list:\n",
    "    \n",
    "    # Select random timestamps from each dataset to ensure they are the same\n",
    "    # Assume all timestamps within a dataset have the same timestamp type\n",
    "    random_storm_timestamp = random.choice(TC_track_dataset.time.values)\n",
    "    random_reanalysis_timestamp = random.choice(reanalysis_data.time.values)\n",
    "\n",
    "    # Ensure timestamps are identically-typed\n",
    "    check_timestamp_formats = type(random_storm_timestamp) == type(random_reanalysis_timestamp)\n",
    "    assert check_timestamp_formats, 'Timestamp types between IBTrACS and reanalysis require alignment.'\n",
    "    \n",
    "    # Iterate through storm timestamps to make sure they are in the reanalysis data\n",
    "    reanalysis_storm_timestamps = [storm_timestamp for storm_timestamp in TC_track_dataset.time.values if \n",
    "                                   storm_timestamp in reanalysis_data.time.values]\n",
    "\n",
    "    return reanalysis_storm_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f4f1a22-b45e-4225-a67b-5c881907a7be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_coordinates(TC_track_dataset: pd.DataFrame,\n",
    "                          reanalysis_data: xr.Dataset,\n",
    "                          reanalysis_storm_timestamps: list,\n",
    "                          reanalysis_resolution: float=0.25) -> dict:\n",
    "\n",
    "    interval_round = lambda x, y: y * round(x / y) # round coordinates to nearest dataset coordinates\n",
    "    \n",
    "    # Initialize dictionary for storm track coordinates\n",
    "    storm_track_coordinates = {}\n",
    "    # Construct dictionary for coordinates pertaining to each storm timestamp\n",
    "    for reanalysis_storm_timestamp in reanalysis_storm_timestamps:\n",
    "        # Obtain longitude and latitude for each timestamp\n",
    "        storm_track_longitude = TC_track_dataset['center_lon'].loc[TC_track_dataset['time'] == reanalysis_storm_timestamp]\n",
    "        storm_track_latitude = TC_track_dataset['center_lat'].loc[TC_track_dataset['time'] == reanalysis_storm_timestamp]\n",
    "        # Round coordinates to align with dataset coordinate system and resolution\n",
    "        storm_track_coordinates[reanalysis_storm_timestamp] = {'lon': interval_round(storm_track_longitude.item(), reanalysis_resolution),\n",
    "                                                               'lat': interval_round(storm_track_latitude.item(), reanalysis_resolution)}\n",
    "\n",
    "    return storm_track_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17fe31ca-cef6-4b44-8f1e-64d5b0b5c084",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def reanalysis_grid_redefinition(storm_track_coordinates: dict,\n",
    "                                 reanalysis_resolution: float,\n",
    "                                 coarsen_factor: int=2,\n",
    "                                 storm_reanalysis_window_size: int|float=12):\n",
    "\n",
    "    ''' Generate a consistent grid for reanalysis data to allow for all timestamps to be interpolated to the same grid. '''\n",
    "\n",
    "    # Coarsening factor\n",
    "    interpolation_resolution = reanalysis_resolution * coarsen_factor\n",
    "    \n",
    "    # Define storm spatial extents for future interpolation\n",
    "    minimum_longitude = np.min([entry['lon'] for entry in storm_track_coordinates.values()])\n",
    "    minimum_latitude = np.min([entry['lat'] for entry in storm_track_coordinates.values()])\n",
    "    maximum_longitude = np.max([entry['lon'] for entry in storm_track_coordinates.values()])\n",
    "    maximum_latitude = np.max([entry['lat'] for entry in storm_track_coordinates.values()])\n",
    "    \n",
    "    # Define basis vectors for data interpolation\n",
    "    # Subtract and add window sizes to minima and maxima, respectively, to capture full desired extent\n",
    "    zonal_basis_vector = np.arange(minimum_longitude - storm_reanalysis_window_size, \n",
    "                                   maximum_longitude + storm_reanalysis_window_size, interpolation_resolution)\n",
    "    meridional_basis_vector = np.arange(minimum_latitude - storm_reanalysis_window_size, \n",
    "                                        maximum_latitude + storm_reanalysis_window_size, interpolation_resolution)\n",
    "\n",
    "    return zonal_basis_vector, meridional_basis_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dae8965e-84a3-433e-aebe-18fca10a5297",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_reanalysis_storm_timestamp(storm_track_coordinates: dict,\n",
    "                                    storm_reanalysis_window_size: int | float,\n",
    "                                    reanalysis_resolution: float,\n",
    "                                    reanalysis_data: xr.Dataset,\n",
    "                                    zonal_basis_vector: np.array,\n",
    "                                    meridional_basis_vector: np.array,\n",
    "                                    storm_timestamp):\n",
    "\n",
    "    ''' \n",
    "    Method to link track data and reanalysis data for a single timestamp. \n",
    "    This is compartmentalized to allow for straightforward parallelization.\n",
    "    '''\n",
    "\n",
    "    # Define reanalysis dataset coordinate names\n",
    "    grid_xt = 'longitude'\n",
    "    grid_yt = 'latitude'\n",
    "    \n",
    "    # Initialize container dictionaries\n",
    "    storm_reanalysis_container = {}\n",
    "    storm_reanalysis_window_extent = {}\n",
    "    storm_reanalysis_window_extent[storm_timestamp] = {}\n",
    "    \n",
    "    # Generate trimming window extents for each timestamp.\n",
    "    # Window extents are defined as: \n",
    "    # 'grid_xt' = (longitude - window_extent, longitude + window_extent), \n",
    "    # 'grid_yt' = (latitude - window_extent, latitude + window_extent)\n",
    "    \n",
    "    # Assign zonal window\n",
    "    storm_reanalysis_window_extent[storm_timestamp][grid_xt] = np.arange(storm_track_coordinates[storm_timestamp]['lon'] - storm_reanalysis_window_size,\n",
    "                                                                         storm_track_coordinates[storm_timestamp]['lon'] + storm_reanalysis_window_size,\n",
    "                                                                         reanalysis_resolution)\n",
    "    # Assign meridional window\n",
    "    storm_reanalysis_window_extent[storm_timestamp][grid_yt] = np.arange(storm_track_coordinates[storm_timestamp]['lat'] - storm_reanalysis_window_size,\n",
    "                                                                         storm_track_coordinates[storm_timestamp]['lat'] + storm_reanalysis_window_size,\n",
    "                                                                         reanalysis_resolution)\n",
    "    # Extract GCM data for the given timestamp and spatial extent\n",
    "    storm_reanalysis_container[storm_timestamp] = reanalysis_data.sel(time=storm_timestamp)\n",
    "    storm_reanalysis_container[storm_timestamp] = storm_reanalysis_container[storm_timestamp].sel({grid_xt: storm_reanalysis_window_extent[storm_timestamp][grid_xt]})\n",
    "    storm_reanalysis_container[storm_timestamp] = storm_reanalysis_container[storm_timestamp].sel({grid_yt: storm_reanalysis_window_extent[storm_timestamp][grid_yt]})\n",
    "\n",
    "    # Interpolate to different resolution (shoot for 0.5 degrees)\n",
    "    storm_reanalysis_container[storm_timestamp] = storm_reanalysis_container[storm_timestamp].interp(longitude=zonal_basis_vector).interp(latitude=meridional_basis_vector)\n",
    "    \n",
    "    return storm_reanalysis_container[storm_timestamp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "319f50af-12f1-4bcc-8921-927ac63a2bdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_reanalysis_storm(storm_track_coordinates: dict,\n",
    "                          storm_timestamps: list,\n",
    "                          reanalysis_data: xr.Dataset,\n",
    "                          reanalysis_resolution: float,\n",
    "                          target_resolution: float,\n",
    "                          storm_reanalysis_window_size: int|float,\n",
    "                          parallel: bool,\n",
    "                          diagnostic: bool=False):\n",
    "\n",
    "    ''' Method to load reanalysis data for a given storm given its track coordinates, track timestamps, and reanalysis data. '''\n",
    "    \n",
    "    # Get basis vectors for reanalysis data generation\n",
    "    coarsen_factor = int(np.round(target_resolution / reanalysis_resolution)) # factor by which reanalysis data will be coarsened to match a target resolution\n",
    "    zonal_basis_vector, meridional_basis_vector = reanalysis_grid_redefinition(storm_track_coordinates,\n",
    "                                                                               reanalysis_resolution,\n",
    "                                                                               coarsen_factor,\n",
    "                                                                               storm_reanalysis_window_size)\n",
    "\n",
    "    # Initialize a container to hold GCM output connected to each storm timestamp and the corresponding spatial extent\n",
    "    storm_reanalysis_container = {}\n",
    "    # Define partial function to streamline function calls, since the only variable argument is `storm_timestamps`\n",
    "    partial_load_timestamp_reanalysis_entry = functools.partial(load_reanalysis_storm_timestamp,\n",
    "                                                                 storm_track_coordinates,\n",
    "                                                                 storm_reanalysis_window_size,\n",
    "                                                                 reanalysis_resolution,\n",
    "                                                                 reanalysis_data,\n",
    "                                                                 zonal_basis_vector,\n",
    "                                                                 meridional_basis_vector)\n",
    "    # Keep time for profiling\n",
    "    start_time = time.time()\n",
    "    # Parallel implementation\n",
    "    if parallel:\n",
    "        # Distribute data loading in parallel over each timestamp\n",
    "        with Pool() as pool:\n",
    "            storm_reanalysis_timestamp_entry = pool.map(partial_load_timestamp_reanalysis_entry, storm_timestamps)\n",
    "            storm_reanalysis_data = xr.concat(storm_reanalysis_timestamp_entry, dim='time').sortby('time')\n",
    "            pool.close()\n",
    "    # Serial implementation\n",
    "    else:\n",
    "        # Initialize container dictionary\n",
    "        storm_reanalysis_container = {}\n",
    "        # Iterate over all timestamps to find reanalysis data for the given entry\n",
    "        for storm_timestamp in storm_timestamps:\n",
    "            storm_reanalysis_container[storm_timestamp] = partial_load_timestamp_reanalysis_entry(storm_timestamp)\n",
    "        # Concatenate all GCM output data corresponding to storm into a single xArray Dataset\n",
    "        storm_reanalysis_data = xr.concat(storm_reanalysis_container.values(), dim='time').sortby('time')\n",
    "\n",
    "    if diagnostic:\n",
    "        print(f'Elapsed time to load reanalysis storm: {(time.time() - start_time):.2f} s.')\n",
    "        print(f'\\t per timestamp: {((time.time() - start_time) / len(storm_timestamps)):.2f} s.')\n",
    "\n",
    "    return storm_reanalysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "712dad54-30a7-4b79-9e4b-1b1d994b5576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reanalysis_storm_generator(reanalysis_track_dataset: pd.DataFrame,\n",
    "                               reanalysis_dataset: xr.Dataset,\n",
    "                               reanalysis_resolution: float,\n",
    "                               target_resolution: float,\n",
    "                               storm_reanalysis_window_size: int,\n",
    "                               parallel: bool=True,\n",
    "                               storm_ID: str|None=None):\n",
    "\n",
    "    ''' Method to perform all steps related to binding corresponding GFDL QuickTracks and GCM model output together for a given TC. '''\n",
    "\n",
    "    print(f'[storm_generator] Processing storm ID {storm_ID}...')\n",
    "\n",
    "    # 4. Find a candidate storm from the track data, ensure it is ordered by time\n",
    "    storm_track_dataset = TC_tracker.pick_storm(reanalysis_track_dataset, selection_method='storm_number', storm_ID=storm_ID).sort_values('time')\n",
    "    # 5. Pull storm-specific timestamps\n",
    "    storm_track_timestamps = get_reanalysis_timestamps(storm_track_dataset, reanalysis_dataset)\n",
    "    # 6. Pull storm-specific coordinates to align with track timestamps\n",
    "    storm_track_coordinates = get_storm_coordinates(storm_track_dataset, reanalysis_dataset, storm_track_timestamps, reanalysis_resolution)\n",
    "    # 7. Load reanalysis data for the iterand storm to align with track data\n",
    "    storm_reanalysis_dataset = load_reanalysis_storm(storm_track_coordinates, \n",
    "                                                     storm_track_timestamps,\n",
    "                                                     reanalysis_dataset,\n",
    "                                                     reanalysis_resolution,\n",
    "                                                     target_resolution,\n",
    "                                                     storm_reanalysis_window_size,\n",
    "                                                     parallel=parallel)\n",
    "    # 8. Append information from track data to object containing reanalysis output.\n",
    "    storm_reanalysis_dataset = TC_tracker.join_track_GCM_data(storm_track_data=storm_track_dataset,\n",
    "                                                              storm_gcm_data=storm_reanalysis_dataset,\n",
    "                                                              storm_time_variable='time')\n",
    "    # 9. Save xArray Dataset to netCDF file\n",
    "    TC_tracker.save_storm_netcdf(storm_reanalysis_dataset, model_name='ERA5', experiment_name='reanalysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "85aa8cef-3d2c-415b-98b6-6240ddca3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(date_range: tuple[str, str],\n",
    "         basin_name: str='global',\n",
    "         intensity_parameter: str='min_slp',\n",
    "         intensity_range: tuple[int | float, int | float]=(980, 1000),\n",
    "         number_of_storms: int=1,\n",
    "         reanalysis_resolution: float=0.25,\n",
    "         target_resolution: float=0.5,\n",
    "         storm_reanalysis_window_size: int|float=12,\n",
    "         parallel: bool=True):\n",
    "\n",
    "    # 1. Pull track data for a given date range, basin, and intensity range\n",
    "    reanalysis_track_dataset = access_IBTrACS_tracks(date_range, basin_name, intensity_parameter, intensity_range)\n",
    "    # 2. Access reanalysis dataset. Ensure this is done lazily to avoid excessive memory usage.\n",
    "    reanalysis_dataset = load_reanalysis_data()\n",
    "    # 2a. Perform dataset field correction\n",
    "    reanalysis_dataset = field_correction(reanalysis_dataset)\n",
    "    # 3. Obtain N randomized storm IDs from the filtered track data, where 'N' is `number_of_storms`\n",
    "    storm_IDs = TC_tracker.pick_storm_IDs(reanalysis_track_dataset, number_of_storms)\n",
    "    \n",
    "    ''' Offload TC-specific data generation onto parallel processes. '''\n",
    "    # Maximum number of processors for computation\n",
    "    max_number_procs = 10\n",
    "    # Specify number of processors to use\n",
    "    number_procs = len(storm_IDs) if len(storm_IDs) < max_number_procs else max_number_procs\n",
    "    # Define partial function to allow for using Pool.map since `track_data` is equivalent for all subprocesses\n",
    "    preloaded_reanalysis_storm_generator = functools.partial(reanalysis_storm_generator, \n",
    "                                                             reanalysis_track_dataset, \n",
    "                                                             reanalysis_dataset,\n",
    "                                                             reanalysis_resolution,\n",
    "                                                             target_resolution,\n",
    "                                                             storm_reanalysis_window_size,\n",
    "                                                             parallel)\n",
    "    \n",
    "    for storm_ID in storm_IDs:\n",
    "        preloaded_reanalysis_storm_generator(storm_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24e39dfa-f5c5-46ba-9213-5905daf3a738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[storm_generator] Processing storm ID 2010236N12341...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010236N12341.max_wind-64.min_slp-927.basin-NA.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010236N12341.max_wind-64.min_slp-927.basin-NA.nc: 8.65 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010236N12341.max_wind-64.min_slp-927.basin-NA.nc: 146.54 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010084S09138...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010084S09138.max_wind-36.min_slp-971.basin-SP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010084S09138.max_wind-36.min_slp-971.basin-SP.nc: 17.19 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010084S09138.max_wind-36.min_slp-971.basin-SP.nc: 16.93 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010240N15142...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010240N15142.max_wind-41.min_slp-960.basin-WP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010240N15142.max_wind-41.min_slp-960.basin-WP.nc: 9.25 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010240N15142.max_wind-41.min_slp-960.basin-WP.nc: 11.18 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010191N12138...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010191N12138.max_wind-36.min_slp-970.basin-WP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010191N12138.max_wind-36.min_slp-970.basin-WP.nc: 7.00 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010191N12138.max_wind-36.min_slp-970.basin-WP.nc: 18.48 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010070S15168...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010070S15168.max_wind-59.min_slp-915.basin-SP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010070S15168.max_wind-59.min_slp-915.basin-SP.nc: 4.27 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010070S15168.max_wind-59.min_slp-915.basin-SP.nc: 28.51 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010279N22293...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010279N22293.max_wind-39.min_slp-976.basin-NA.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010279N22293.max_wind-39.min_slp-976.basin-NA.nc: 11.94 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010279N22293.max_wind-39.min_slp-976.basin-NA.nc: 85.41 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010263N15328...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010263N15328.max_wind-39.min_slp-982.basin-NA.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010263N15328.max_wind-39.min_slp-982.basin-NA.nc: 3.98 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010263N15328.max_wind-39.min_slp-982.basin-NA.nc: 21.06 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010285N13145...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010285N13145.max_wind-64.min_slp-885.basin-WP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010285N13145.max_wind-64.min_slp-885.basin-WP.nc: 8.56 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010285N13145.max_wind-64.min_slp-885.basin-WP.nc: 36.78 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010169N13266...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010169N13266.max_wind-64.min_slp-938.basin-EP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010169N13266.max_wind-64.min_slp-938.basin-EP.nc: 11.95 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010169N13266.max_wind-64.min_slp-938.basin-EP.nc: 38.13 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010257N16282...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010257N16282.max_wind-57.min_slp-956.basin-NA.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010257N16282.max_wind-57.min_slp-956.basin-NA.nc: 5.76 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010257N16282.max_wind-57.min_slp-956.basin-NA.nc: 11.50 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010255N13340...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010255N13340.max_wind-62.min_slp-948.basin-NA.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010255N13340.max_wind-62.min_slp-948.basin-NA.nc: 8.61 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010255N13340.max_wind-62.min_slp-948.basin-NA.nc: 66.67 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010284N14278...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010284N14278.max_wind-46.min_slp-981.basin-NA.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010284N14278.max_wind-46.min_slp-981.basin-NA.nc: 5.44 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010284N14278.max_wind-46.min_slp-981.basin-NA.nc: 10.48 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010172N08272...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010172N08272.max_wind-54.min_slp-960.basin-EP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010172N08272.max_wind-54.min_slp-960.basin-EP.nc: 8.43 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010172N08272.max_wind-54.min_slp-960.basin-EP.nc: 23.90 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010019S11123...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010019S11123.max_wind-33.min_slp-978.basin-SP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010019S11123.max_wind-33.min_slp-978.basin-SP.nc: 9.01 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010019S11123.max_wind-33.min_slp-978.basin-SP.nc: 10.75 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010040S11185...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010040S11185.max_wind-44.min_slp-945.basin-SP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010040S11185.max_wind-44.min_slp-945.basin-SP.nc: 15.02 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010040S11185.max_wind-44.min_slp-945.basin-SP.nc: 39.46 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010293N17277...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010293N17277.max_wind-44.min_slp-977.basin-NA.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010293N17277.max_wind-44.min_slp-977.basin-NA.nc: 5.28 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010293N17277.max_wind-44.min_slp-977.basin-NA.nc: 14.91 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010038S08194...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010038S08194.max_wind-39.min_slp-960.basin-SP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010038S08194.max_wind-39.min_slp-960.basin-SP.nc: 3.19 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010038S08194.max_wind-39.min_slp-960.basin-SP.nc: 11.01 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010263N15149...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010263N15149.max_wind-44.min_slp-945.basin-WP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010263N15149.max_wind-44.min_slp-945.basin-WP.nc: 9.39 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010263N15149.max_wind-44.min_slp-945.basin-WP.nc: 8.78 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010151N14065...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010151N14065.max_wind-44.min_slp-964.basin-NI.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010151N14065.max_wind-44.min_slp-964.basin-NI.nc: 4.75 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010151N14065.max_wind-44.min_slp-964.basin-NI.nc: 20.60 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 2010029S12177...\n",
      "[save_storm_netcdf] Loading data for TC.model-ERA5.experiment-reanalysis.storm_ID-2010029S12177.max_wind-51.min_slp-925.basin-SP.nc\n",
      "Elapsed loading time for TC.model-ERA5.experiment-reanalysis.storm_ID-2010029S12177.max_wind-51.min_slp-925.basin-SP.nc: 11.93 s\n",
      "File size for TC.model-ERA5.experiment-reanalysis.storm_ID-2010029S12177.max_wind-51.min_slp-925.basin-SP.nc: 98.59 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_range = ('2010-01-01', '2011-01-01')\n",
    "basin_name = 'global'\n",
    "intensity_parameter = 'max_wind'\n",
    "intensity_range = (30, np.inf)\n",
    "\n",
    "number_of_storms = 20\n",
    "\n",
    "main(basin_name, date_range, intensity_parameter, intensity_range, number_of_storms=number_of_storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d39e6-3fea-460d-bb53-ecca9c939266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
