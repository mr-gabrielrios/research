{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f7ae02-d28c-43f2-9e4a-872fc6a1f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import packages. '''\n",
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy, numba\n",
    "# Local data storage packages\n",
    "import functools, os, pickle, collections, sys\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr, nc_time_axis\n",
    "xr.set_options(keep_attrs=True)\n",
    "# Visualization tools\n",
    "import cartopy, cartopy.crs as ccrs, matplotlib, matplotlib.pyplot as plt\n",
    "# Local imports\n",
    "import accessor, composite, composite_snapshots, derived, utilities, socket, visualization, tc_analysis, tc_processing, track_TCs\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import importlib\n",
    "importlib.reload(composite);\n",
    "importlib.reload(xr);\n",
    "importlib.reload(composite_snapshots);\n",
    "importlib.reload(utilities);\n",
    "importlib.reload(tc_analysis);\n",
    "importlib.reload(tc_processing);\n",
    "importlib.reload(visualization);\n",
    "importlib.reload(derived);\n",
    "importlib.reload(track_TCs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416f556-3712-4a00-91b9-06069b99c4e4",
   "metadata": {},
   "source": [
    "#### Storm tracking methodology\n",
    "\n",
    "__Storm datasets__: \n",
    "- `GCM output`: output from global climate model\n",
    "- `track data`: output from GFDL QuickTracks model that is run on `GCM output`\n",
    "\n",
    "__For each storm__:\n",
    "1. Find a candidate storm from `track data`\n",
    "2. Get candidate storm timestamps\n",
    "3. Get candidate storm coordinates\n",
    "4. For each candidate storm timestamp, find corresponding `GCM output` file\n",
    "5. For each candidate storm timestamp, use storm coordinates to trim time of `GCM output` file in \n",
    "6. For each candidate storm timestamp, use storm coordinates to trim spatial extent of `GCM output` file\n",
    "7. Append information from `track data` to netCDF object containing GCM output\n",
    "9. Save xArray Dataset to netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d142044-6f5f-4c11-bcce-634c88c0e9d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def access_storm_tracks(model_name: str,\n",
    "                        experiment_name: str,\n",
    "                        year_range: tuple[int, int]) -> pd.DataFrame:\n",
    "\n",
    "    ''' Obtain track data for a given model, experiment, and year range. '''\n",
    "\n",
    "    track_data = tc_analysis.load_TC_tracks(model_name, experiment_name, year_range)[model_name][experiment_name]['raw']\n",
    "\n",
    "    return track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd70715b-9fc9-419d-afcc-20911227f7a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_single_storm(storm_track_data: pd.DataFrame):\n",
    "\n",
    "    ''' Provide tests to ensure tracked storm is legitimate. '''\n",
    "    criterion_flag = True\n",
    "\n",
    "    # Look for duplicate storms\n",
    "    # assert len(storm_track_data['duration'].unique()) == 1, 'It looks like two tracked storms are using the same storm ID.'\n",
    "    if len(storm_track_data['duration'].unique()) != 0:\n",
    "        criterion_flag = False\n",
    "\n",
    "    # Constrain the distance the storm can move between two timestamps\n",
    "    threshold_delta_longitude = 10 # units of degrees\n",
    "    threshold_delta_latitude = 10 # units of degrees\n",
    "    \n",
    "    delta_longitude = storm_track_data['center_lon'].diff().dropna().abs()\n",
    "    # assert max(delta_longitude) < threshold_delta_longitude, f'Longitude threshold exceeded, value = {max(delta_longitude):.2f} degrees.'\n",
    "    if max(delta_longitude) >= threshold_delta_longitude:\n",
    "        criterion_flag = False\n",
    "        \n",
    "    delta_latitude = storm_track_data['center_lat'].diff().dropna().abs()\n",
    "    # assert max(delta_latitude) < threshold_delta_latitude, f'Longitude threshold exceeded, value = {max(delta_latitude):.2f} degrees.'\n",
    "    if max(delta_latitude) >= threshold_delta_latitude:\n",
    "        criterion_flag = False\n",
    "        \n",
    "    # Ensure the necessary columns are in the storm DataFrame\n",
    "    # assert ('cftime' in storm_track_data.columns), f'cftime data is not in the track DataFrame.'\n",
    "    if 'cftime' not in storm_track_data.columns:\n",
    "        criterion_flag = False\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cae5e5-12be-4645-ba59-532dc137ccff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def intensity_filter(track_data: pd.DataFrame,\n",
    "                     intensity_parameter: str='min_slp',\n",
    "                     intensity_range: tuple[int|float, int|float]=(0, np.inf)) -> pd.DataFrame:\n",
    "\n",
    "    ''' \n",
    "    Method to filter QuickTracks output track data to find storms that fit a given intensity parameter and range.\n",
    "    Note: storms are only filtered by maximum winds ('max_wind') and minimum sea-level pressure ('min_slp').\n",
    "    '''\n",
    "\n",
    "    assert intensity_parameter in ['min_slp', 'max_wind'], f'Parameter {intensity_parameter} not recognized, please use `max_wind` or `min_slp`.'\n",
    "\n",
    "    # Obtain maximum intensities for each storm\n",
    "    maximum_intensity_by_storm = track_data.groupby('storm_id')[intensity_parameter].min() if intensity_parameter == 'min_slp' else track_data.groupby('storm_id')[intensity_parameter].max()\n",
    "    # Obtain the storm IDs that contain maximum intensities within the given intensity band\n",
    "    threshold_storm_IDs = maximum_intensity_by_storm[((maximum_intensity_by_storm >= min(intensity_range)) & \n",
    "                                                      (maximum_intensity_by_storm < max(intensity_range)))]\n",
    "    # Obtain track entries matching the storms tht meet the threshold\n",
    "    threshold_storms = track_data.loc[track_data['storm_id'].isin(threshold_storm_IDs)]\n",
    "\n",
    "    return threshold_storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b81eb6aa-701a-4d76-b0aa-551ea1852d93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def latitude_filter(track_data: pd.DataFrame,\n",
    "                    intensity_parameter: str='min_slp',\n",
    "                    latitude_range: tuple[int|float, int|float]=(-40, 40)) -> pd.DataFrame:\n",
    "\n",
    "    ''' Filter out TCs with LMIs occurring outside a given latitude band to prevent capturing ETCs. '''\n",
    "\n",
    "    # Initialize a container list for storm IDs that meet the criteria\n",
    "    storm_IDs = []\n",
    "\n",
    "    # Iterate over each storm to determine if its LMI occurs within the given latitude band\n",
    "    for storm_ID, storm_track_data in track_data.groupby('storm_id'):\n",
    "        # Get maximum storm intensity\n",
    "        maximum_intensity = storm_track_data[intensity_parameter].max() if intensity_parameter == 'max_wind' else storm_track_data[intensity_parameter].min()\n",
    "        # Get latitude corresponding to LMI occurrence.\n",
    "        # Note the absolute and maximum functions. This prevents duplicate timestamps with identical maximum intensities of being loaded, gets the more poleward occurrence.\n",
    "        latitude_of_maximum_intensity = abs(storm_track_data.loc[storm_track_data[intensity_parameter] == maximum_intensity]['center_lat']).max()\n",
    "        # Append to list if the latitude of LMI is in the given range\n",
    "        if min(latitude_range) <= latitude_of_maximum_intensity <= max(latitude_range):\n",
    "            storm_IDs.append(storm_ID)\n",
    "\n",
    "    threshold_track_data = track_data.loc[track_data['storm_id'].isin(storm_IDs)]\n",
    "\n",
    "    return threshold_track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6030f3ce-de84-4be8-ae84-63a14c44a9a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pick_storm_IDs(track_data: pd.DataFrame,\n",
    "                   number_of_storms: int) -> list:\n",
    "\n",
    "    ''' Method to obtain `number_of_storms` random storm IDs from a provided track dataset. '''\n",
    "\n",
    "    # Get list of unique storm IDs\n",
    "    unique_storm_IDs = track_data['storm_id'].unique()\n",
    "    # Make sure the number of requested storms is less than the number of unique IDs; \n",
    "    number_of_storms = len(unique_storm_IDs) if number_of_storms > len(unique_storm_IDs) else number_of_storms\n",
    "    # If not, make the number of storms equal to `unique_storm_IDs`\n",
    "    # Get indices for `number_of_storms` random storm IDs\n",
    "    storm_ID_indices = np.random.choice(range(len(unique_storm_IDs)), size=number_of_storms, replace=False)\n",
    "    # Get randomized storm IDs\n",
    "    storm_IDs = track_data['storm_id'].unique()[storm_ID_indices]\n",
    "\n",
    "    return storm_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "085914d6-65c7-4267-9804-b0d716046910",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pick_storm(track_data: pd.DataFrame,\n",
    "               selection_method: str='random',\n",
    "               storm_ID: str|None=None):\n",
    "\n",
    "    ''' Pick a single storm from the track data. '''\n",
    "\n",
    "    if selection_method == 'random':\n",
    "        storm_index = np.random.randint(0, len(track_data['storm_id'].unique())) # choose random storm\n",
    "        storm_ID = track_data['storm_id'].unique()[storm_index]\n",
    "    elif selection_method == 'storm_number' and storm_ID:\n",
    "        assert isinstance(storm_ID, str), 'Storm ID must be a string.'\n",
    "        assert storm_ID in track_data['storm_id'].values, 'Storm ID not found in track dataset.'\n",
    "    else:\n",
    "        print('Please provide a storm ID or set `selection_method` to `random`. Exiting.')\n",
    "        sys.exit()\n",
    "\n",
    "    # Pull data for a specific storm and sort values by time\n",
    "    storm_track_data = track_data.loc[track_data['storm_id'] == storm_ID].sort_values('cftime')\n",
    "    # Check for storm quality\n",
    "    test_single_storm(storm_track_data)\n",
    "\n",
    "    return storm_track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0a656-2b56-4c94-8914-111920597f0d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def storm_GCM_calendar_alignment(storm_timestamps: list[cftime.datetime], \n",
    "                                 GCM_timestamps: list[cftime.datetime]) -> list[cftime.datetime]:\n",
    "\n",
    "    ''' \n",
    "    Ensure GFDL QuickTracks track data and GCM output data have same calendars. \n",
    "    GCM data takes precedence since GCM timestamps are less mutable than QuickTracks timestamps. \n",
    "    '''\n",
    "\n",
    "    # Calendar types (see https://unidata.github.io/cftime/api.html#cftime.datetime for reference)\n",
    "    # Assumes calendars are either `noleap` or `julian` based on GFDL GCM output data\n",
    "    calendar_types = {\"<class 'cftime._cftime.DatetimeJulian'>\": 'julian',\n",
    "                      \"<class 'cftime._cftime.datetime'>\": 'noleap',\n",
    "                      \"<class 'cftime._cftime.DatetimeNoLeap'>\": 'noleap'}\n",
    "    # Scrape calendar type from variable type\n",
    "    get_calendar_type = lambda timestamp: (calendar_types[str(type(timestamp))], timestamp.has_year_zero)\n",
    "    # Function to convert timestamp formats for a given timestamp and calendar\n",
    "    timestamp_conversion = lambda t, calendar, has_year_zero: cftime.datetime(year=t.year,\n",
    "                                                                              month=t.month,\n",
    "                                                                              day=t.day,\n",
    "                                                                              hour=t.hour,\n",
    "                                                                              calendar=calendar,\n",
    "                                                                              has_year_zero=has_year_zero)\n",
    "    \n",
    "    # Iterate through timestamps to get types for storm and GCM data.\n",
    "    # Assume all entries have the same data type.\n",
    "    storm_timestamp_type = str(type(storm_timestamps[0]))\n",
    "    GCM_timestamp_type = str(type(GCM_timestamps[0]))\n",
    "    # print(f'Track data timestamp type: {storm_timestamp_type}; GCM timestamp type: {GCM_timestamp_type}')\n",
    "    \n",
    "    # Reformat storm timestamps to the GCM timestamp format, if different\n",
    "    # Assume all entries have the same data type.\n",
    "    GCM_calendar_type, GCM_has_year_zero = get_calendar_type(GCM_timestamps[0])\n",
    "    # print(GCM_calendar_type, GCM_has_year_zero)\n",
    "    storm_timestamps_reformatted = [timestamp_conversion(t, GCM_calendar_type, GCM_has_year_zero) \n",
    "                                    for index, t in enumerate(storm_timestamps)]\n",
    "    \n",
    "    return storm_timestamps_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea01faf-e39d-4eb6-a868-e8b056d954d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_GCM_data(model_name: str,\n",
    "                       experiment_name: str,\n",
    "                       storm_track_timestamps,\n",
    "                       gcm_data_type: str='atmos_4xdaily') -> list:\n",
    "    \n",
    "    ''' For each candidate storm timestamp, find corresponding `GCM output` file. '''\n",
    "\n",
    "    # Define top-level directory where GCM output is kept\n",
    "    gcm_container_dirname = '/tigress/GEOCLIM/gr7610/MODEL_OUT' \n",
    "    # Define the configuration-specific directory\n",
    "    gcm_dirname = os.path.join(gcm_container_dirname, model_name, experiment_name, 'POSTP') \n",
    "    # Obtain filenames in the configuration-specific directory for the chosen data type\n",
    "    gcm_pathnames = [os.path.join(gcm_dirname, gcm_filename) for gcm_filename in os.listdir(gcm_dirname)\n",
    "                     if gcm_filename.endswith('.nc') and \n",
    "                     gcm_data_type in gcm_filename] \n",
    "    # Ensure files are found in the directory. If not, exit.\n",
    "    if len(gcm_pathnames) > 0:\n",
    "        # Filter pathname list for paths containing years relevant to storm\n",
    "        # Note that a minimum and maximum year is obtained to handle storms that run into the following year\n",
    "        storm_track_year_min, storm_track_year_max = min(storm_track_timestamps).year, max(storm_track_timestamps).year\n",
    "        # Filter by year, ends-inclusive. \n",
    "        # Note that the year is obtained crudely, assuming that GCM model output is in YYYYMMDD.{gcm_data_type}.nc format.\n",
    "        storm_gcm_pathnames = [pathname for pathname in gcm_pathnames \n",
    "                               if int(os.path.basename(pathname).split('.')[0][0:4]) >= storm_track_year_min \n",
    "                               and int(os.path.basename(pathname).split('.')[0][0:4]) <= storm_track_year_max] \n",
    "        # Check on pathnames length\n",
    "        assert (len(storm_gcm_pathnames) > 0), f'No files found in {gcm_dirname} for model {model_name} and experiment {experiment_name} for the years {storm_track_year_min}-{storm_track_year_max}. Please check the directory and retry.'\n",
    "    \n",
    "    else:\n",
    "        print(f'No files found in {gcm_dirname} for model {model_name} and experiment {experiment_name}. Please check the directory and retry.')\n",
    "        sys.exit()\n",
    "\n",
    "    ''' Timestamp alignment. '''\n",
    "    # Get storm GCM timestamps for dataset calendar alignment.\n",
    "    storm_gcm_data_timestamps = xr.open_mfdataset(storm_gcm_pathnames).time.values\n",
    "    # Check if GCM data timestamps are non-noleap. \n",
    "    # If so, assume Julian and convert to match the QuickTracks data with GCM data calendar conventions.\n",
    "    storm_track_timestamps_reformatted = storm_GCM_calendar_alignment(storm_track_timestamps.values,\n",
    "                                                                      storm_gcm_data_timestamps)\n",
    "\n",
    "    return storm_gcm_pathnames, storm_track_timestamps_reformatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e801100-3225-47ca-8423-5f26b3a676b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_coordinates(storm_track_data: pd.DataFrame,\n",
    "                          storm_track_timestamps) -> dict:\n",
    "\n",
    "    ''' Get candidate storm coordinates. '''\n",
    "\n",
    "    # Note the variable name convention ('storm_track' instead of just 'storm').\n",
    "    # This is intended to distinguish tracker coordinates from actual storm centered coordinates, which may be different in GCM output.\n",
    "    \n",
    "    # Initialize coordinates dictionary, which will save a longitude and latitude as values for a timestamp key\n",
    "    storm_track_coordinates = {}\n",
    "    # Iterate over timestamps to pair a timestamp to corresponding coordinates\n",
    "    for storm_track_timestamp in storm_track_timestamps:\n",
    "        # Obtain longitude and latitudes for each timestamp\n",
    "        storm_track_longitude = storm_track_data.loc[storm_track_data['cftime'] == storm_track_timestamp]['center_lon'].item()\n",
    "        storm_track_latitude = storm_track_data.loc[storm_track_data['cftime'] == storm_track_timestamp]['center_lat'].item()\n",
    "        storm_track_coordinates[storm_track_timestamp] = {'lon': storm_track_longitude, 'lat': storm_track_latitude}\n",
    "\n",
    "    return storm_track_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0d1dfb1-c139-4f12-a159-bef55e13abe1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_GCM_data(storm_gcm_pathnames, \n",
    "                  storm_track_timestamps,\n",
    "                  storm_track_coordinates) -> (xr.Dataset, ):\n",
    "\n",
    "    ''' Load and trim the data in time and space. '''\n",
    "    \n",
    "    # Load the data.\n",
    "    storm_gcm_data = xr.open_mfdataset(storm_gcm_pathnames)\n",
    "    # # Check if GCM data timestamps are non-noleap. If so, assume Julian and convert.\n",
    "    # storm_track_timestamps_reformatted = storm_GCM_calendar_alignment(storm_track_timestamps.values,\n",
    "    #                                                                   storm_gcm_data.time.values)\n",
    "    # Obtain timestamps shared by GCM data and track timestamps.\n",
    "    storm_gcm_timestamps = list(set(storm_track_timestamps) & set(storm_gcm_data.time.values))\n",
    "    # Trim storm track coordinates to match the shared timestamps\n",
    "    storm_track_coordinates = {storm_gcm_timestamp: storm_track_coordinates[storm_gcm_timestamp] for storm_gcm_timestamp in storm_gcm_timestamps}\n",
    "    # Trim GCM output data in time.\n",
    "    storm_gcm_data = storm_gcm_data.sel(time=storm_gcm_timestamps)\n",
    "\n",
    "    return storm_gcm_data, storm_gcm_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38123aeb-b280-45a1-b02e-5ea1fee2ea91",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def trim_GCM_data(storm_gcm_data: xr.Dataset,\n",
    "                  storm_gcm_timestamps,\n",
    "                  storm_track_coordinates, \n",
    "                  storm_gcm_window_size: int | float=12) -> xr.Dataset:\n",
    "    \n",
    "    ''' For each candidate storm timestamp, use storm coordinates to trim spatial extent of `GCM output` file. '''\n",
    "\n",
    "    # Initialize a container to hold GCM output connected to each storm timestamp and the corresponding spatial extent\n",
    "    storm_gcm_container = {}\n",
    "    # Generate trimming window extents for each timestamp.\n",
    "    # Window extents are defined as: \n",
    "    # 'grid_xt' = (longitude - window_extent, longitude + window_extent), \n",
    "    # 'grid_yt' = (latitude - window_extent, latitude + window_extent)\n",
    "    storm_gcm_window_extent = {}\n",
    "    for storm_gcm_timestamp in storm_gcm_timestamps:\n",
    "        storm_gcm_window_extent[storm_gcm_timestamp] = {}\n",
    "        # Assign zonal window\n",
    "        storm_gcm_window_extent[storm_gcm_timestamp]['grid_xt'] = slice(storm_track_coordinates[storm_gcm_timestamp]['lon'] - storm_gcm_window_size,\n",
    "                                                                        storm_track_coordinates[storm_gcm_timestamp]['lon'] + storm_gcm_window_size)\n",
    "        # Assign meridional window\n",
    "        storm_gcm_window_extent[storm_gcm_timestamp]['grid_yt'] = slice(storm_track_coordinates[storm_gcm_timestamp]['lat'] - storm_gcm_window_size,\n",
    "                                                                        storm_track_coordinates[storm_gcm_timestamp]['lat'] + storm_gcm_window_size)\n",
    "        # Extract GCM data for the given timestamp and spatial extent\n",
    "        storm_gcm_container[storm_gcm_timestamp] = storm_gcm_data.sel(time=storm_gcm_timestamp,\n",
    "                                                                      grid_xt=storm_gcm_window_extent[storm_gcm_timestamp]['grid_xt'],\n",
    "                                                                      grid_yt=storm_gcm_window_extent[storm_gcm_timestamp]['grid_yt'])\n",
    "    \n",
    "    # Concatenate all GCM output data corresponding to storm into a single xArray Dataset\n",
    "    storm_gcm_data = xr.concat(storm_gcm_container.values(), dim='time').sortby('time')\n",
    "\n",
    "    return storm_gcm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1237343-81d0-42af-964b-4bd86c0b6cfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def join_track_GCM_data(storm_track_data: pd.DataFrame,\n",
    "                        storm_gcm_data: xr.Dataset):\n",
    "\n",
    "    ''' Append information from `track data` to netCDF object containing GCM output. '''\n",
    "    \n",
    "    # Filter storm track data that has matching timestamps with xArray Dataset `storm_gcm_data`\n",
    "    # These should already match, but this is for posterity\n",
    "    storm_track_data_gcm = storm_track_data.loc[storm_track_data['cftime'].isin(storm_gcm_data.time.values)]\n",
    "    # Define variables to append to netCDF\n",
    "    storm_track_gcm_vars = ['center_lon', 'center_lat', 'min_slp', 'max_wind', 'storm_id']\n",
    "    # Test to make sure that the variables requested are all in the track data DataFrame\n",
    "    assert set(storm_track_gcm_vars) <= set(storm_track_data_gcm.columns), 'Not all requested track data columns are available in the given track dataset.'\n",
    "    \n",
    "    # Add the data to the xArray Dataset `storm_gcm_data`\n",
    "    for storm_track_gcm_var in storm_track_gcm_vars:\n",
    "        # Handle storm ID as an attribute since it's time-invariant\n",
    "        if storm_track_gcm_var == 'storm_id':\n",
    "            # Get unique storm ID value\n",
    "            storm_id = storm_track_data_gcm['storm_id'].unique().item()\n",
    "            # Add to xArray Dataset attributes\n",
    "            storm_gcm_data.attrs['storm_id'] = storm_track_data_gcm['storm_id'].unique().item()\n",
    "        # Otherwise, append track data along the time axis\n",
    "        else:\n",
    "            storm_gcm_data[storm_track_gcm_var] = xr.DataArray(data=storm_track_data_gcm[storm_track_gcm_var].values,\n",
    "                                                               dims=['time'],\n",
    "                                                               coords={'time': ('time', storm_gcm_data.time.values)})\n",
    "\n",
    "    return storm_gcm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d926b44-31af-4128-aa15-59c509618557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_basin_name(storm: xr.Dataset) -> str:\n",
    "\n",
    "    ''' \n",
    "    Method to obtain the basin a TC belongs to. Assume the TC's first timestamp is representative of its basin. \n",
    "    \n",
    "    Algorithm: for a given TC, use the coordinates at the first timestamp to determine which basin the TC is in.\n",
    "    '''\n",
    "\n",
    "    # Ensure storm is sorted by time\n",
    "    storm = storm.sortby('time')\n",
    "    # Get coordinates at first timestamp\n",
    "    lon, lat = storm.isel(time=0)['center_lon'].item(), storm.isel(time=0)['center_lat'].item()\n",
    "    # Pull archived basin masks for GCM output dats with 0.5 degree spatial reslution\n",
    "    basin_masks = xr.open_dataset('/projects/GEOCLIM/gr7610/tools/basin_mask.nc')\n",
    "    # Iterate through basin names until a match is found\n",
    "    basin_name = [basin_name for basin_name in basin_masks.keys() if\n",
    "                  basin_name not in ['global', 'IPWP', 'ENSO'] and \n",
    "                  basin_masks[basin_name].sel(grid_xt=lon, method='nearest').sel(grid_yt=lat, method='nearest').item() == 1]\n",
    "    # Ensure the length of the resulting list only has 0 or 1 elements\n",
    "    assert len(basin_name) < 2, f'Storm {storm.attrs['storm_id']} found in basins {basin_name}, investigate why this is the case.'\n",
    "    # Extract the basin name, or if none is found, assign as extratropical\n",
    "    basin_name = 'ET' if len(basin_name) == 0 else basin_name[0] # handle storms that occur outside conventional basis as extratropical (ET)\n",
    "\n",
    "    return basin_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "911d0e83-ff48-4457-b88b-51f2d872b3e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_storm_netcdf(storm_gcm_data: xr.Dataset,\n",
    "                      overwrite: bool=False):\n",
    "    \n",
    "    ''' Save xArray Dataset to netCDF file. '''\n",
    "\n",
    "    # Define storage directory\n",
    "    storage_dirname = '/projects/GEOCLIM/gr7610/analysis/tc_storage/individual_TCs'\n",
    "    \n",
    "    # Obtain parameters for filename construction\n",
    "    storm_ID = storm_gcm_data.attrs['storm_id']\n",
    "    max_wind = f'{np.round(storm_gcm_data['max_wind'].max()):.0f}' # round to nearest integer for brevity\n",
    "    min_slp = f'{np.round(storm_gcm_data['min_slp'].min()):.0f}' # round to nearest integer for brevity\n",
    "\n",
    "    # Obtain the basin name for the storm filename\n",
    "    storm_basin = get_storm_basin_name(storm_gcm_data)\n",
    "    # Build storm filename\n",
    "    storm_filename = f'TC.model-{model_name}.experiment-{experiment_name}.storm_ID-{storm_ID}.max_wind-{max_wind}.min_slp-{min_slp}.basin-{storm_basin}.nc'\n",
    "    storm_pathname = os.path.join(storage_dirname, storm_filename)\n",
    "    \n",
    "    # Load the data into memory before saving to ensure output is fully there\n",
    "    print(f'[save_storm_netcdf] Loading data for {storm_filename}')\n",
    "\n",
    "    # Profile loading time\n",
    "    start_time = time.time()\n",
    "    storm_gcm_data.load()\n",
    "    print(f'Elapsed loading time for {storm_filename}: {(time.time() - start_time):.2f} s')\n",
    "    \n",
    "    # Print output file size as a diagnostic\n",
    "    print(f'File size for {storm_filename}: {(storm_gcm_data.nbytes / 1e6):.2f} MB\\n')\n",
    "    \n",
    "    # Save the data\n",
    "    storm_gcm_data.to_netcdf(storm_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96e7c12f-6b97-4d06-8860-7a7f4a74c5db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def storm_generator(track_data: pd.DataFrame,\n",
    "                    storm_ID: str):\n",
    "\n",
    "    ''' Method to perform all steps related to binding corresponding GFDL QuickTracks and GCM model output together for a given TC. '''\n",
    "\n",
    "    print(f'[storm_generator] Processing storm ID {storm_ID}...')\n",
    "    \n",
    "    # 3. Find a candidate storm from the track data\n",
    "    storm_track_data = pick_storm(track_data, selection_method='storm_number', storm_ID=storm_ID)\n",
    "    # 4. Get candidate storm timestamps\n",
    "    storm_track_timestamps = storm_track_data['cftime']\n",
    "    # 5. For each candidate storm timestamp, find corresponding `GCM output` file\n",
    "    storm_gcm_pathnames, storm_track_timestamps = get_storm_GCM_data(model_name, experiment_name, storm_track_timestamps)\n",
    "    # 5a. Correct GFDL QuickTracks cftime timestamp format to match GCM output format\n",
    "    storm_track_data['cftime'] = storm_track_timestamps\n",
    "    # 6. Get candidate storm coordinates\n",
    "    storm_track_coordinates = get_storm_coordinates(storm_track_data, storm_track_timestamps)\n",
    "    # 7. For each candidate storm timestamp, load GCM data and use storm timestamps to trim time of `GCM output` file\n",
    "    storm_gcm_data, storm_gcm_timestamps = load_GCM_data(storm_gcm_pathnames, storm_track_timestamps, storm_track_coordinates)\n",
    "    # 8. For each candidate storm timestamp, use storm coordinates to trim spatial extent of `GCM output` file\n",
    "    storm_gcm_data = trim_GCM_data(storm_gcm_data, storm_gcm_timestamps, storm_track_coordinates)\n",
    "    # 9. Append information from `track data` to netCDF object containing GCM output\n",
    "    storm_gcm_data = join_track_GCM_data(storm_track_data, storm_gcm_data)\n",
    "    # 10. Save xArray Dataset to netCDF file\n",
    "    save_storm_netcdf(storm_gcm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "277005e5-d846-47f8-a00c-6687959e49b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def main(model_name: str, \n",
    "         experiment_name: str, \n",
    "         year_range: tuple[int, int],\n",
    "         intensity_parameter: str,\n",
    "         intensity_range: tuple[int|float, int|float]=(0, np.inf),\n",
    "         latitude_range: tuple[int|float, int|float]=(-40, 40),\n",
    "         number_of_storms: int=1):\n",
    "\n",
    "    # 0. Obtain track data for a given model, experiment, and year range\n",
    "    track_data = access_storm_tracks(model_name, experiment_name, year_range)\n",
    "    # 1a. Filter storms by intensity\n",
    "    track_data = intensity_filter(track_data, intensity_parameter, intensity_range)\n",
    "    # 1b. Filter storms by latitude\n",
    "    track_data = latitude_filter(track_data, intensity_parameter, latitude_range)\n",
    "    # 2. Obtain N randomized storm IDs from the filtered data, where 'N' is `number_of_storms`\n",
    "    storm_IDs = pick_storm_IDs(track_data, number_of_storms)\n",
    "\n",
    "    ''' Offload TC-specific data generation onto parallel processes. '''\n",
    "    # Maximum number of processors for computation\n",
    "    max_number_procs = 20\n",
    "    # Specify number of processors to use\n",
    "    number_procs = len(storm_IDs) if len(storm_IDs) < max_number_procs else max_number_procs\n",
    "    # Define partial function to allow for using Pool.map since `track_data` is equivalent for all subprocesses\n",
    "    preloaded_storm_generator = functools.partial(storm_generator, track_data)\n",
    "    \n",
    "    with Pool(processes=number_procs) as pool:\n",
    "        pool.map(preloaded_storm_generator, storm_IDs)\n",
    "        pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb4ab1-3926-4d47-9448-a373913e441d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for FLOR, experiment CTL1990s_FA_tiger3 from /projects/GEOCLIM/gr7610/analysis/tc_storage/track_data/TC_track_data.s1901_e1905.model_FLOR.experiment_CTL1990s_FA_tiger3.pkl...\n",
      "[load_TC_tracks] month range: (1, 13)\n",
      "[load_TC_tracks] month range: (1, 13)\n",
      "-------------------------------------------------------------\n",
      "Statistics for TCs in model: FLOR; experiment: CTL1990s_FA_tiger3\n",
      "Number of storms: total = 212; per year = 71.0\n",
      "Storm duration: mean = 10.80 +/- 4.71 days\n",
      "Storm maximum winds: mean = 28.26 +/- 4.62 m/s\n",
      "Storm minimum pressure: mean = 981.69 +/- 14.98 hPa\n",
      "-------------------------------------------------------------\n",
      "\n",
      "[storm_generator] Processing storm ID 1902-3604...\n",
      "[storm_generator] Processing storm ID 1903-1478...\n",
      "[storm_generator] Processing storm ID 1902-0586...\n",
      "[storm_generator] Processing storm ID 1902-3877...\n",
      "[storm_generator] Processing storm ID 1901-0768...\n",
      "Track data timestamp type: <class 'cftime._cftime.datetime'>; GCM timestamp type: <class 'cftime._cftime.DatetimeJulian'>\n",
      "[save_storm_netcdf] Loading data for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1903-1478.max_wind-31.min_slp-994.basin-SI.nc\n",
      "Track data timestamp type: <class 'cftime._cftime.datetime'>; GCM timestamp type: <class 'cftime._cftime.DatetimeJulian'>\n",
      "[save_storm_netcdf] Loading data for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1901-0768.max_wind-23.min_slp-989.basin-SP.nc\n",
      "Track data timestamp type: <class 'cftime._cftime.datetime'>; GCM timestamp type: <class 'cftime._cftime.DatetimeJulian'>Track data timestamp type: <class 'cftime._cftime.datetime'>; GCM timestamp type: <class 'cftime._cftime.DatetimeJulian'>Track data timestamp type: <class 'cftime._cftime.datetime'>; GCM timestamp type: <class 'cftime._cftime.DatetimeJulian'>\n",
      "\n",
      "\n",
      "[save_storm_netcdf] Loading data for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-3604.max_wind-33.min_slp-960.basin-WP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-0586.max_wind-39.min_slp-939.basin-SP.nc\n",
      "[save_storm_netcdf] Loading data for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-3877.max_wind-36.min_slp-960.basin-NA.nc\n",
      "Elapsed loading time for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1903-1478.max_wind-31.min_slp-994.basin-SI.nc: 54.94 s\n",
      "File size for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1903-1478.max_wind-31.min_slp-994.basin-SI.nc: 4.72 MB\n",
      "\n",
      "Elapsed loading time for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-3877.max_wind-36.min_slp-960.basin-NA.nc: 158.95 s\n",
      "File size for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-3877.max_wind-36.min_slp-960.basin-NA.nc: 55.58 MB\n",
      "\n",
      "Elapsed loading time for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-0586.max_wind-39.min_slp-939.basin-SP.nc: 166.02 s\n",
      "File size for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-0586.max_wind-39.min_slp-939.basin-SP.nc: 65.34 MB\n",
      "\n",
      "Elapsed loading time for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-3604.max_wind-33.min_slp-960.basin-WP.nc: 167.68 s\n",
      "File size for TC.model-FLOR.experiment-CTL1990s_FA_tiger3.storm_ID-1902-3604.max_wind-33.min_slp-960.basin-WP.nc: 25.72 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(track_TCs);\n",
    "\n",
    "model_name = 'FLOR'\n",
    "experiment_name = 'CTL1990s_FA_tiger3'\n",
    "year_range = (1901, 1905)\n",
    "\n",
    "intensity_parameter = 'min_slp'\n",
    "intensity_range = (980, 1000)\n",
    "\n",
    "main(model_name, experiment_name, year_range, intensity_parameter, intensity_range, number_of_storms=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b417faee-8c91-4942-a73a-6e6e1dae4770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
