{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6f7ae02-d28c-43f2-9e4a-872fc6a1f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Import packages. '''\n",
    "# Time packages\n",
    "import cftime, datetime, time\n",
    "# Numerical analysis packages\n",
    "import numpy as np, random, scipy, numba\n",
    "# Local data storage packages\n",
    "import functools, os, pickle, collections, sys\n",
    "# Data structure packages\n",
    "import pandas as pd, xarray as xr, nc_time_axis\n",
    "xr.set_options(keep_attrs=True)\n",
    "# Visualization tools\n",
    "import cartopy, cartopy.crs as ccrs, matplotlib, matplotlib.pyplot as plt\n",
    "# Local imports\n",
    "import accessor, composite, composite_snapshots, derived, utilities, socket, visualization, tc_analysis, tc_processing, track_TCs\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import importlib\n",
    "importlib.reload(composite);\n",
    "importlib.reload(xr);\n",
    "importlib.reload(composite_snapshots);\n",
    "importlib.reload(utilities);\n",
    "importlib.reload(tc_analysis);\n",
    "importlib.reload(tc_processing);\n",
    "importlib.reload(visualization);\n",
    "importlib.reload(derived);\n",
    "importlib.reload(track_TCs);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416f556-3712-4a00-91b9-06069b99c4e4",
   "metadata": {},
   "source": [
    "#### Storm tracking methodology\n",
    "\n",
    "__Storm datasets__: \n",
    "- `GCM output`: output from global climate model\n",
    "- `track data`: output from GFDL QuickTracks model that is run on `GCM output`\n",
    "\n",
    "__For each storm__:\n",
    "1. Find a candidate storm from `track data`\n",
    "2. Get candidate storm timestamps\n",
    "3. Get candidate storm coordinates\n",
    "4. For each candidate storm timestamp, find corresponding `GCM output` file\n",
    "5. For each candidate storm timestamp, use storm coordinates to trim time of `GCM output` file in \n",
    "6. For each candidate storm timestamp, use storm coordinates to trim spatial extent of `GCM output` file\n",
    "7. Append information from `track data` to netCDF object containing GCM output\n",
    "9. Save xArray Dataset to netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d142044-6f5f-4c11-bcce-634c88c0e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_storm_tracks(model_name: str,\n",
    "                        experiment_name: str,\n",
    "                        year_range: tuple[int, int]) -> pd.DataFrame:\n",
    "\n",
    "    ''' Obtain track data for a given model, experiment, and year range. '''\n",
    "\n",
    "    track_data = tc_analysis.load_TC_tracks(model_name, experiment_name, year_range)[model_name][experiment_name]['raw']\n",
    "\n",
    "    return track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd70715b-9fc9-419d-afcc-20911227f7a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_single_storm(storm_track_data: pd.DataFrame):\n",
    "\n",
    "    ''' Provide tests to ensure tracked storm is legitimate. '''\n",
    "    criterion_flag = True\n",
    "\n",
    "    # Look for duplicate storms\n",
    "    # assert len(storm_track_data['duration'].unique()) == 1, 'It looks like two tracked storms are using the same storm ID.'\n",
    "    if len(storm_track_data['duration'].unique()) != 0:\n",
    "        criterion_flag = False\n",
    "\n",
    "    # Constrain the distance the storm can move between two timestamps\n",
    "    threshold_delta_longitude = 10 # units of degrees\n",
    "    threshold_delta_latitude = 10 # units of degrees\n",
    "    \n",
    "    delta_longitude = storm_track_data['center_lon'].diff().dropna().abs()\n",
    "    # assert max(delta_longitude) < threshold_delta_longitude, f'Longitude threshold exceeded, value = {max(delta_longitude):.2f} degrees.'\n",
    "    if max(delta_longitude) >= threshold_delta_longitude:\n",
    "        criterion_flag = False\n",
    "        \n",
    "    delta_latitude = storm_track_data['center_lat'].diff().dropna().abs()\n",
    "    # assert max(delta_latitude) < threshold_delta_latitude, f'Longitude threshold exceeded, value = {max(delta_latitude):.2f} degrees.'\n",
    "    if max(delta_latitude) >= threshold_delta_latitude:\n",
    "        criterion_flag = False\n",
    "        \n",
    "    # Ensure the necessary columns are in the storm DataFrame\n",
    "    # assert ('cftime' in storm_track_data.columns), f'cftime data is not in the track DataFrame.'\n",
    "    if 'cftime' not in storm_track_data.columns:\n",
    "        criterion_flag = False\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9cae5e5-12be-4645-ba59-532dc137ccff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def intensity_filter(track_data: pd.DataFrame,\n",
    "                     intensity_parameter: str='min_slp',\n",
    "                     intensity_range: tuple[int|float, int|float]=(0, np.inf)) -> pd.DataFrame:\n",
    "\n",
    "    ''' \n",
    "    Method to filter QuickTracks output track data to find storms that fit a given intensity parameter and range.\n",
    "    Note: storms are only filtered by maximum winds ('max_wind') and minimum sea-level pressure ('min_slp').\n",
    "    '''\n",
    "\n",
    "    assert intensity_parameter in ['min_slp', 'max_wind'], f'Parameter {intensity_parameter} not recognized, please use `max_wind` or `min_slp`.'\n",
    "\n",
    "    # Obtain the storm IDs that occur within the given intensity band\n",
    "    threshold_storm_IDs = track_data.loc[((track_data[intensity_parameter] >= min(intensity_range)) & \n",
    "                                          (track_data[intensity_parameter] < max(intensity_range)))]['storm_id'].unique()\n",
    "    # Obtain track entries matching the storms tht meet the threshold\n",
    "    threshold_storms = track_data.loc[track_data['storm_id'].isin(threshold_storm_IDs)]\n",
    "\n",
    "    return threshold_storms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b81eb6aa-701a-4d76-b0aa-551ea1852d93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def latitude_filter(track_data: pd.DataFrame,\n",
    "                    intensity_parameter: str='min_slp',\n",
    "                    latitude_range: tuple[int|float, int|float]=(-40, 40)) -> pd.DataFrame:\n",
    "\n",
    "    ''' Filter out TCs with LMIs occurring outside a given latitude band to prevent capturing ETCs. '''\n",
    "\n",
    "    # Initialize a container list for storm IDs that meet the criteria\n",
    "    storm_IDs = []\n",
    "\n",
    "    # Iterate over each storm to determine if its LMI occurs within the given latitude band\n",
    "    for storm_ID, storm_track_data in track_data.groupby('storm_id'):\n",
    "        # Get maximum storm intensity\n",
    "        maximum_intensity = storm_track_data[intensity_parameter].max() if intensity_parameter == 'max_wind' else storm_track_data[intensity_parameter].min()\n",
    "        # Get latitude corresponding to LMI occurrence.\n",
    "        # Note the absolute and maximum functions. This prevents duplicate timestamps with identical maximum intensities of being loaded, gets the more poleward occurrence.\n",
    "        latitude_of_maximum_intensity = abs(storm_track_data.loc[storm_track_data[intensity_parameter] == maximum_intensity]['center_lat']).max()\n",
    "        # Append to list if the latitude of LMI is in the given range\n",
    "        if min(latitude_range) <= latitude_of_maximum_intensity <= max(latitude_range):\n",
    "            storm_IDs.append(storm_ID)\n",
    "\n",
    "    threshold_track_data = track_data.loc[track_data['storm_id'].isin(storm_IDs)]\n",
    "\n",
    "    return threshold_track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6030f3ce-de84-4be8-ae84-63a14c44a9a4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pick_storm_IDs(track_data: pd.DataFrame,\n",
    "                   number_of_storms: int) -> list:\n",
    "\n",
    "    ''' Method to obtain `number_of_storms` random storm IDs from a provided track dataset. '''\n",
    "\n",
    "    # Get list of unique storm IDs\n",
    "    unique_storm_IDs = track_data['storm_id'].unique()\n",
    "    # Make sure the number of requested storms is less than the number of unique IDs; \n",
    "    number_of_storms = len(unique_storm_IDs) if number_of_storms > len(unique_storm_IDs) else number_of_storms\n",
    "    # If not, make the number of storms equal to `unique_storm_IDs`\n",
    "    # Get indices for `number_of_storms` random storm IDs\n",
    "    storm_ID_indices = np.random.choice(range(len(unique_storm_IDs)), size=number_of_storms, replace=False)\n",
    "    # Get randomized storm IDs\n",
    "    storm_IDs = track_data['storm_id'].unique()[storm_ID_indices]\n",
    "\n",
    "    return storm_IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "085914d6-65c7-4267-9804-b0d716046910",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def pick_storm(track_data: pd.DataFrame,\n",
    "               selection_method: str='random',\n",
    "               storm_ID: str|None=None):\n",
    "\n",
    "    ''' Pick a single storm from the track data. '''\n",
    "\n",
    "    if selection_method == 'random':\n",
    "        storm_index = np.random.randint(0, len(track_data['storm_id'].unique())) # choose random storm\n",
    "        storm_ID = track_data['storm_id'].unique()[storm_index]\n",
    "    elif selection_method == 'storm_number' and storm_ID:\n",
    "        assert isinstance(storm_ID, str), 'Storm ID must be a string.'\n",
    "        assert storm_ID in track_data['storm_id'].values, 'Storm ID not found in track dataset.'\n",
    "    else:\n",
    "        print('Please provide a storm ID or set `selection_method` to `random`. Exiting.')\n",
    "        sys.exit()\n",
    "\n",
    "    # Pull data for a specific storm and sort values by time\n",
    "    storm_track_data = track_data.loc[track_data['storm_id'] == storm_ID].sort_values('cftime')\n",
    "    # Check for storm quality\n",
    "    test_single_storm(storm_track_data)\n",
    "\n",
    "    return storm_track_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e801100-3225-47ca-8423-5f26b3a676b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_coordinates(storm_track_data: pd.DataFrame,\n",
    "                          storm_track_timestamps) -> dict:\n",
    "\n",
    "    ''' Get candidate storm coordinates. '''\n",
    "\n",
    "    # Note the variable name convention ('storm_track' instead of just 'storm').\n",
    "    # This is intended to distinguish tracker coordinates from actual storm centered coordinates, which may be different in GCM output.\n",
    "    \n",
    "    # Initialize coordinates dictionary, which will save a longitude and latitude as values for a timestamp key\n",
    "    storm_track_coordinates = {}\n",
    "    # Iterate over timestamps to pair a timestamp to corresponding coordinates\n",
    "    for storm_track_timestamp in storm_track_timestamps:\n",
    "        # Obtain longitude and latitudes for each timestamp\n",
    "        storm_track_longitude = storm_track_data.loc[storm_track_data['cftime'] == storm_track_timestamp]['center_lon'].item()\n",
    "        storm_track_latitude = storm_track_data.loc[storm_track_data['cftime'] == storm_track_timestamp]['center_lat'].item()\n",
    "        storm_track_coordinates[storm_track_timestamp] = {'lon': storm_track_longitude, 'lat': storm_track_latitude}\n",
    "\n",
    "    return storm_track_coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ea01faf-e39d-4eb6-a868-e8b056d954d1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_GCM_data(model_name: str,\n",
    "                       experiment_name: str,\n",
    "                       storm_track_timestamps,\n",
    "                       gcm_data_type: str='atmos_4xdaily') -> list:\n",
    "    \n",
    "    ''' For each candidate storm timestamp, find corresponding `GCM output` file. '''\n",
    "\n",
    "    # Define top-level directory where GCM output is kept\n",
    "    gcm_container_dirname = '/tigress/GEOCLIM/gr7610/MODEL_OUT' \n",
    "    # Define the configuration-specific directory\n",
    "    gcm_dirname = os.path.join(gcm_container_dirname, model_name, experiment_name, 'POSTP') \n",
    "    # Obtain filenames in the configuration-specific directory for the chosen data type\n",
    "    gcm_pathnames = [os.path.join(gcm_dirname, gcm_filename) for gcm_filename in os.listdir(gcm_dirname)\n",
    "                     if gcm_filename.endswith('.nc') and \n",
    "                     gcm_data_type in gcm_filename] \n",
    "    # Ensure files are found in the directory. If not, exit.\n",
    "    if len(gcm_pathnames) > 0:\n",
    "        # Filter pathname list for paths containing years relevant to storm\n",
    "        # Note that a minimum and maximum year is obtained to handle storms that run into the following year\n",
    "        storm_track_year_min, storm_track_year_max = min(storm_track_timestamps).year, max(storm_track_timestamps).year\n",
    "        # Filter by year, ends-inclusive. \n",
    "        # Note that the year is obtained crudely, assuming that GCM model output is in YYYYMMDD.{gcm_data_type}.nc format.\n",
    "        storm_gcm_pathnames = [pathname for pathname in gcm_pathnames \n",
    "                               if int(os.path.basename(pathname).split('.')[0][0:4]) >= storm_track_year_min \n",
    "                               and int(os.path.basename(pathname).split('.')[0][0:4]) <= storm_track_year_max] \n",
    "        # Check on pathnames length\n",
    "        assert (len(storm_gcm_pathnames) > 0), f'No files found in {gcm_dirname} for model {model_name} and experiment {experiment_name} for the years {storm_track_year_min}-{storm_track_year_max}. Please check the directory and retry.'\n",
    "    \n",
    "    else:\n",
    "        print(f'No files found in {gcm_dirname} for model {model_name} and experiment {experiment_name}. Please check the directory and retry.')\n",
    "        sys.exit()\n",
    "\n",
    "    return storm_gcm_pathnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0d1dfb1-c139-4f12-a159-bef55e13abe1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_GCM_data(storm_gcm_pathnames, \n",
    "                  storm_track_timestamps,\n",
    "                  storm_track_coordinates) -> (xr.Dataset, ):\n",
    "\n",
    "    ''' Load and trim the data in time and space. '''\n",
    "    \n",
    "    # Load the data.\n",
    "    storm_gcm_data = xr.open_mfdataset(storm_gcm_pathnames)\n",
    "    # Obtain timestamps shared by GCM data and track timestamps.\n",
    "    storm_gcm_timestamps = list(set(storm_track_timestamps) & set(storm_gcm_data.time.values))\n",
    "    # Trim storm track coordinates to match the shared timestamps\n",
    "    storm_track_coordinates = {storm_gcm_timestamp: storm_track_coordinates[storm_gcm_timestamp] for storm_gcm_timestamp in storm_gcm_timestamps}\n",
    "    # Trim GCM output data in time.\n",
    "    storm_gcm_data = storm_gcm_data.sel(time=storm_gcm_timestamps)\n",
    "\n",
    "    return storm_gcm_data, storm_gcm_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38123aeb-b280-45a1-b02e-5ea1fee2ea91",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def trim_GCM_data(storm_gcm_data: xr.Dataset,\n",
    "                  storm_gcm_timestamps,\n",
    "                  storm_track_coordinates, \n",
    "                  storm_gcm_window_size: int | float=10) -> xr.Dataset:\n",
    "    \n",
    "    ''' For each candidate storm timestamp, use storm coordinates to trim spatial extent of `GCM output` file. '''\n",
    "\n",
    "    # Initialize a container to hold GCM output connected to each storm timestamp and the corresponding spatial extent\n",
    "    storm_gcm_container = {}\n",
    "    # Define trim window size in units of degrees.\n",
    "    storm_gcm_window_size = 10\n",
    "    # Generate trimming window extents for each timestamp.\n",
    "    # Window extents are defined as: \n",
    "    # 'grid_xt' = (longitude - window_extent, longitude + window_extent), \n",
    "    # 'grid_yt' = (latitude - window_extent, latitude + window_extent)\n",
    "    storm_gcm_window_extent = {}\n",
    "    for storm_gcm_timestamp in storm_gcm_timestamps:\n",
    "        storm_gcm_window_extent[storm_gcm_timestamp] = {}\n",
    "        # Assign zonal window\n",
    "        storm_gcm_window_extent[storm_gcm_timestamp]['grid_xt'] = slice(storm_track_coordinates[storm_gcm_timestamp]['lon'] - storm_gcm_window_size,\n",
    "                                                                        storm_track_coordinates[storm_gcm_timestamp]['lon'] + storm_gcm_window_size)\n",
    "        # Assign meridional window\n",
    "        storm_gcm_window_extent[storm_gcm_timestamp]['grid_yt'] = slice(storm_track_coordinates[storm_gcm_timestamp]['lat'] - storm_gcm_window_size,\n",
    "                                                                        storm_track_coordinates[storm_gcm_timestamp]['lat'] + storm_gcm_window_size)\n",
    "        # Extract GCM data for the given timestamp and spatial extent\n",
    "        storm_gcm_container[storm_gcm_timestamp] = storm_gcm_data.sel(time=storm_gcm_timestamp,\n",
    "                                                                      grid_xt=storm_gcm_window_extent[storm_gcm_timestamp]['grid_xt'],\n",
    "                                                                      grid_yt=storm_gcm_window_extent[storm_gcm_timestamp]['grid_yt'])\n",
    "    \n",
    "    # Concatenate all GCM output data corresponding to storm into a single xArray Dataset\n",
    "    storm_gcm_data = xr.concat(storm_gcm_container.values(), dim='time').sortby('time')\n",
    "\n",
    "    return storm_gcm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c1237343-81d0-42af-964b-4bd86c0b6cfc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def join_track_GCM_data(storm_track_data: pd.DataFrame,\n",
    "                        storm_gcm_data: xr.Dataset):\n",
    "\n",
    "    ''' Append information from `track data` to netCDF object containing GCM output. '''\n",
    "    \n",
    "    # Filter storm track data that has matching timestamps with xArray Dataset `storm_gcm_data`\n",
    "    # These should already match, but this is for posterity\n",
    "    storm_track_data_gcm = storm_track_data.loc[storm_track_data['cftime'].isin(storm_gcm_data.time.values)]\n",
    "    # Define variables to append to netCDF\n",
    "    storm_track_gcm_vars = ['center_lon', 'center_lat', 'min_slp', 'max_wind', 'storm_id']\n",
    "    # Test to make sure that the variables requested are all in the track data DataFrame\n",
    "    assert set(storm_track_gcm_vars) <= set(storm_track_data_gcm.columns), 'Not all requested track data columns are available in the given track dataset.'\n",
    "    \n",
    "    # Add the data to the xArray Dataset `storm_gcm_data`\n",
    "    for storm_track_gcm_var in storm_track_gcm_vars:\n",
    "        # Handle storm ID as an attribute since it's time-invariant\n",
    "        if storm_track_gcm_var == 'storm_id':\n",
    "            # Get unique storm ID value\n",
    "            storm_id = storm_track_data_gcm['storm_id'].unique().item()\n",
    "            # Add to xArray Dataset attributes\n",
    "            storm_gcm_data.attrs['storm_id'] = storm_track_data_gcm['storm_id'].unique().item()\n",
    "        # Otherwise, append track data along the time axis\n",
    "        else:\n",
    "            storm_gcm_data[storm_track_gcm_var] = xr.DataArray(data=storm_track_data_gcm[storm_track_gcm_var].values,\n",
    "                                                               dims=['time'],\n",
    "                                                               coords={'time': ('time', storm_gcm_data.time.values)})\n",
    "\n",
    "    return storm_gcm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d926b44-31af-4128-aa15-59c509618557",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_storm_basin_name(storm: xr.Dataset) -> str:\n",
    "\n",
    "    ''' \n",
    "    Method to obtain the basin a TC belongs to. Assume the TC's first timestamp is representative of its basin. \n",
    "    \n",
    "    Algorithm: for a given TC, use the coordinates at the first timestamp to determine which basin the TC is in.\n",
    "    '''\n",
    "\n",
    "    # Ensure storm is sorted by time\n",
    "    storm = storm.sortby('time')\n",
    "    # Get coordinates at first timestamp\n",
    "    lon, lat = storm.isel(time=0)['center_lon'].item(), storm.isel(time=0)['center_lat'].item()\n",
    "    # Pull archived basin masks for GCM output dats with 0.5 degree spatial reslution\n",
    "    basin_masks = xr.open_dataset('/projects/GEOCLIM/gr7610/tools/basin_mask.nc')\n",
    "    # Iterate through basin names until a match is found\n",
    "    basin_name = [basin_name for basin_name in basin_masks.keys() if\n",
    "                  basin_name not in ['global', 'IPWP', 'ENSO'] and \n",
    "                  basin_masks[basin_name].sel(grid_xt=lon, method='nearest').sel(grid_yt=lat, method='nearest').item() == 1]\n",
    "    # Ensure the length of the resulting list only has 0 or 1 elements\n",
    "    assert len(basin_name) < 2, f'Storm {storm.attrs['storm_id']} found in basins {basin_name}, investigate why this is the case.'\n",
    "    # Extract the basin name, or if none is found, assign as extratropical\n",
    "    basin_name = 'ET' if len(basin_name) == 0 else basin_name[0] # handle storms that occur outside conventional basis as extratropical (ET)\n",
    "\n",
    "    return basin_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "911d0e83-ff48-4457-b88b-51f2d872b3e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_storm_netcdf(storm_gcm_data: xr.Dataset,\n",
    "                      overwrite: bool=False):\n",
    "    \n",
    "    ''' Save xArray Dataset to netCDF file. '''\n",
    "\n",
    "    # Define storage directory\n",
    "    storage_dirname = '/projects/GEOCLIM/gr7610/analysis/tc_storage/individual_TCs'\n",
    "    \n",
    "    # Obtain parameters for filename construction\n",
    "    storm_ID = storm_gcm_data.attrs['storm_id']\n",
    "    max_wind = f'{np.round(storm_gcm_data['max_wind'].max()):.0f}' # round to nearest integer for brevity\n",
    "    min_slp = f'{np.round(storm_gcm_data['min_slp'].min()):.0f}' # round to nearest integer for brevity\n",
    "\n",
    "    # Obtain the basin name for the storm filename\n",
    "    storm_basin = get_storm_basin_name(storm_gcm_data)\n",
    "    # Build storm filename\n",
    "    storm_filename = f'TC.model-{model_name}.experiment-{experiment_name}.storm_ID-{storm_ID}.max_wind-{max_wind}.min_slp-{min_slp}.basin-{storm_basin}.nc'\n",
    "    storm_pathname = os.path.join(storage_dirname, storm_filename)\n",
    "    \n",
    "    # Load the data into memory before saving to ensure output is fully there\n",
    "    storm_gcm_data.load()\n",
    "    \n",
    "    # Print output file size as a diagnostic\n",
    "    print(f'File size for {storm_filename}: {(storm_gcm_data.nbytes / 1e6):.2f} MB\\n')\n",
    "    \n",
    "    # Save the data\n",
    "    storm_gcm_data.to_netcdf(storm_pathname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96e7c12f-6b97-4d06-8860-7a7f4a74c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def storm_generator(track_data: pd.DataFrame,\n",
    "                    storm_ID: str):\n",
    "\n",
    "    ''' Method to perform all steps related to binding corresponding GFDL QuickTracks and GCM model output together for a given TC. '''\n",
    "\n",
    "    print(f'[storm_generator] Processing storm ID {storm_ID}...')\n",
    "    \n",
    "    # 3. Find a candidate storm from the track data\n",
    "    storm_track_data = pick_storm(track_data, selection_method='storm_number', storm_ID=storm_ID)\n",
    "    # 4. Get candidate storm timestamps\n",
    "    storm_track_timestamps = storm_track_data['cftime']\n",
    "    # 5. Get candidate storm coordinates\n",
    "    storm_track_coordinates = get_storm_coordinates(storm_track_data, storm_track_timestamps)\n",
    "    # 6. For each candidate storm timestamp, find corresponding `GCM output` file\n",
    "    storm_gcm_pathnames = get_storm_GCM_data(model_name, experiment_name, storm_track_timestamps)\n",
    "    # 7. For each candidate storm timestamp, load GCM data and use storm timestamps to trim time of `GCM output` file\n",
    "    storm_gcm_data, storm_gcm_timestamps = load_GCM_data(storm_gcm_pathnames, storm_track_timestamps, storm_track_coordinates)\n",
    "    # 8. For each candidate storm timestamp, use storm coordinates to trim spatial extent of `GCM output` file\n",
    "    storm_gcm_data = trim_GCM_data(storm_gcm_data, storm_gcm_timestamps, storm_track_coordinates)\n",
    "    # 9. Append information from `track data` to netCDF object containing GCM output\n",
    "    storm_gcm_data = join_track_GCM_data(storm_track_data, storm_gcm_data)\n",
    "    # 10. Save xArray Dataset to netCDF file\n",
    "    save_storm_netcdf(storm_gcm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "277005e5-d846-47f8-a00c-6687959e49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model_name: str, \n",
    "         experiment_name: str, \n",
    "         year_range: tuple[int, int],\n",
    "         intensity_parameter: str,\n",
    "         intensity_range: tuple[int|float, int|float]=(0, np.inf),\n",
    "         latitude_range: tuple[int|float, int|float]=(-40, 40),\n",
    "         number_of_storms: int=1):\n",
    "\n",
    "    # 0. Obtain track data for a given model, experiment, and year range\n",
    "    track_data = access_storm_tracks(model_name, experiment_name, year_range)\n",
    "    # 1a. Filter storms by intensity\n",
    "    track_data = intensity_filter(track_data, intensity_parameter, intensity_range)\n",
    "    # 1b. Filter storms by latitude\n",
    "    track_data = latitude_filter(track_data, intensity_parameter, latitude_range)\n",
    "    # 2. Obtain N randomized storm IDs from the filtered data, where 'N' is `number_of_storms`\n",
    "    storm_IDs = pick_storm_IDs(track_data, number_of_storms)\n",
    "\n",
    "    ''' Offload TC-specific data generation onto parallel processes. '''\n",
    "    # Maximum number of processors for computation\n",
    "    max_number_procs = 20\n",
    "    # Specify number of processors to use\n",
    "    number_procs = len(storm_IDs) if len(storm_IDs) < max_number_procs else max_number_procs\n",
    "    # Define partial function to allow for using Pool.map since `track_data` is equivalent for all subprocesses\n",
    "    preloaded_storm_generator = functools.partial(storm_generator, track_data)\n",
    "    \n",
    "    with Pool(processes=20) as pool:\n",
    "        pool.map(preloaded_storm_generator, storm_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df02d31b-f6b0-474a-9ae7-a50d0ff5a0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for AM2.5, experiment CTL1990s_tiger3 from /projects/GEOCLIM/gr7610/analysis/tc_storage/track_data/TC_track_data.s101_e125.model_AM2.5.experiment_CTL1990s_tiger3.pkl...\n",
      "[load_TC_tracks] month range: (1, 13)\n",
      "[load_TC_tracks] month range: (1, 13)\n",
      "-------------------------------------------------------------\n",
      "Statistics for TCs in model: AM2.5; experiment: CTL1990s_tiger3\n",
      "Number of storms: total = 1178; per year = 51.0\n",
      "Storm duration: mean = 9.79 +/- 4.32 days\n",
      "Storm maximum winds: mean = 28.82 +/- 4.55 m/s\n",
      "Storm minimum pressure: mean = 980.17 +/- 15.89 hPa\n",
      "-------------------------------------------------------------\n",
      "\n",
      "[storm_generator] Processing storm ID 0114-0894...\n",
      "[storm_generator] Processing storm ID 0113-0104...\n",
      "[storm_generator] Processing storm ID 0121-4045...\n",
      "[storm_generator] Processing storm ID 0113-3859...\n",
      "[storm_generator] Processing storm ID 0113-4739...\n",
      "[storm_generator] Processing storm ID 0115-2001...\n",
      "[storm_generator] Processing storm ID 0101-0720...\n",
      "[storm_generator] Processing storm ID 0115-0516...\n",
      "[storm_generator] Processing storm ID 0102-3014...\n",
      "[storm_generator] Processing storm ID 0117-2899...\n",
      "[storm_generator] Processing storm ID 0111-3452...\n",
      "[storm_generator] Processing storm ID 0111-3585...\n",
      "[storm_generator] Processing storm ID 0103-0220...\n",
      "[storm_generator] Processing storm ID 0107-0961...\n",
      "[storm_generator] Processing storm ID 0115-0849...\n",
      "[storm_generator] Processing storm ID 0101-3958...\n",
      "[storm_generator] Processing storm ID 0119-0402...\n",
      "[storm_generator] Processing storm ID 0117-0614...\n",
      "[storm_generator] Processing storm ID 0109-3428...\n",
      "[storm_generator] Processing storm ID 0105-3281...\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0113-0104.max_wind-30.min_slp-973.basin-SP.nc: 2.86 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 0103-3751...\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0113-4739.max_wind-32.min_slp-976.basin-SP.nc: 5.04 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0117-2899.max_wind-32.min_slp-969.basin-NI.nc: 4.72 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 0119-2607...\n",
      "[storm_generator] Processing storm ID 0106-3393...\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0113-3859.max_wind-29.min_slp-980.basin-WP.nc: 8.34 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0117-0614.max_wind-33.min_slp-972.basin-SP.nc: 15.45 MB\n",
      "\n",
      "[storm_generator] Processing storm ID 0115-3938...\n",
      "[storm_generator] Processing storm ID 0106-3847...\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0102-3014.max_wind-33.min_slp-964.basin-WP.nc: 17.09 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0105-3281.max_wind-36.min_slp-952.basin-WP.nc: 23.82 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0111-3452.max_wind-33.min_slp-966.basin-WP.nc: 6.72 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0111-3585.max_wind-29.min_slp-980.basin-WP.nc: 6.61 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0115-0849.max_wind-32.min_slp-978.basin-SI.nc: 9.16 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0115-2001.max_wind-38.min_slp-955.basin-WP.nc: 8.22 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0115-0516.max_wind-29.min_slp-973.basin-SP.nc: 8.14 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0115-3938.max_wind-31.min_slp-980.basin-NI.nc: 4.11 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0103-0220.max_wind-26.min_slp-968.basin-NA.nc: 7.39 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0109-3428.max_wind-29.min_slp-978.basin-WP.nc: 12.27 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0119-0402.max_wind-32.min_slp-966.basin-SP.nc: 17.29 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0103-3751.max_wind-34.min_slp-964.basin-NI.nc: 11.55 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0119-2607.max_wind-31.min_slp-973.basin-WP.nc: 21.10 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0114-0894.max_wind-34.min_slp-971.basin-SI.nc: 4.91 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0106-3847.max_wind-34.min_slp-967.basin-WP.nc: 6.07 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0106-3393.max_wind-29.min_slp-976.basin-WP.nc: 20.85 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0101-0720.max_wind-34.min_slp-958.basin-SP.nc: 8.34 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0101-3958.max_wind-32.min_slp-968.basin-WP.nc: 4.00 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0121-4045.max_wind-33.min_slp-980.basin-WP.nc: 28.84 MB\n",
      "\n",
      "File size for TC.model-AM2.5.experiment-CTL1990s_tiger3.storm_ID-0107-0961.max_wind-41.min_slp-946.basin-SI.nc: 11.84 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(track_TCs);\n",
    "\n",
    "model_name = 'AM2.5'\n",
    "experiment_name = 'CTL1990s_tiger3'\n",
    "year_range = (101, 125)\n",
    "\n",
    "intensity_parameter = 'min_slp'\n",
    "intensity_range = (0, 980)\n",
    "main(model_name, experiment_name, year_range, intensity_parameter, intensity_range, number_of_storms=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532f537-a61a-4df0-9e0c-e16ebfa6decf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
